ARXIVID,Title,Abstract,Authors,Affiliations,Date_published,URL
2509.06234v1,Minimum-Cost Synthetic Genome Planning: An Algorithmic Framework,"As synthetic genomics scales toward the construction of increasingly larger genomes, computational strategies are needed to address technical feasibility. We introduce an algorithmic framework for the Minimum-Cost Synthetic Genome Planning problem, aiming to identify the most cost-effective strategy to assemble a target genome from a source genome through a combination of reuse, synthesis, and join operations. By comparing dynamic programming and greedy heuristic strategies under diverse cost regimes, we demonstrate how algorithmic choices influence the cost-efficiency of large-scale genome construction. In parallel, solving the Minimum-Cost Synthetic Genome Planning problem can help us better understand genome architecture and evolution. We applied our framework in case studies on viral genomes, including SARS-CoV-2, to examine how source-target genome similarity shapes construction costs. Our analyses revealed that conserved regions such as ORF1ab can be reconstructed cost-effectively from related templates, while highly variable regions such as the S (spike) gene are more reliant on DNA synthesis, highlighting the biological and economic trade-offs of genome design.",Michail Patsakis; Ioannis Mouratidis; Ilias Georgakopoulos-Soares,,2025-09-07T22:46:57Z,http://arxiv.org/abs/2509.06234v1
2505.08918v1,When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T   primate genomes,"The emergence of telomere-to-telomere (T2T) genome assemblies has opened new avenues for comparative genomics, yet effective tokenization strategies for genomic sequences remain underexplored. In this pilot study, we apply Byte Pair Encoding (BPE) to nine T2T primate genomes including three human assemblies by training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are shared across all assemblies, while nearly 991,854 tokens are unique to a single genome, indicating a rapid decline in shared vocabulary with increasing assembly comparisons. Moreover, phylogenetic trees derived from token overlap failed to recapitulate established primate relationships, a discrepancy attributed to the disproportionate influence of species-specific high-copy repetitive elements. These findings underscore the dual nature of BPE tokenization: while it effectively compresses repetitive sequences, its sensitivity to high-copy elements limits its utility as a universal tool for comparative genomics. We discuss potential hybrid strategies and repeat-masking approaches to refine genomic tokenization, emphasizing the need for domain-specific adaptations in the development of large-scale genomic language models. The dnaBPE tool used in this study is open-source and available at https://github.com/aglabx/dnaBPE.",Marina Popova; Iaroslav Chelombitko; Aleksey Komissarov,,2025-05-13T19:27:58Z,http://arxiv.org/abs/2505.08918v1
2505.16680v1,Learning Genomic Structure from $k$-mers,"Sequencing a genome to determine an individual's DNA produces an enormous number of short nucleotide subsequences known as reads, which must be reassembled to reconstruct the full genome. We present a method for analyzing this type of data using contrastive learning, in which an encoder model is trained to produce embeddings that cluster together sequences from the same genomic region. The sequential nature of genomic regions is preserved in the form of trajectories through this embedding space. Trained solely to reflect the structure of the genome, the resulting model provides a general representation of $k$-mer sequences, suitable for a range of downstream tasks involving read data. We apply our framework to learn the structure of the $E.\ coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read mapping and identification of structural variations. Furthermore, we illustrate the potential of using this type of model for metagenomic species identification. We show how incorporating a domain-specific noise model can enhance embedding robustness, and how a supervised contrastive learning setting can be adopted when a linear reference genome is available, by introducing a distance thresholding parameter $\Gamma$. The model can also be trained fully self-supervised on read data, enabling analysis without the need to construct a full genome assembly using specialized algorithms. Small prediction heads based on a pre-trained embedding are shown to perform on par with BWA-aln, the current gold standard approach for aDNA mapping, in terms of accuracy and runtime for short genomes. Given the method's favorable scaling properties with respect to total genome size, inference using our approach is highly promising for metagenomic applications and for mapping to genomes comparable in size to the human genome.",Filip Thor; Carl Nettelblad,,2025-05-22T13:46:18Z,http://arxiv.org/abs/2505.16680v1
2504.17860v1,"An Integrated Genomics Workflow Tool: Simulating Reads, Evaluating Read   Alignments, and Optimizing Variant Calling Algorithms","Next-generation sequencing (NGS) is a pivotal technique in genome sequencing due to its high throughput, rapid results, cost-effectiveness, and enhanced accuracy. Its significance extends across various domains, playing a crucial role in identifying genetic variations and exploring genomic complexity. NGS finds applications in diverse fields such as clinical genomics, comparative genomics, functional genomics, and metagenomics, contributing substantially to advancements in research, medicine, and scientific disciplines. Within the sphere of genomics data science, the execution of read simulation, mapping, and variant calling holds paramount importance for obtaining precise and dependable results. Given the plethora of tools available for these purposes, each employing distinct methodologies and options, a nuanced understanding of their intricacies becomes imperative for optimization. This research, situated at the intersection of data science and genomics, involves a meticulous assessment of various tools, elucidating their individual strengths and weaknesses through rigorous experimentation and analysis. This comprehensive evaluation has enabled the researchers to pinpoint the most accurate tools, reinforcing the alignment between the established workflow and the demonstrated efficacy of specific tools in the context of genomics data analysis. To meet these requirements, ""VarFind"", an open-source and freely accessible pipeline tool designed to automate the entire process has been introduced (VarFind GitHub repository: https://github.com/shanikawm/varfinder)",Fathima Nuzla Ismail; Shanika Amarasoma,,2025-04-24T18:05:08Z,http://arxiv.org/abs/2504.17860v1
2502.21125v1,Eukaryotes evade information storage-replication rate trade-off with   endosymbiont assistance leading to larger genomes,"Genome length varies widely among organisms, from compact genomes of prokaryotes to vast and complex genomes of eukaryotes. In this study, we theoretically identify the evolutionary pressures that may have driven this divergence in genome length. We use a parameter-free model to study genome length evolution under selection pressure to minimize replication time and maximize information storage capacity. We show that prokaryotes tend to reduce genome length, constrained by a single replication origin, while eukaryotes expand their genomes by incorporating multiple replication origins. We propose a connection between genome length and cellular energetics, suggesting that endosymbiotic organelles, mitochondria and chloroplasts, evolutionarily regulate the number of replication origins, thereby influencing genome length in eukaryotes. We show that the above two selection pressures also lead to strict equalization of the number of purines and their corresponding base-pairing pyrimidines within a single DNA strand, known as Chagraff's second parity rule, a hitherto unexplained observation in genomes of nearly all known species. This arises from the symmetrization of replichore length, another observation that has been shown to hold across species, which our model reproduces. The model also reproduces other experimentally observed phenomena, such as a general preference for deletions over insertions, and elongation and high variance of genome lengths under reduced selection pressure for replication rate, termed the C-value paradox. We highlight the possibility of regulation of the firing of latent replication origins in response to cues from the extracellular environment leading to the regulation of cell cycle rates in multicellular eukaryotes.",Parthasarathi Sahu; Sashikanta Barik; Koushik Ghosh; Hemachander Subramanian,,2025-02-28T15:01:36Z,http://arxiv.org/abs/2502.21125v1
2504.09060v1,Multimodal 3D Genome Pre-training,"Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research.",Minghao Yang; Pengteng Li; Yan Liang; Qianyi Cai; Zhihang Zheng; Shichen Zhang; Pengfei Zhang; Zhi-An Huang; Hui Xiong,,2025-04-12T03:31:03Z,http://arxiv.org/abs/2504.09060v1
2505.07919v1,Revolutionising Bacterial Genomics: Graph-Based Strategies for Improved   Variant Identification,"A significant advancement in bioinformatics is using genome graph techniques to improve variation discovery across organisms. Traditional approaches, such as bwa mem, rely on linear reference genomes for genomic analyses but may introduce biases when applied to highly diverse bacterial genomes of the same species. Pangenome graphs provide an alternative paradigm for evaluating structural and minor variations within a graphical framework, including insertions, deletions, and single nucleotide polymorphisms. Pangenome graphs enhance the detection and interpretation of complex genetic variants by representing the full genetic diversity of a species. In this study, we present a robust and reliable bioinformatics pipeline utilising the PanGenome Graph Builder (PGGB) and the Variation Graph toolbox (vg giraffe) to align whole-genome sequencing data, call variants against a graph reference, and construct pangenomes from assembled genomes. Our results demonstrate that leveraging pangenome graphs over a single linear reference genome significantly improves mapping rates and variant calling accuracy for simulated and actual bacterial pathogens datasets.",Fathima Nuzla Ismail; Abira Sengupta,,2025-05-12T15:34:24Z,http://arxiv.org/abs/2505.07919v1
2505.14402v1,OmniGenBench: A Modular Platform for Reproducible Genomic Foundation   Models Benchmarking,"The code of nature, embedded in DNA and RNA genomes since the origin of life, holds immense potential to impact both humans and ecosystems through genome modeling. Genomic Foundation Models (GFMs) have emerged as a transformative approach to decoding the genome. As GFMs scale up and reshape the landscape of AI-driven genomics, the field faces an urgent need for rigorous and reproducible evaluation. We present OmniGenBench, a modular benchmarking platform designed to unify the data, model, benchmarking, and interpretability layers across GFMs. OmniGenBench enables standardized, one-command evaluation of any GFM across five benchmark suites, with seamless integration of over 31 open-source models. Through automated pipelines and community-extensible features, the platform addresses critical reproducibility challenges, including data transparency, model interoperability, benchmark fragmentation, and black-box interpretability. OmniGenBench aims to serve as foundational infrastructure for reproducible genomic AI research, accelerating trustworthy discovery and collaborative innovation in the era of genome-scale modeling.",Heng Yang; Jack Cole; Yuan Li; Renzhi Chen; Geyong Min; Ke Li,,2025-05-20T14:16:25Z,http://arxiv.org/abs/2505.14402v1
2503.19367v3,VGAT: A Cancer Survival Analysis Framework Transitioning from Generative   Visual Question Answering to Genomic Reconstruction,"Multimodal learning combining pathology images and genomic sequences enhances cancer survival analysis but faces clinical implementation barriers due to limited access to genomic sequencing in under-resourced regions. To enable survival prediction using only whole-slide images (WSI), we propose the Visual-Genomic Answering-Guided Transformer (VGAT), a framework integrating Visual Question Answering (VQA) techniques for genomic modality reconstruction. By adapting VQA's text feature extraction approach, we derive stable genomic representations that circumvent dimensionality challenges in raw genomic data. Simultaneously, a cluster-based visual prompt module selectively enhances discriminative WSI patches, addressing noise from unfiltered image regions. Evaluated across five TCGA datasets, VGAT outperforms existing WSI-only methods, demonstrating the viability of genomic-informed inference without sequencing. This approach bridges multimodal research and clinical feasibility in resource-constrained settings. The code link is https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT.",Zizhi Chen; Minghao Han; Xukun Zhang; Shuwei Ma; Tao Liu; Xing Wei; Lihua Zhang,,2025-03-25T05:48:31Z,http://arxiv.org/abs/2503.19367v3
2503.23691v1,A Conceptual Framework for Human-AI Collaborative Genome Annotation,"Genome annotation is essential for understanding the functional elements within genomes. While automated methods are indispensable for processing large-scale genomic data, they often face challenges in accurately predicting gene structures and functions. Consequently, manual curation by domain experts remains crucial for validating and refining these predictions. These combined outcomes from automated tools and manual curation highlight the importance of integrating human expertise with AI capabilities to improve both the accuracy and efficiency of genome annotation. However, the manual curation process is inherently labor-intensive and time-consuming, making it difficult to scale for large datasets. To address these challenges, we propose a conceptual framework, Human-AI Collaborative Genome Annotation (HAICoGA), which leverages the synergistic partnership between humans and artificial intelligence to enhance human capabilities and accelerate the genome annotation process. Additionally, we explore the potential of integrating Large Language Models (LLMs) into this framework to support and augment specific tasks. Finally, we discuss emerging challenges and outline open research questions to guide further exploration in this area.",Xiaomei Li; Alex Whan; Meredith McNeil; David Starns; Jessica Irons; Samuel C. Andrew; Rad Suchecki,,2025-03-31T03:44:00Z,http://arxiv.org/abs/2503.23691v1
2503.03773v1,A Phylogenetic Approach to Genomic Language Modeling,"Genomic language models (gLMs) have shown mostly modest success in identifying evolutionarily constrained elements in mammalian genomes. To address this issue, we introduce a novel framework for training gLMs that explicitly models nucleotide evolution on phylogenetic trees using multispecies whole-genome alignments. Our approach integrates an alignment into the loss function during training but does not require it for making predictions, thereby enhancing the model's applicability. We applied this framework to train PhyloGPN, a model that excels at predicting functionally disruptive variants from a single sequence alone and demonstrates strong transfer learning capabilities.",Carlos Albors; Jianan Canal Li; Gonzalo Benegas; Chengzhong Ye; Yun S. Song,,2025-03-04T06:53:03Z,http://arxiv.org/abs/2503.03773v1
2509.05539v1,Investigating DNA words and their distributions across the tree of life,"The frequency distributions of DNA k-mers are shaped by fundamental biological processes and offer a window into genome structure and evolution. Inspired by analogies to natural language, prior studies have attempted to model genomic k-mer usage using Zipf's law, a rank-frequency law originally formulated for words in human language. However, the extent to which this law accurately captures the distribution of k-mers across diverse species remains unclear. Here, we systematically analyze k-mer frequency spectra across more than 225,000 genome assemblies spanning all three domains of life and viruses. We demonstrate that Zipf's law consistently underperforms in modeling k-mer distributions. In contrast, we propose the truncated power law and Zipf-Mandelbrot distributions, which provide substantially improved fits across taxonomic groups. We show that genome size and GC content influence model performance, with larger and GC-content imbalanced genomes yielding better adherence. Additionally, we perform an extensive analysis on vocabulary expansion and exhaustion across the same organisms using Heaps' law. We apply our modeling framework to evaluate simulated genomes generated by k-let preserving shuffling and deep generative language models. Our results reveal substantial differences between organismal genomes and their synthetic or shuffled counterparts, offering a novel approach to benchmark the biological plausibility of artificial genomes. Collectively, this work establishes new standards for modeling genomic k-mer distributions and provides insights relevant to synthetic biology, and evolutionary sequence analysis.",Charalampos Koilakos; Kimonas Provatas; Michail Patsakis; Aris Karatzikos; Ilias Georgakopoulos-Soares,,2025-09-05T23:32:30Z,http://arxiv.org/abs/2509.05539v1
2501.11982v1,WinPCA: A package for windowed principal component analysis,"Principal component analysis (PCA) is routinely used in population genetics to assess genetic structure. With chromosomal reference genomes and population-scale whole genome-sequencing becoming increasingly accessible, contemporary studies often include characterizations of the genomic landscape as it varies along chromosomes, commonly termed genome scans. While traditional summary statistics like FST and dXY remain integral to characterizing the genomic divergence profile, PCA fundamentally differs by providing single-sample resolution, thereby making results intuitively interpretable to help identify polymorphic inversions, introgression and other types of divergent sequence. Here, we introduce WinPCA, a user-friendly package to compute, polarize and visualize genetic principal components in windows along the genome. To accommodate low-coverage whole genome-sequencing datasets, WinPCA can optionally make use of PCAngsd methods to compute principal components in a genotype likelihood framework. WinPCA accepts variant data in either VCF or BEAGLE format and can generate rich plots for interactive data exploration and downstream presentation.",L. Moritz Blumer; Jeffrey M. Good; Richard Durbin,,2025-01-21T08:57:45Z,http://arxiv.org/abs/2501.11982v1
2502.16470v1,Bancroft: Genomics Acceleration Beyond On-Device Memory,"This paper presents Bancroft, a computational genomics acceleration platform that provides the illusion of practically infinite on-device memory capacity by compressing genomic data movement over PCIe. Bancroft introduces novel optimizations for efficient accelerator implementation to reference-based genome compression, including fixed-stride matching using cuckoo hashes and grouped header encoding, incorporated into a familiar interface supporting random accesses. We evaluate a prototype implementation of Bancroft on an affordable Alveo U50 FPGA equipped with 8 GB of HBM. Thanks to the orders of magnitude improvements in performance and resource efficiency of genomic compression, our prototype provides access to TBs of host-side genomic data at memory-class performance, measuring speeds over 30% of the on-device HBM bandwidth, an order of magnitude higher than conventional PCIe-limited architectures. Using a real-world pre-alignment filtering application, Bancroft demonstrates over 6x improvement over the conventional PCIe-attached architecture, achieving 30% of peak internal throughput of an accelerator with HBM, and 90% of the one with DDR4. Bancroft supports memory-class performance to practically infinite data capacity, using a small, fixed amount of HBM, making it an attractive solution to continued future scalability of computational genomics.",Se-Min Lim; Seongyoung Kang; Sang-Woo Jun,,2025-02-23T07:07:09Z,http://arxiv.org/abs/2502.16470v1
2504.20034v1,Pan-genome Analysis of Angiosperm Plastomes using PGR-TK,"We present a novel approach for taxonomic analysis of chloroplast genomes in angiosperms using the Pan-genome Research Toolkit (PGR-TK). Comparative plots generated by PGR-TK across diverse angiosperm genera reveal a wide range of structural complexity, from straightforward to highly intricate patterns. Notably, the characteristic quadripartite plastome structure, comprising the large single copy (LSC), small single copy (SSC), and inverted repeat (IR) regions, is clearly identifiable in over 75% of the genera analyzed. Our findings also underscore several occurrences of species mis-annotations in public genomic databases, which are readily detected through visual anomalies in the PGR-TK plots. While more complex plot patterns remain difficult to interpret, they likely reflect underlying biological variation or technical inconsistencies in genome assembly. Overall, this approach effectively integrates classical botanical visualization with modern molecular taxonomy, providing a powerful tool for genome-based classification in plant systematics.",Manoj P. Samanta,,2025-04-28T17:56:13Z,http://arxiv.org/abs/2504.20034v1
2506.01833v1,SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,"Inspired by the success of unsupervised pre-training paradigms, researchers have applied these approaches to DNA pre-training. However, we argue that these approaches alone yield suboptimal results because pure DNA sequences lack sufficient information, since their functions are regulated by genomic profiles like chromatin accessibility. Here, we demonstrate that supervised training for genomic profile prediction serves as a more effective alternative to pure sequence pre-training. Furthermore, considering the multi-species and multi-profile nature of genomic profile prediction, we introduce our $\textbf{S}$pecies-$\textbf{P}$rofile $\textbf{A}$daptive $\textbf{C}$ollaborative $\textbf{E}$xperts (SPACE) that leverages Mixture of Experts (MoE) to better capture the relationships between DNA sequences across different species and genomic profiles, thereby learning more effective DNA representations. Through extensive experiments across various tasks, our model achieves state-of-the-art performance, establishing that DNA models trained with supervised genomic profiles serve as powerful DNA representation learners. The code is available at https://github.com/ZhuJiwei111/SPACE.",Zhao Yang; Jiwei Zhu; Bing Su,,2025-06-02T16:23:05Z,http://arxiv.org/abs/2506.01833v1
2507.02877v1,AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and   Scalable Circular Genome Visualizations,"Circular genome visualizations are essential for exploring structural variants and gene regulation. However, existing tools often require complex scripting and manual configuration, making the process time-consuming, error-prone, and difficult to learn. To address these challenges, we introduce AuraGenome, an LLM-powered framework for rapid, reusable, and scalable generation of multi-layered circular genome visualizations. AuraGenome combines a semantic-driven multi-agent workflow with an interactive visual analytics system. The workflow employs seven specialized LLM-driven agents, each assigned distinct roles such as intent recognition, layout planning, and code generation, to transform raw genomic data into tailored visualizations. The system supports multiple coordinated views tailored for genomic data, offering ring, radial, and chord-based layouts to represent multi-layered circular genome visualizations. In addition to enabling interactions and configuration reuse, the system supports real-time refinement and high-quality report export. We validate its effectiveness through two case studies and a comprehensive user study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.",Chi Zhang; Yu Dong; Yang Wang; Yuetong Han; Guihua Shan; Bixia Tang,,2025-06-18T03:29:30Z,http://arxiv.org/abs/2507.02877v1
2503.09496v2,Robust Multimodal Survival Prediction with the Latent Differentiation   Conditional Variational AutoEncoder,"The integrative analysis of histopathological images and genomic data has received increasing attention for survival prediction of human cancers. However, the existing studies always hold the assumption that full modalities are available. As a matter of fact, the cost for collecting genomic data is high, which sometimes makes genomic data unavailable in testing samples. A common way of tackling such incompleteness is to generate the genomic representations from the pathology images. Nevertheless, such strategy still faces the following two challenges: (1) The gigapixel whole slide images (WSIs) are huge and thus hard for representation. (2) It is difficult to generate the genomic embeddings with diverse function categories in a unified generative framework. To address the above challenges, we propose a Conditional Latent Differentiation Variational AutoEncoder (LD-CVAE) for robust multimodal survival prediction, even with missing genomic data. Specifically, a Variational Information Bottleneck Transformer (VIB-Trans) module is proposed to learn compressed pathological representations from the gigapixel WSIs. To generate different functional genomic features, we develop a novel Latent Differentiation Variational AutoEncoder (LD-VAE) to learn the common and specific posteriors for the genomic embeddings with diverse functions. Finally, we use the product-of-experts technique to integrate the genomic common posterior and image posterior for the joint latent distribution estimation in LD-CVAE. We test the effectiveness of our method on five different cancer datasets, and the experimental results demonstrate its superiority in both complete and missing modality scenarios.",Junjie Zhou; Jiao Tang; Yingli Zuo; Peng Wan; Daoqiang Zhang; Wei Shao,,2025-03-12T15:58:37Z,http://arxiv.org/abs/2503.09496v2
2504.03732v2,SAGe: A Lightweight Algorithm-Architecture Co-Design for Mitigating the   Data Preparation Bottleneck in Large-Scale Genome Analysis,"Given the exponentially growing volumes of genomic data, there are extensive efforts to accelerate genome analysis. We demonstrate a major bottleneck that greatly limits and diminishes the benefits of state-of-the-art genome analysis accelerators: the data preparation bottleneck, where genomic data is stored in compressed form and needs to be decompressed and formatted first before an accelerator can operate on it. To mitigate this bottleneck, we propose SAGe, an algorithm-architecture co-design for highly-compressed storage and high-performance access of large-scale genomic data. SAGe overcomes the challenges of mitigating the data preparation bottleneck while maintaining high compression ratios (comparable to genomic-specific compression algorithms) at low hardware cost. This is enabled by leveraging key features of genomic datasets to co-design (i) a new (de)compression algorithm, (ii) hardware, (iii) storage data layout, and (iv) interface commands to access storage. SAGe stores data in structures that can be rapidly interpreted and decompressed by efficient streaming accesses and lightweight hardware. To achieve high compression ratios using only these lightweight structures, SAGe exploits unique features of genomic data. We show that SAGe can be seamlessly integrated with a broad range of genome analysis hardware accelerators to mitigate their data preparation bottlenecks. Our results demonstrate that SAGe improves the average end-to-end performance and energy efficiency of two state-of-the-art genome analysis accelerators by 3.0x-32.1x and 18.8x-49.6x, respectively, compared to when the accelerators rely on state-of-the-art decompression tools.",Nika Mansouri Ghiasi; Talu Güloglu; Harun Mustafa; Can Firtina; Konstantina Koliogeorgi; Konstantinos Kanellopoulos; Haiyu Mao; Rakesh Nadig; Mohammad Sadrosadati; Jisung Park; Onur Mutlu,,2025-03-31T23:36:26Z,http://arxiv.org/abs/2504.03732v2
2507.08060v1,MicroTrace: A Lightweight R Tool for SNP-Based Pathogen Clustering in   Outbreak Detection,"MicroTrace is an open-source R tool that performs SNP-based hierarchical clustering to detect potential transmission clusters from pathogen whole-genome sequencing (WGS) data. Designed for epidemiologists, microbiologists, and genomic surveillance teams, it processes SNP distance matrices and outputs dendrograms and cluster tables with optional metadata integration. MicroTrace enables reproducible outbreak detection workflows with minimal setup.",Kaitao Lai,,2025-07-10T15:41:53Z,http://arxiv.org/abs/2507.08060v1
2506.17766v1,Improving Genomic Models via Task-Specific Self-Pretraining,"Pretraining DNA language models (DNALMs) on the full human genome is resource-intensive, yet often considered necessary for strong downstream performance. Inspired by recent findings in NLP and long-context modeling, we explore an alternative: self-pretraining on task-specific, unlabeled data. Using the BEND benchmark, we show that DNALMs trained with self-pretraining match or exceed the performance of models trained from scratch under identical compute. While genome-scale pretraining may still offer higher absolute performance, task-specific self-pretraining provides a practical and compute-efficient strategy for building stronger supervised baselines.",Sohan Mupparapu; Parameswari Krishnamurthy; Ratish Puduppully,,2025-06-21T17:19:21Z,http://arxiv.org/abs/2506.17766v1
2501.16982v1,"Human Genome Book: Words, Sentences and Paragraphs","Since the completion of the human genome sequencing project in 2001, significant progress has been made in areas such as gene regulation editing and protein structure prediction. However, given the vast amount of genomic data, the segments that can be fully annotated and understood remain relatively limited. If we consider the genome as a book, constructing its equivalents of words, sentences, and paragraphs has been a long-standing and popular research direction. Recently, studies on transfer learning in large language models have provided a novel approach to this challenge.Multilingual transfer ability, which assesses how well models fine-tuned on a source language can be applied to other languages, has been extensively studied in multilingual pre-trained models. Similarly, the transfer of natural language capabilities to ""DNA language"" has also been validated. Building upon these findings, we first trained a foundational model capable of transferring linguistic capabilities from English to DNA sequences. Using this model, we constructed a vocabulary of DNA words and mapped DNA words to their English equivalents.Subsequently, we fine-tuned this model using English datasets for paragraphing and sentence segmentation to develop models capable of segmenting DNA sequences into sentences and paragraphs. Leveraging these models, we processed the GRCh38.p14 human genome by segmenting, tokenizing, and organizing it into a ""book"" comprised of genomic ""words,"" ""sentences,"" and ""paragraphs."" Additionally, based on the DNA-to-English vocabulary mapping, we created an ""English version"" of the genomic book. This study offers a novel perspective for understanding the genome and provides exciting possibilities for developing innovative tools for DNA search, generation, and analysis.",Wang Liang,,2025-01-23T04:39:24Z,http://arxiv.org/abs/2501.16982v1
2503.16565v1,Gene42: Long-Range Genomic Foundation Model With Dense Attention,"We introduce Gene42, a novel family of Genomic Foundation Models (GFMs) designed to manage context lengths of up to 192,000 base pairs (bp) at a single-nucleotide resolution. Gene42 models utilize a decoder-only (LLaMA-style) architecture with a dense self-attention mechanism. Initially trained on fixed-length sequences of 4,096 bp, our models underwent continuous pretraining to extend the context length to 192,000 bp. This iterative extension allowed for the comprehensive processing of large-scale genomic data and the capture of intricate patterns and dependencies within the human genome. Gene42 is the first dense attention model capable of handling such extensive long context lengths in genomics, challenging state-space models that often rely on convolutional operators among other mechanisms. Our pretrained models exhibit notably low perplexity values and high reconstruction accuracy, highlighting their strong ability to model genomic data. Extensive experiments on various genomic benchmarks have demonstrated state-of-the-art performance across multiple tasks, including biotype classification, regulatory region identification, chromatin profiling prediction, variant pathogenicity prediction, and species classification. The models are publicly available at huggingface.co/inceptionai.",Kirill Vishniakov; Boulbaba Ben Amor; Engin Tekin; Nancy A. ElNaker; Karthik Viswanathan; Aleksandr Medvedev; Aahan Singh; Maryam Nadeem; Mohammad Amaan Sayeed; Praveenkumar Kanithi; Tiago Magalhaes; Natalia Vassilieva; Dwarikanath Mahapatra; Marco Pimentel; and Shadab Khan,,2025-03-20T07:10:04Z,http://arxiv.org/abs/2503.16565v1
2504.06304v2,Leveraging State Space Models in Long Range Genomics,"Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.",Matvei Popov; Aymen Kallala; Anirudha Ramesh; Narimane Hennouni; Shivesh Khaitan; Rick Gentry; Alain-Sam Cohen,,2025-04-07T18:34:06Z,http://arxiv.org/abs/2504.06304v2
2506.19097v1,Quantum Gradient Optimized Drug Repurposing Prototype for Omics Data,This paper presents a novel quantum-enhanced prototype for drug repurposing and addresses the challenge of managing massive genomics data in precision medicine.,Don Roosan; Saif Nirzhor; Rubayat Khan; Fahmida Hai,,2025-06-23T20:17:55Z,http://arxiv.org/abs/2506.19097v1
2508.13191v1,NucEL: Single-Nucleotide ELECTRA-Style Genomic Pre-training for   Efficient and Interpretable Representations,"Pre-training large language models on genomic sequences is a powerful approach for learning biologically meaningful representations. Masked language modeling (MLM) methods, such as DNABERT and Nucleotide Transformer (NT), achieve strong performance but suffer from partial token supervision, pre-training/fine-tuning mismatches, and high computational costs. We introduce NucEL, the first ELECTRA-style pre-training framework for genomic foundation models, addressing these limitations. Using a discriminator to identify tokens altered by a generator, NucEL provides comprehensive token-level supervision across all sequence positions, improving efficiency over the partial supervision of MLM. Incorporating ModernBERT's hybrid local-global attention and flash attention, NucEL offers an optimized BERT architecture for genomic modeling. Unlike 6-mer tokenization, NucEL uses single-nucleotide tokens for fine-grained resolution, boosting both efficiency and interpretability. Pre-trained on the human genome, NucEL achieves state-of-the-art results on diverse downstream tasks -- regulatory element identification (e.g., promoters, enhancers), transcription factor binding prediction, open chromatin classification, and histone modification profiling -- surpassing similarly sized MLM-based models and rivaling models 25x larger, such as NT. Ablation studies highlight optimal tokenization and masking strategies for ELECTRA-style DNA pre-training. Attention analysis reveals NucEL's superior capture of biologically relevant motifs compared to NT, providing insights into hierarchical learning and regulatory element modeling. These findings demonstrate ELECTRA-style pre-training as an efficient, effective strategy for genomic representation learning with broad implications for genomic research.",Ke Ding; Brian Parker; Jiayu Wen,,2025-08-15T12:34:51Z,http://arxiv.org/abs/2508.13191v1
2502.15109v4,Social Genome: Grounded Social Reasoning Abilities of Multimodal Models,"Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce SOCIAL GENOME, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). SOCIAL GENOME is also the first modeling challenge to study external knowledge in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of SOCIAL GENOME through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.",Leena Mathur; Marian Qian; Paul Pu Liang; Louis-Philippe Morency,,2025-02-21T00:05:40Z,http://arxiv.org/abs/2502.15109v4
2503.20451v1,Agptools: a utility suite for editing genome assemblies,"The AGP format is a tab-separated table format describing how components of a genome assembly fit together. A standard submission format for genome assemblies is a fasta file giving the sequence of contigs along with an AGP file showing how these components are assembled into larger pieces like scaffolds or chromosomes. For this reason, many scaffolding software pipelines output assemblies in this format. However, although many programs for assembling and scaffolding genomes read and write this format, there is currently no published software for making edits to AGP files when performing assembly curation. We present agptools, a suite of command-line programs that can perform common operations on AGP files, such as breaking and joining sequences, inverting pieces of assembly components, assembling contigs into larger sequences based on an AGP file, and transforming between coordinate systems of different assembly layouts. Additionally, agptools includes an API that writers of other software packages can use to read, write, and manipulate AGP files within their own programs. Agptools gives bioinformaticians a simple, robust, and reproducible way to edit genome assemblies that avoids the shortfalls of other methods for editing AGP files.",Edward S. Ricemeyer; Rachel A. Carroll; Wesley C. Warren,,2025-03-26T11:26:21Z,http://arxiv.org/abs/2503.20451v1
2506.00082v1,An AI-powered Knowledge Hub for Potato Functional Genomics,"Potato functional genomics lags due to unsystematic gene information curation, gene identifier inconsistencies across reference genome versions, and the increasing volume of research publications. To address these limitations, we developed the Potato Knowledge Hub (http://www.potato-ai.top), leveraging Large Language Models (LLMs) and a systematically curated collection of over 3,200 high-quality potato research papers spanning over 120 years. This platform integrates two key modules: a functional gene database containing 2,571 literature-reported genes, meticulously mapped to the latest DMv8.1 reference genome with resolved nomenclature discrepancies and links to original publications; and a potato knowledge base. The knowledge base, built using a Retrieval-Augmented Generation (RAG) architecture, accurately answers research queries with literature citations, mitigating LLM ""hallucination."" Users can interact with the hub via a natural language AI agent, ""Potato Research Assistant,"" for querying specialized knowledge, retrieving gene information, and extracting sequences. The continuously updated Potato Knowledge Hub aims to be a comprehensive resource, fostering advancements in potato functional genomics and supporting breeding programs.",Jia Yuxin; Li Jinye; Jia Yudong; Li Futing; Su Xiaoqi; Luo Jilin; Dong Yarui; Sun Chunyan; Cui Qinghan; Wang Li; Li Axiu; Shang Yi; Zhu Yujuan; Huang Sanwen,,2025-05-30T03:09:59Z,http://arxiv.org/abs/2506.00082v1
2506.00597v1,Processing-in-memory for genomics workloads,"Low-cost, high-throughput DNA and RNA sequencing (HTS) data is the main workforce for the life sciences. Genome sequencing is now becoming a part of Predictive, Preventive, Personalized, and Participatory (termed 'P4') medicine. All genomic data are currently processed in energy-hungry computer clusters and centers, necessitating data transfer, consuming substantial energy, and wasting valuable time. Therefore, there is a need for fast, energy-efficient, and cost-efficient technologies that enable genomics research without requiring data centers and cloud platforms. We recently started the BioPIM Project to leverage the emerging processing-in-memory (PIM) technologies to enable energy and cost-efficient analysis of bioinformatics workloads. The BioPIM Project focuses on co-designing algorithms and data structures commonly used in genomics with several PIM architectures for the highest cost, energy, and time savings benefit.",William Andrew Simon; Leonid Yavits; Konstantina Koliogeorgi; Yann Falevoz; Yoshihiro Shibuya; Dominique Lavenier; Irem Boybat; Klea Zambaku; Berkan Şahin; Mohammad Sadrosadati; Onur Mutlu; Abu Sebastian; Rayan Chikhi; The BioPIM Consortium; Can Alkan,,2025-05-31T15:13:06Z,http://arxiv.org/abs/2506.00597v1
2506.19324v1,Memory-Augmented Incomplete Multimodal Survival Prediction via   Cross-Slide and Gene-Attentive Hypergraph Learning,"Multimodal pathology-genomic analysis is critical for cancer survival prediction. However, existing approaches predominantly integrate formalin-fixed paraffin-embedded (FFPE) slides with genomic data, while neglecting the availability of other preservation slides, such as Fresh Froze (FF) slides. Moreover, as the high-resolution spatial nature of pathology data tends to dominate the cross-modality fusion process, it hinders effective multimodal fusion and leads to modality imbalance challenges between pathology and genomics. These methods also typically require complete data modalities, limiting their clinical applicability with incomplete modalities, such as missing either pathology or genomic data. In this paper, we propose a multimodal survival prediction framework that leverages hypergraph learning to effectively integrate multi-WSI information and cross-modality interactions between pathology slides and genomics data while addressing modality imbalance. In addition, we introduce a memory mechanism that stores previously learned paired pathology-genomic features and dynamically compensates for incomplete modalities. Experiments on five TCGA datasets demonstrate that our model outperforms advanced methods by over 2.3% in C-Index. Under incomplete modality scenarios, our approach surpasses pathology-only (3.3%) and gene-only models (7.9%). Code: https://github.com/MCPathology/M2Surv",Mingcheng Qu; Guang Yang; Donglin Di; Yue Gao; Tonghua Su; Yang Song; Lei Fan,,2025-06-24T05:31:13Z,http://arxiv.org/abs/2506.19324v1
2507.11950v1,RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery,"Functional annotation of microbial genomes is often biased toward protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs (ncRNAs) that are critical for regulating bacterial and archaeal physiology, stress response and metabolism. Identifying ncRNAs directly from genomic sequence is a paramount challenge in bioinformatics and biology, essential for understanding the complete regulatory potential of an organism. This paper presents RNAMunin, a machine learning (ML) model that is capable of finding ncRNAs using genomic sequence alone. It is also computationally viable for large sequence datasets such as long read metagenomic assemblies with contigs totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary samples. We know of no other model that can detect ncRNAs based solely on genomic sequence at this scale. Since RNAMunin only requires genomic sequence as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do not need transcriptomics data. We wrote this manuscript in a narrative style in order to best convey how RNAMunin was developed and how it works in detail. Unlike almost all current ML models, at approximately 1M parameters, RNAMunin is very small and very fast.",Lauren Lui; Torben Nielsen,,2025-07-16T06:33:50Z,http://arxiv.org/abs/2507.11950v1
2508.09406v1,CAKL: Commutative algebra k-mer learning of genomics,"Despite the availability of various sequence analysis models, comparative genomic analysis remains a challenge in genomics, genetics, and phylogenetics. Commutative algebra, a fundamental tool in algebraic geometry and number theory, has rarely been used in data and biological sciences. In this study, we introduce commutative algebra k-mer learning (CAKL) as the first-ever nonlinear algebraic framework for analyzing genomic sequences. CAKL bridges between commutative algebra, algebraic topology, combinatorics, and machine learning to establish a new mathematical paradigm for comparative genomic analysis. We evaluate its effectiveness on three tasks -- genetic variant identification, phylogenetic tree analysis, and viral genome classification -- typically requiring alignment-based, alignment-free, and machine-learning approaches, respectively. Across eleven datasets, CAKL outperforms five state-of-the-art sequence analysis methods, particularly in viral classification, and maintains stable predictive accuracy as dataset size increases, underscoring its scalability and robustness. This work ushers in a new era in commutative algebraic data analysis and learning.",Faisal Suwayyid; Yuta Hozumi; Hongsong Feng; Mushal Zia; JunJie Wee; Guo-Wei Wei,,2025-08-13T00:43:32Z,http://arxiv.org/abs/2508.09406v1
2501.02284v3,"Origin of $α$-satellite repeat arrays from mitochondrial molecular   fossils -- sequential insertion, expansion, and evolution in the nuclear   genome","Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its origin remains an evolutionary mystery. In this research, we identified 1,545 alpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp Nasonia vitripennis. Among them, thirty-nine copies of SatL were organized in two palindromic arrays in mitochondria, resulting in a 50% increase in the genome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL repeats revealed that they are located in NuMT (nuclear mitochondrial DNA) regions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT pseudogenes. These results support that SatL arrays originated from ten independent mitochondria insertion events into the nuclear genome within the last 500,000 years, after divergence from its sister species N. giraulti. Dramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of rapid SatL sequence evolution in mitochondria due to GC-biased gene conversion facilitated by the palindromic sequence pairing of the two mitochondrial SatL arrays. The nuclear SatL repeat arrays underwent substantial copy number expansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B array consists of four types of repeat units derived from deletions in the AT-rich region of ancestral repeats, and complex high-order structures have evolved through duplications. We also discovered similar repeat insertions into the nuclear genome of Muscidifurax, suggesting this mechanism can be common in insects. This is the first report of the mitochondrial origin of nuclear satellite sequences, and our findings shed new light on the origin and evolution of satellite DNA.",Yihang Zhou,,2025-01-04T13:22:30Z,http://arxiv.org/abs/2501.02284v3
2501.07737v1,Multi-megabase scale genome interpretation with genetic language models,"Understanding how molecular changes caused by genetic variation drive disease risk is crucial for deciphering disease mechanisms. However, interpreting genome sequences is challenging because of the vast size of the human genome, and because its consequences manifest across a wide range of cells, tissues and scales -- spanning from molecular to whole organism level. Here, we present Phenformer, a multi-scale genetic language model that learns to generate mechanistic hypotheses as to how differences in genome sequence lead to disease-relevant changes in expression across cell types and tissues directly from DNA sequences of up to 88 million base pairs. Using whole genome sequencing data from more than 150 000 individuals, we show that Phenformer generates mechanistic hypotheses about disease-relevant cell and tissue types that match literature better than existing state-of-the-art methods, while using only sequence data. Furthermore, disease risk predictors enriched by Phenformer show improved prediction performance and generalisation to diverse populations. Accurate multi-megabase scale interpretation of whole genomes without additional experimental data enables both a deeper understanding of molecular mechanisms involved in disease and improved disease risk prediction at the level of individuals.",Frederik Träuble; Lachlan Stuart; Andreas Georgiou; Pascal Notin; Arash Mehrjou; Ron Schwessinger; Mathieu Chevalley; Kim Branson; Bernhard Schölkopf; Cornelia van Duijn; Debora Marks; Patrick Schwab,,2025-01-13T23:00:40Z,http://arxiv.org/abs/2501.07737v1
2502.07272v3,GENERator: A Long-Context Generative Genomic Foundation Model,"Advancements in DNA sequencing technologies have significantly improved our ability to decode genomic sequences. However, the prediction and interpretation of these sequences remain challenging due to the intricate nature of genetic material. Large language models (LLMs) have introduced new opportunities for biological sequence analysis. Recent developments in genomic language models have underscored the potential of LLMs in deciphering DNA sequences. Nonetheless, existing models often face limitations in robustness and application scope, primarily due to constraints in model structure and training data scale. To address these limitations, we present GENERator, a generative genomic foundation model featuring a context length of 98k base pairs (bp) and 1.2B parameters. Trained on an expansive dataset comprising 386B bp of eukaryotic DNA, the GENERator demonstrates state-of-the-art performance across both established and newly proposed benchmarks. The model adheres to the central dogma of molecular biology, accurately generating protein-coding sequences that translate into proteins structurally analogous to known families. It also shows significant promise in sequence optimization, particularly through the prompt-responsive generation of enhancer sequences with specific activity profiles. These capabilities position the GENERator as a pivotal tool for genomic research and biotechnological advancement, enhancing our ability to interpret and predict complex biological systems and enabling precise genomic interventions. Implementation details and supplementary resources are available at https://github.com/GenerTeam/GENERator.",Wei Wu; Qiuyi Li; Mingyang Li; Kun Fu; Fuli Feng; Jieping Ye; Hui Xiong; Zheng Wang,,2025-02-11T05:39:49Z,http://arxiv.org/abs/2502.07272v3
2503.02997v1,"Enabling Fast, Accurate, and Efficient Real-Time Genome Analysis via New   Algorithms and Techniques","The advent of high-throughput sequencing technologies has revolutionized genome analysis by enabling the rapid and cost-effective sequencing of large genomes. Despite these advancements, the increasing complexity and volume of genomic data present significant challenges related to accuracy, scalability, and computational efficiency. These challenges are mainly due to various forms of unwanted and unhandled variations in sequencing data, collectively referred to as noise. In this dissertation, we address these challenges by providing a deep understanding of different types of noise in genomic data and developing techniques to mitigate the impact of noise on genome analysis.   First, we introduce BLEND, a noise-tolerant hashing mechanism that quickly identifies both exactly matching and highly similar sequences with arbitrary differences using a single lookup of their hash values. Second, to enable scalable and accurate analysis of noisy raw nanopore signals, we propose RawHash, a novel mechanism that effectively reduces noise in raw nanopore signals and enables accurate, real-time analysis by proposing the first hash-based similarity search technique for raw nanopore signals. Third, we extend the capabilities of RawHash with RawHash2, an improved mechanism that 1) provides a better understanding of noise in raw nanopore signals to reduce it more effectively and 2) improves the robustness of mapping decisions. Fourth, we explore the broader implications and new applications of raw nanopore signal analysis by introducing Rawsamble, the first mechanism for all-vs-all overlapping of raw signals using hash-based search. Rawsamble enables the construction of de novo assemblies directly from raw signals without basecalling, which opens up new directions and uses for raw nanopore signal analysis.",Can Firtina,,2025-03-04T20:44:37Z,http://arxiv.org/abs/2503.02997v1
2508.17211v1,Liquid-liquid phase separation enables highly selective viral genome   packaging,"In many viruses, hundreds of proteins assemble an outer shell (capsid) around the viral nucleic acid to form an infectious virion. How the assembly process selects the viral genome amidst a vast excess of diverse cellular nucleic acids is poorly understood. It has recently been discovered that many viruses perform assembly and genome packaging within liquid-liquid phase separated biomolecular condensates inside the host cell. However, the role of condensates in genome packaging is poorly understood. Here, we construct equilibrium and dynamical rate equation models for condensate-coupled assembly and genome packaging. We show that when the viral genome and capsid proteins favorably partition into the condensate, assembly rates, yields, and packaging efficiencies can increase by orders of magnitude. Selectivity is further enhanced by the condensate when capsid proteins are translated during assembly and packaging. Our results suggest that viral condensates provide a mechanism to ensure robust and highly selective assembly of virions around viral genomes. More broadly, our results may apply to other types of selective co-assembly processes that occur within biomolecular condensates, and suggest that liquid-liquid phase-separated condensates could be exploited for selective encapsulation of microscopic cargo in human-engineered systems.",Layne B. Frechette; Michael F. Hagan,,2025-08-24T04:51:01Z,http://arxiv.org/abs/2508.17211v1
2502.07749v1,Whole-Genome Phenotype Prediction with Machine Learning: Open Problems   in Bacterial Genomics,"How can we identify causal genetic mechanisms that govern bacterial traits? Initial efforts entrusting machine learning models to handle the task of predicting phenotype from genotype return high accuracy scores. However, attempts to extract any meaning from the predictive models are found to be corrupted by falsely identified ""causal"" features. Relying solely on pattern recognition and correlations is unreliable, significantly so in bacterial genomics settings where high-dimensionality and spurious associations are the norm. Though it is not yet clear whether we can overcome this hurdle, significant efforts are being made towards discovering potential high-risk bacterial genetic variants. In view of this, we set up open problems surrounding phenotype prediction from bacterial whole-genome datasets and extending those to learning causal effects, and discuss challenges that impact the reliability of a machine's decision-making when faced with datasets of this nature.",Tamsin James; Ben Williamson; Peter Tino; Nicole Wheeler,,2025-02-11T18:25:14Z,http://arxiv.org/abs/2502.07749v1
2504.07065v1,Enhancing Downstream Analysis in Genome Sequencing: Species   Classification While Basecalling,"The ability to quickly and accurately identify microbial species in a sample, known as metagenomic profiling, is critical across various fields, from healthcare to environmental science. This paper introduces a novel method to profile signals coming from sequencing devices in parallel with determining their nucleotide sequences, a process known as basecalling, via a multi-objective deep neural network for simultaneous basecalling and multi-class genome classification. We introduce a new loss strategy where losses for basecalling and classification are back-propagated separately, with model weights combined for the shared layers, and a pre-configured ranking strategy allowing top-K species accuracy, giving users flexibility to choose between higher accuracy or higher speed at identifying the species. We achieve state-of-the-art basecalling accuracies, while classification accuracies meet and exceed the results of state-of-the-art binary classifiers, attaining an average of 92.5%/98.9% accuracy at identifying the top-1/3 species among a total of 17 genomes in the Wick bacterial dataset. The work presented here has implications for future studies in metagenomic profiling by accelerating the bottleneck step of matching the DNA sequence to the correct genome.",Riselda Kodra; Hadjer Benmeziane; Irem Boybat; William Andrew Simon,,2025-04-09T17:30:43Z,http://arxiv.org/abs/2504.07065v1
2504.11340v1,Organisation and dynamics of individual DNA segments in topologically   complex genomes,"Capturing the physical organisation and dynamics of genomic regions is one of the major open challenges in biology. The kinetoplast DNA (kDNA) is a topologically complex genome, made by thousands of DNA (mini and maxi) circles interlinked into a two-dimensional Olympic network. The organisation and dynamics of these DNA circles are poorly understood. In this paper, we show that dCas9 linked to Quantum Dots can efficiently label different classes of DNA minicircles in kDNA. We use this method to study the distribution and dynamics of different classes of DNA minicircles within the network. We discover that maxicircles display a preference to localise at the periphery of the network and that they undergo subdiffusive dynamics. From the latter, we can also quantify the effective network stiffness, confirming previous indirect estimations via AFM. Our method could be used more generally, to quantify the location, dynamics and material properties of genomic regions in other complex genomes, such as that of bacteria, and to study their behaviour in the presence of DNA-binding proteins.",Saminathan Ramakrishnan; Auro Varat Patnaik; Guglielmo Grillo; Luca Tubiana; Davide Michieletto,,2025-04-15T16:12:54Z,http://arxiv.org/abs/2504.11340v1
2505.07188v1,Securing Genomic Data Against Inference Attacks in Federated Learning   Environments,"Federated Learning (FL) offers a promising framework for collaboratively training machine learning models across decentralized genomic datasets without direct data sharing. While this approach preserves data locality, it remains susceptible to sophisticated inference attacks that can compromise individual privacy. In this study, we simulate a federated learning setup using synthetic genomic data and assess its vulnerability to three key attack vectors: Membership Inference Attack (MIA), Gradient-Based Membership Inference Attack, and Label Inference Attack (LIA). Our experiments reveal that Gradient-Based MIA achieves the highest effectiveness, with a precision of 0.79 and F1-score of 0.87, underscoring the risk posed by gradient exposure in federated updates. Additionally, we visualize comparative attack performance through radar plots and quantify model leakage across clients. The findings emphasize the inadequacy of na\""ive FL setups in safeguarding genomic privacy and motivate the development of more robust privacy-preserving mechanisms tailored to the unique sensitivity of genomic data.",Chetan Pathade; Shubham Patil,,2025-05-12T02:36:50Z,http://arxiv.org/abs/2505.07188v1
2506.00662v1,Uncertainty-Aware Genomic Classification of Alzheimer's Disease: A   Transformer-Based Ensemble Approach with Monte Carlo Dropout,"INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating robust classification from genomic data. METHODS: We developed a transformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for uncertainty estimation in AD classification from whole-genome sequencing (WGS). We combined a transformer that preserves single-nucleotide polymorphism (SNP) sequence structure with a concurrent random forest using flattened genotypes. An uncertainty threshold separated samples into an uncertain (high-variance) group and a more certain (low-variance) group. RESULTS: We analyzed 1050 individuals, holding out half for testing. Overall accuracy and area under the receiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636, respectively. Excluding the uncertain group improved accuracy from 0.6263 to 0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase). DISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous cases that may require further clinical evaluation, thus improving reliability in AD genomic classification.",Taeho Jo; Eun Hye Lee; Alzheimer's Disease Sequencing Project,,2025-05-31T18:20:49Z,http://arxiv.org/abs/2506.00662v1
2506.02212v1,"Leveraging Natural Language Processing to Unravel the Mystery of Life: A   Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics","Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.",Ella Rannon; David Burstein,,2025-06-02T19:54:03Z,http://arxiv.org/abs/2506.02212v1
2506.11896v1,GlobDB: A comprehensive species-dereplicated microbial genome resource,"Over the past years, substantial numbers of microbial species' genomes have been deposited outside of conventional INSDC databases. The GlobDB aggregates 14 independent genomic catalogues to provide a comprehensive database of species-dereplicated microbial genomes, with consistent taxonomy, annotations, and additional analysis resources. The GlobDB is available at https://globdb.org/.",Daan R. Speth; Nick Pullen; Samuel T. N. Aroney; Benjamin L. Coltman; Jay T. Osvatic; Ben J. Woodcroft; Thomas Rattei; Michael Wagner,"Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria; Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria; Centre for Microbiome Research School of Biomedical Sciences, Queensland University of Technology, Translational Research Institute, Woolloongabba, Australia; Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria; Joint Microbiome Facility of the Medical University of Vienna and the University of Vienna, Vienna, Austria; Centre for Microbiome Research School of Biomedical Sciences, Queensland University of Technology, Translational Research Institute, Woolloongabba, Australia; Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria; Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria",2025-06-13T15:43:15Z,http://arxiv.org/abs/2506.11896v1
2507.05452v1,Topological Sequence Analysis of Genomes: Delta Complex approaches,"Algebraic topology has been widely applied to point cloud data to capture geometric shapes and topological structures. However, its application to genome sequence analysis remains rare. In this work, we propose topological sequence analysis (TSA) techniques by constructing $\Delta$-complexes and classifying spaces, leading to persistent homology, and persistent path homology on genome sequences. We also develop $\Delta$-complex-based persistent Laplacians to facilitate the topological spectral analysis of genome sequences. Finally, we demonstrate the utility of the proposed TSA approaches in phylogenetic analysis using Ebola virus sequences and whole bacterial genomes. The present TSA methods are more efficient than earlier TSA model, k-mer topology, and thus have a potential to be applied to other time-consuming sequential data analyses, such as those in linguistics, literature, music, media, and social contexts.",Jian Liu; Li Shen; Dong Chen; Guo-Wei Wei,,2025-07-07T20:10:31Z,http://arxiv.org/abs/2507.05452v1
2506.10886v2,"S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable   with DBOS","To meet the needs of a large pharmaceutical organization, we set out to create S3Mirror - an application for transferring large genomic sequencing datasets between S3 buckets quickly, reliably, and observably. We used the DBOS Transact durable execution framework to achieve these goals and benchmarked the performance and cost of the application. S3Mirror is an open source DBOS Python application that can run in a variety of environments, including DBOS Cloud Pro, where it runs as much as 40x faster than AWS DataSync at a fraction of the cost. Moreover, S3Mirror is resilient to failures and allows for real-time filewise observability of ongoing and past transfers.",Steven Vasquez-Grinnell; Alex Poliakov,,2025-06-12T16:50:04Z,http://arxiv.org/abs/2506.10886v2
2501.02045v1,METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring,"We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.",Ollie Liu; Sami Jaghouar; Johannes Hagemann; Shangshang Wang; Jason Wiemels; Jeff Kaufman; Willie Neiswanger,,2025-01-03T18:44:43Z,http://arxiv.org/abs/2501.02045v1
2501.04941v1,MyESL: Sparse learning in molecular evolution and phylogenetic analysis,"Evolutionary sparse learning (ESL) uses a supervised machine learning approach, Least Absolute Shrinkage and Selection Operator (LASSO), to build models explaining the relationship between a hypothesis and the variation across genomic features (e.g., sites) in sequence alignments. ESL employs sparsity between and within the groups of genomic features (e.g., genomic loci) by using sparse-group LASSO. Although some software packages are available for performing sparse group LASSO, we found them less well-suited for processing and analyzing genome-scale data containing millions of features, such as bases. MyESL software fills the need for open-source software for conducting ESL analyses with facilities to pre-process the input hypotheses and large alignments, make LASSO flexible and computationally efficient, and post-process the output model to produce different metrics useful in functional or evolutionary genomics. MyESL can take phylogenetic trees and sequence alignments as input and transform them into numeric responses and features, respecetively. The model outputs are processed into user-friendly text and graphical files. The computational core of MyESL is written in C++, which offers model building with or without group sparsity, while the pre- and post-processing of inputs and model outputs is performed using customized functions written in Python. One of its applications in phylogenomics showcases the utility of MyESL. Our analysis of empirical genome-scale datasets shows that MyESL can build evolutionary models quickly and efficiently on a personal desktop, while other computational packages were unable due to their prohibitive requirements of computational resources and time. MyESL is available for Python environments on Linux and distributed as a standalone application for Windows and macOS. It is available from https://github.com/kumarlabgit/MyESL.",Maxwell Sanderford; Sudip Sharma; Glen Stecher; Jun Liu; Jieping Ye; Sudhir Kumar,,2025-01-09T03:16:16Z,http://arxiv.org/abs/2501.04941v1
2501.11217v1,CoverM: Read alignment statistics for metagenomics,"Genome-centric analysis of metagenomic samples is a powerful method for understanding the function of microbial communities. Calculating read coverage is a central part of analysis, enabling differential coverage binning for recovery of genomes and estimation of microbial community composition. Coverage is determined by processing read alignments to reference sequences of either contigs or genomes. Per-reference coverage is typically calculated in an ad-hoc manner, with each software package providing its own implementation and specific definition of coverage. Here we present a unified software package CoverM which calculates several coverage statistics for contigs and genomes in an ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational efficiency and avoids unnecessary I/O overhead by calculating coverage statistics from streamed read alignment results. CoverM is free software available at https://github.com/wwood/coverm. CoverM is implemented in Rust, with Python (https://github.com/apcamargo/pycoverm) and Julia (https://github.com/JuliaBinaryWrappers/CoverM_jll.jl) interfaces.",Samuel T. N. Aroney; Rhys J. P. Newell; Jakob N. Nissen; Antonio Pedro Camargo; Gene W. Tyson; Ben J. Woodcroft,"Centre for Microbiome Research, School of Biomedical Sciences, Queensland University of Technology; Centre for Microbiome Research, School of Biomedical Sciences, Queensland University of Technology; The Novo Nordisk Foundation Center for Protein Research, University of Copenhagen; Departamento de Genética e Evolução, Instituto de Biologia, Universidade Estadual de Campinas; Centre for Microbiome Research, School of Biomedical Sciences, Queensland University of Technology; Centre for Microbiome Research, School of Biomedical Sciences, Queensland University of Technology",2025-01-20T01:43:39Z,http://arxiv.org/abs/2501.11217v1
2502.03499v1,Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and   Multi-Task Learning,"Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model sizes grow. Moreover, existing GFMs are constrained by rigid output formats, limiting their applicability to various genomic tasks. In this work, we revisit the transformer-based auto-regressive models and introduce Omni-DNA, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters. Our approach consists of two stages: (i) pretraining on DNA sequences with next token prediction objective, and (ii) expanding the multi-modal task-specific tokens and finetuning for multiple downstream tasks simultaneously. When evaluated on the Nucleotide Transformer and GB benchmarks, Omni-DNA achieves state-of-the-art performance on 18 out of 26 tasks. Through multi-task finetuning, Omni-DNA addresses 10 acetylation and methylation tasks at once, surpassing models trained on each task individually. Finally, we design two complex genomic tasks, DNA2Function and Needle-in-DNA, which map DNA sequences to textual functional descriptions and images, respectively, indicating Omni-DNA's cross-modal capabilities to broaden the scope of genomic applications. All the models are available through https://huggingface.co/collections/zehui127",Zehui Li; Vallijah Subasri; Yifei Shen; Dongsheng Li; Yiren Zhao; Guy-Bart Stan; Caihua Shan,,2025-02-05T09:20:52Z,http://arxiv.org/abs/2502.03499v1
2503.15377v1,Genomic data processing with GenomeFlow,"Advances in genome sequencing technologies generate massive amounts of sequence data that are increasingly analyzed and shared through public repositories. On-demand infrastructure services on cloud computing platforms enable the processing of such large-scale genomic sequence data in distributed processing environments with a significant reduction in analysis time. However, parallel processing on cloud computing platforms presents many challenges to researchers, even skillful bioinformaticians. In particular, it is difficult to design a computing architecture optimized to reduce the cost of computing and disk storage as genomic data analysis pipelines often employ many heterogeneous tools with different resource requirements. To address these issues, we developed GenomeFlow, a tool for automated development of computing architecture and resource optimization on Google Cloud Platform, which allows users to process a large number of samples at minimal cost. We outline multiple use cases of GenomeFlow demonstrating its utility to significantly reduce computing time and cost associated with analyzing genomic and transcriptomic data from hundreds to tens of thousands of samples from several consortia. Here, we describe a step-by-step protocol on how to use GenomeFlow for a common genomic data processing task. We introduce this example protocol geared toward a bioinformatician with little experience in cloud computing.",Junseok Park; Eduardo A. Maury; Changhoon Oh; Donghoon Shin; Danielle Denisko; Eunjung Alice Lee,,2025-03-19T16:13:05Z,http://arxiv.org/abs/2503.15377v1
2505.08071v1,NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome   Assembly,"De novo assembly enables investigations of unknown genomes, paving the way for personalized medicine and disease management. However, it faces immense computational challenges arising from the excessive data volumes and algorithmic complexity.   While state-of-the-art de novo assemblers utilize distributed systems for extreme-scale genome assembly, they demand substantial computational and memory resources. They also fail to address the inherent challenges of de novo assembly, including a large memory footprint, memory-bound behavior, and irregular data patterns stemming from complex, interdependent data structures. Given these challenges, de novo assembly merits a custom hardware solution, though existing approaches have not fully addressed the limitations.   We propose NMP-PaK, a hardware-software co-design that accelerates scalable de novo genome assembly through near-memory processing (NMP). Our channel-level NMP architecture addresses memory bottlenecks while providing sufficient scratchpad space for processing elements. Customized processing elements maximize parallelism while efficiently handling large data structures that are both dynamic and interdependent. Software optimizations include customized batch processing to reduce the memory footprint and hybrid CPU-NMP processing to address hardware underutilization caused by irregular data patterns.   NMP-PaK conducts the same genome assembly while incurring a 14X smaller memory footprint compared to the state-of-the-art de novo assembly. Moreover, NMP-PaK delivers a 16X performance improvement over the CPU baseline, with a 2.4X reduction in memory operations. Consequently, NMP-PaK achieves 8.3X greater throughput than state-of-the-art de novo assembly under the same resource constraints, showcasing its superior computational efficiency.",Heewoo Kim; Sanjay Sri Vallabh Singapuram; Haojie Ye; Joseph Izraelevitz; Trevor Mudge; Ronald Dreslinski; Nishil Talati,,2025-05-12T21:17:20Z,http://arxiv.org/abs/2505.08071v1
2507.19229v1,TrinityDNA: A Bio-Inspired Foundational Model for Efficient   Long-Sequence DNA Modeling,"The modeling of genomic sequences presents unique challenges due to their length and structural complexity. Traditional sequence models struggle to capture long-range dependencies and biological features inherent in DNA. In this work, we propose TrinityDNA, a novel DNA foundational model designed to address these challenges. The model integrates biologically informed components, including Groove Fusion for capturing DNA's structural features and Gated Reverse Complement (GRC) to handle the inherent symmetry of DNA sequences. Additionally, we introduce a multi-scale attention mechanism that allows the model to attend to varying levels of sequence dependencies, and an evolutionary training strategy that progressively adapts the model to both prokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and efficient approach to genomic sequence modeling, offering significant improvements in gene function prediction, regulatory mechanism discovery, and other genomics applications. Our model bridges the gap between machine learning techniques and biological insights, paving the way for more effective analysis of genomic data. Additionally, we introduced a new DNA long-sequence CDS annotation benchmark to make evaluations more comprehensive and oriented toward practical applications.",Qirong Yang; Yucheng Guo; Zicheng Liu; Yujie Yang; Qijin Yin; Siyuan Li; Shaomin Ji; Linlin Chao; Xiaoming Zhang; Stan Z. Li,,2025-07-25T12:55:30Z,http://arxiv.org/abs/2507.19229v1
2507.21648v1,Hyperbolic Genome Embeddings,"Current approaches to genomic sequence modeling often struggle to align the inductive biases of machine learning models with the evolutionarily-informed structure of biological systems. To this end, we formulate a novel application of hyperbolic CNNs that exploits this structure, enabling more expressive DNA sequence representations. Our strategy circumvents the need for explicit phylogenetic mapping while discerning key properties of sequences pertaining to core functional and regulatory behavior. Across 37 out of 42 genome interpretation benchmark datasets, our hyperbolic models outperform their Euclidean equivalents. Notably, our approach even surpasses state-of-the-art performance on seven GUE benchmark datasets, consistently outperforming many DNA language models while using orders of magnitude fewer parameters and avoiding pretraining. Our results include a novel set of benchmark datasets--the Transposable Elements Benchmark--which explores a major but understudied component of the genome with deep evolutionary significance. We further motivate our work by exploring how our hyperbolic models recognize genomic signal under various data-generating conditions and by constructing an empirical method for interpreting the hyperbolicity of dataset embeddings. Throughout these assessments, we find persistent evidence highlighting the potential of our hyperbolic framework as a robust paradigm for genome representation learning. Our code and benchmark datasets are available at https://github.com/rrkhan/HGE.",Raiyan R. Khan; Philippe Chlenski; Itsik Pe'er,,2025-07-29T10:06:17Z,http://arxiv.org/abs/2507.21648v1
2501.04718v1,Knowledge-Guided Biomarker Identification for Label-Free Single-Cell   RNA-Seq Data: A Reinforcement Learning Perspective,"Gene panel selection aims to identify the most informative genomic biomarkers in label-free genomic datasets. Traditional approaches, which rely on domain expertise, embedded machine learning models, or heuristic-based iterative optimization, often introduce biases and inefficiencies, potentially obscuring critical biological signals. To address these challenges, we present an iterative gene panel selection strategy that harnesses ensemble knowledge from existing gene selection algorithms to establish preliminary boundaries or prior knowledge, which guide the initial search space. Subsequently, we incorporate reinforcement learning through a reward function shaped by expert behavior, enabling dynamic refinement and targeted selection of gene panels. This integration mitigates biases stemming from initial boundaries while capitalizing on RL's stochastic adaptability. Comprehensive comparative experiments, case studies, and downstream analyses demonstrate the effectiveness of our method, highlighting its improved precision and efficiency for label-free biomarker discovery. Our results underscore the potential of this approach to advance single-cell genomics data analysis.",Meng Xiao; Weiliang Zhang; Xiaohan Huang; Hengshu Zhu; Min Wu; Xiaoli Li; Yuanchun Zhou,,2025-01-02T07:57:41Z,http://arxiv.org/abs/2501.04718v1
2501.07606v1,Heuristics based on Adjacency Graph Packing for DCJ Distance Considering   Intergenic Regions,"In this work, we explore heuristics for the Adjacency Graph Packing problem, which can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ is a rearrangement operation and the distance problem considering it is a well established method for genome comparison. Our heuristics will use the structure called adjacency graph adapted to include information about intergenic regions, multiple copies of genes in the genomes, and multiple circular or linear chromosomes. The only required property from the genomes is that it must be possible to turn one into the other with DCJ operations. We propose one greedy heuristic and one heuristic based on Genetic Algorithms. Our experimental tests in artificial genomes show that the use of heuristics is capable of finding good results that are superior to a simpler random strategy.",Gabriel Siqueira; Alexsandro Oliveira Alexandrino; Andre Rodrigues Oliveira; Zanoni Dias,,2025-01-11T18:57:46Z,http://arxiv.org/abs/2501.07606v1
2501.08193v1,Modeling Quantum Machine Learning for Genomic Data Analysis,"Quantum Machine Learning (QML) continues to evolve, unlocking new opportunities for diverse applications. In this study, we investigate and evaluate the applicability of QML models for binary classification of genome sequence data by employing various feature mapping techniques. We present an open-source, independent Qiskit-based implementation to conduct experiments on a benchmark genomic dataset. Our simulations reveal that the interplay between feature mapping techniques and QML algorithms significantly influences performance. Notably, the Pegasos Quantum Support Vector Classifier (Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recall metrics, while Quantum Neural Networks (QNN) achieve the highest training accuracy across all feature maps. However, the pronounced variability in classifier performance, dependent on feature mapping, highlights the risk of overfitting to localized output distributions in certain scenarios. This work underscores the transformative potential of QML for genomic data classification while emphasizing the need for continued advancements to enhance the robustness and accuracy of these methodologies.",Navneet Singh; Shiva Raj Pokhrel,,2025-01-14T15:14:26Z,http://arxiv.org/abs/2501.08193v1
2502.04067v1,Generalised Bayesian distance-based phylogenetics for the genomics era,"As whole genomes become widely available, maximum likelihood and Bayesian phylogenetic methods are demonstrating their limits in meeting the escalating computational demands. Conversely, distance-based phylogenetic methods are efficient, but are rarely favoured due to their inferior performance. Here, we extend distance-based phylogenetics using an entropy-based likelihood of the evolution among pairs of taxa, allowing for fast Bayesian inference in genome-scale datasets. We provide evidence of a close link between the inference criteria used in distance methods and Felsenstein's likelihood, such that the methods are expected to have comparable performance in practice. Using the entropic likelihood, we perform Bayesian inference on three phylogenetic benchmark datasets and find that estimates closely correspond with previous inferences. We also apply this rapid inference approach to a 60-million-site alignment from 363 avian taxa, covering most avian families. The method has outstanding performance and reveals substantial uncertainty in the avian diversification events immediately after the K-Pg transition event. The entropic likelihood allows for efficient Bayesian phylogenetic inference, accommodating the analysis demands of the genomic era.",Matthew J. Penn; Neil Scheidwasser; Mark P. Khurana; Christl A. Donnelly; David A. Duchêne; Samir Bhatt,,2025-02-06T13:24:29Z,http://arxiv.org/abs/2502.04067v1
2503.09312v2,Terrier: A Deep Learning Repeat Classifier,"Repetitive DNA sequences underpin genome architecture and evolutionary processes, yet they remain challenging to classify accurately. Terrier is a deep learning model designed to overcome these challenges by classifying repetitive DNA sequences using a publicly available, curated repeat sequence library trained under the RepeatMasker schema. Poor representation of taxa within repeat databases often limits the classification accuracy and reproducibility of current repeat annotation methods, limiting our understanding of repeat evolution and function. Terrier overcomes these challenges by leveraging deep learning for improved accuracy. Trained on Repbase, which includes over 100,000 repeat families -- four times more than Dfam -- Terrier maps 97.1% of Repbase sequences to RepeatMasker categories, offering the most comprehensive classification system available. When benchmarked against DeepTE, TERL, and TEclass2 in model organisms (rice, fruit flies, humans, and mice), Terrier achieved superior accuracy while classifying a broader range of sequences. Further validation in non-model amphibian, flatworm and Northern krill genomes highlights its effectiveness in improving classification in non-model species, facilitating research on repeat-driven evolution, genomic instability, and phenotypic variation.",Robert Turnbull; Neil D. Young; Edoardo Tescari; Lee F. Skerratt; Tiffany A. Kosch,,2025-03-12T12:03:26Z,http://arxiv.org/abs/2503.09312v2
2503.10713v1,HiCMamba: Enhancing Hi-C Resolution and Identifying 3D Genome Structures   with State Space Modeling,"Hi-C technology measures genome-wide interaction frequencies, providing a powerful tool for studying the 3D genomic structure within the nucleus. However, high sequencing costs and technical challenges often result in Hi-C data with limited coverage, leading to imprecise estimates of chromatin interaction frequencies. To address this issue, we present a novel deep learning-based method HiCMamba to enhance the resolution of Hi-C contact maps using a state space model. We adopt the UNet-based auto-encoder architecture to stack the proposed holistic scan block, enabling the perception of both global and local receptive fields at multiple scales. Experimental results demonstrate that HiCMamba outperforms state-of-the-art methods while significantly reducing computational resources. Furthermore, the 3D genome structures, including topologically associating domains (TADs) and loops, identified in the contact maps recovered by HiCMamba are validated through associated epigenomic features. Our work demonstrates the potential of a state space model as foundational frameworks in the field of Hi-C resolution enhancement.",Minghao Yang; Zhi-An Huang; Zhihang Zheng; Yuqiao Liu; Shichen Zhang; Pengfei Zhang; Hui Xiong; Shaojun Tang,,2025-03-13T03:04:02Z,http://arxiv.org/abs/2503.10713v1
2505.00598v2,Fast and Low-Cost Genomic Foundation Models via Outlier Removal,"To address the challenge of scarce computational resources in genomic modeling, we introduce GERM, a genomic foundation model with strong compression performance and fast adaptability. GERM improves upon models like DNABERT-2 by eliminating outliers that hinder low-rank adaptation and post-training quantization, enhancing both efficiency and robustness. We replace the vanilla attention layer with an outlier-free mechanism inspired by associative memory models. By removing outliers during both pre-training and fine-tuning, this approach accelerates adaptation, reduces computational costs, and enhances quantization robustness within acceptable loss margins. Additionally, we propose GERM-T, a strategy that employs small-step continual learning within the outlier-free framework, leveraging original checkpoints to avoid retraining from scratch. Empirically, GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over the baseline model. It also reduces average kurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading methods, GERM consistently delivers superior performance, offering a practical solution for genomic modeling in resource-constrained settings. Code is available at https://github.com/MAGICS-LAB/GERM.",Haozheng Luo; Chenghao Qiu; Maojiang Su; Zhihan Zhou; Zoe Mehta; Guo Ye; Jerry Yao-Chieh Hu; Han Liu,,2025-05-01T15:31:09Z,http://arxiv.org/abs/2505.00598v2
2505.03377v1,Gene finding revisited: improved robustness through structured decoding   from learned embeddings,"Gene finding is the task of identifying the locations of coding sequences within the vast amount of genetic code contained in the genome. With an ever increasing quantity of raw genome sequences, gene finding is an important avenue towards understanding the genetic information of (novel) organisms, as well as learning shared patterns across evolutionarily diverse species. The current state of the art are graphical models usually trained per organism and requiring manually curated datasets. However, these models lack the flexibility to incorporate deep learning representation learning techniques that have in recent years been transformative in the analysis of pro tein sequences, and which could potentially help gene finders exploit the growing number of the sequenced genomes to expand performance across multiple organisms. Here, we propose a novel approach, combining learned embeddings of raw genetic sequences with exact decoding using a latent conditional random field. We show that the model achieves performance matching the current state of the art, while increasing training robustness, and removing the need for manually fitted length distributions. As language models for DNA improve, this paves the way for more performant cross-organism gene-finders.",Frederikke I. Marin; Dennis Pultz; Wouter Boomsma,,2025-05-06T09:53:15Z,http://arxiv.org/abs/2505.03377v1
2505.11997v2,Multimodal Cancer Survival Analysis via Hypergraph Learning with   Cross-Modality Rebalance,"Multimodal pathology-genomic analysis has become increasingly prominent in cancer survival prediction. However, existing studies mainly utilize multi-instance learning to aggregate patch-level features, neglecting the information loss of contextual and hierarchical details within pathology images. Furthermore, the disparity in data granularity and dimensionality between pathology and genomics leads to a significant modality imbalance. The high spatial resolution inherent in pathology data renders it a dominant role while overshadowing genomics in multimodal integration. In this paper, we propose a multimodal survival prediction framework that incorporates hypergraph learning to effectively capture both contextual and hierarchical details from pathology images. Moreover, it employs a modality rebalance mechanism and an interactive alignment fusion strategy to dynamically reweight the contributions of the two modalities, thereby mitigating the pathology-genomics imbalance. Quantitative and qualitative experiments are conducted on five TCGA datasets, demonstrating that our model outperforms advanced methods by over 3.4\% in C-Index performance.",Mingcheng Qu; Guang Yang; Donglin Di; Tonghua Su; Yue Gao; Yang Song; Lei Fan,,2025-05-17T13:16:54Z,http://arxiv.org/abs/2505.11997v2
2505.19461v1,Fluctuations in DNA Packing Density Drive the Spatial Segregation   between Euchromatin and Heterochromatin,"In the crowded eukaryotic nucleus, euchromatin and heterochromatin segregate into distinct compartments, a phenomenon often attributed to homotypic interactions mediated by liquid liquid phase separation of chromatin associated proteins. Here, we revisit genome compartmentalization by examining the role of in vivo DNA packing density fluctuations driven by ATP dependent chromatin remodelers. Leveraging DNA accessibility data, we develop a polymer based model that captures these fluctuations and successfully reproduces genome wide compartment patterns observed in HiC data, without invoking homotypic interactions. Further analysis reveals that density fluctuations in a crowded nuclear environment elevate the system energy, while euchromatin heterochromatin segregation facilitates energy dissipation, offering a thermodynamic advantage for spontaneous compartment formation. These findings suggest that euchromatin heterochromatin segregation may arise through a non equilibrium, self organizing process, providing new insights into genome organization.",Luming Meng; Boping Liu; Qiong Luo,,2025-05-26T03:34:57Z,http://arxiv.org/abs/2505.19461v1
2505.19501v2,Toward Scientific Reasoning in LLMs: Training from Expert Discussions   via Reinforcement Learning,"We investigate how to teach large language models (LLMs) to perform scientific reasoning by leveraging expert discussions as a learning signal. Focusing on the genomics domain, we develop an automated pipeline to extract trainable data and introduce Genome-Bench, a new benchmark constructed from over a decade of scientific forum discussions on genome engineering. Our pipeline transforms raw interactions into a reinforcement learning-friendly multiple-choice questions format, supported by 3000+ high-quality question-answer pairs spanning foundational biology, experimental troubleshooting, tool usage, and beyond. We fine-tune an LLM using RL with a rule-based reward signal derived from the synthetic MCQ dataset to enhance domain-specific reasoning. Our results show that reinforcement learning from scientific discussions improves model performance by over 15% compared to the base model on Genome-Bench, narrowing the gap between open-source LLMs and expert-level reasoning. To our knowledge, this is the first end-to-end pipeline for teaching LLMs to reason from scientific discussions, with promising potential for generalization across scientific domains beyond biology.",Ming Yin; Yuanhao Qu; Ling Yang; Le Cong; Mengdi Wang,,2025-05-26T04:28:46Z,http://arxiv.org/abs/2505.19501v2
2506.00673v1,DuAL-Net: A Hybrid Framework for Alzheimer's Disease Prediction from   Whole-Genome Sequencing via Local SNP Windows and Global Annotations,"Alzheimer's disease (AD) dementia is the most common form of dementia. With the emergence of disease-modifying therapies, predicting disease risk before symptom onset has become critical. We introduce DuAL-Net, a hybrid deep learning framework for AD dementia prediction using whole genome sequencing (WGS) data. DuAL-Net integrates two components: local probability modeling, which segments the genome into non-overlapping windows, and global annotation-based modeling, which annotates SNPs and reorganizes WGS input to capture long-range functional relationships. Both employ out-of-fold stacking with TabNet and Random Forest classifiers. Final predictions combine local and global probabilities using an optimized weighting parameter alpha. We analyzed WGS data from 1,050 individuals (443 cognitively normal, 607 AD dementia) using five-fold cross-validation. DuAL-Net achieved an AUC of 0.671 using top-ranked SNPs, representing 35.0% and 20.3% higher performance than bottom-ranked and randomly selected SNPs, respectively. ROC analysis demonstrated strong positive correlation between SNP prioritization rank and predictive power. The model identified known AD-associated SNPs as top contributors alongside potentially novel variants. DuAL-Net presents a promising framework improving both predictive accuracy and biological interpretability. The framework and web implementation offer an accessible platform for broader research applications.",Eun Hye Lee; Taeho Jo,,2025-05-31T18:53:19Z,http://arxiv.org/abs/2506.00673v1
2506.00821v1,SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation   Models,"Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated significant success in variant effect prediction. However, their adversarial robustness remains largely unexplored. To address this gap, we propose SafeGenes: a framework for Secure analysis of genomic foundation models, leveraging adversarial attacks to evaluate robustness against both engineered near-identical adversarial Genes and embedding-space manipulations. In this study, we assess the adversarial vulnerabilities of GFMs using two approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM introduces minimal perturbations to input sequences, while the soft prompt attack optimizes continuous embeddings to manipulate model predictions without modifying the input tokens. By combining these techniques, SafeGenes provides a comprehensive assessment of GFM susceptibility to adversarial manipulation. Targeted soft prompt attacks led to substantial performance degradation, even in large models such as ESM1b and ESM1v. These findings expose critical vulnerabilities in current foundation models, opening new research directions toward improving their security and robustness in high-stakes genomic applications such as variant effect prediction.",Huixin Zhan; Jason H. Moore,,2025-06-01T03:54:03Z,http://arxiv.org/abs/2506.00821v1
2506.12986v1,Improving spliced alignment by modeling splice sites with deep learning,"Motivation: Spliced alignment refers to the alignment of messenger RNA (mRNA) or protein sequences to eukaryotic genomes. It plays a critical role in gene annotation and the study of gene functions. Accurate spliced alignment demands sophisticated modeling of splice sites, but current aligners use simple models, which may affect their accuracy given dissimilar sequences.   Results: We implemented minisplice to learn splice signals with a one-dimensional convolutional neural network (1D-CNN) and trained a model with 7,026 parameters for vertebrate and insect genomes. It captures conserved splice signals across phyla and reveals GC-rich introns specific to mammals and birds. We used this model to estimate the empirical splicing probability for every GT and AG in genomes, and modified minimap2 and miniprot to leverage pre-computed splicing probability during alignment. Evaluation on human long-read RNA-seq data and cross-species protein datasets showed our method greatly improves the junction accuracy especially for noisy long RNA-seq reads and proteins of distant homology.   Availability and implementation: https://github.com/lh3/minisplice",Siying Yang; Neng Huang; Heng Li,,2025-06-15T22:57:33Z,http://arxiv.org/abs/2506.12986v1
2507.04111v1,Quantum computing for genomics: conceptual challenges and practical   perspectives,"We assess the potential of quantum computing to accelerate computation of central tasks in genomics, focusing on often-neglected theoretical limitations. We discuss state-of-the-art challenges of quantum search, optimization, and machine learning algorithms. Examining database search with Grover's algorithm, we show that the expected speedup vanishes under realistic assumptions. For combinatorial optimization prevalent in genomics, we discuss the limitations of theoretical complexity in practice and suggest carefully identifying problems genuinely suited for quantum acceleration. Given the competition from excellent classical approximate solvers, quantum computing could offer a speedup in the near future only for a specific subset of hard enough tasks in assembly, gene selection, and inference. These tasks need to be characterized by core optimization problems that are particularly challenging for classical methods while requiring relatively limited variables. We emphasize rigorous empirical validation through runtime scaling analysis to avoid misleading claims of quantum advantage. Finally, we discuss the problem of trainability and data-loading in quantum machine learning. This work advocates for a balanced perspective on quantum computing in genomics, guiding future research toward targeted applications and robust validation.",Aurora Maurizio; Guglielmo Mazzola,,2025-07-05T17:41:10Z,http://arxiv.org/abs/2507.04111v1
2508.02061v1,A Bayesian approach to model uncertainty in single-cell genomic data,"Network models provide a powerful framework for analysing single-cell count data, facilitating the characterisation of cellular identities, disease mechanisms, and developmental trajectories. However, uncertainty modeling in unsupervised learning with genomic data remains insufficiently explored. Conventional clustering methods assign a singular identity to each cell, potentially obscuring transitional states during differentiation or mutation. This study introduces a variational Bayesian framework for clustering and analysing single-cell genomic data, employing a Bayesian Gaussian mixture model to estimate the probabilistic association of cells with distinct clusters. This approach captures cellular transitions, yielding biologically coherent insights into neurogenesis and breast cancer progression. The inferred clustering probabilities enable further analyses, including Differential Expression Analysis and pseudotime analysis. Furthermore, we propose utilising the misclustering rate and Area Under the Curve in clustering scRNA-seq data as an innovative metric to quantitatively evaluate overall clustering performance. This methodological advancement enhances the resolution of single-cell data analysis, enabling a more nuanced characterisation of dynamic cellular identities in development and disease.",Shanshan Ren; Thomas E. Bartlett; Lina Gerontogianni; Swati Chandna,,2025-08-04T05:00:15Z,http://arxiv.org/abs/2508.02061v1
2508.07127v1,How Effectively Can Large Language Models Connect SNP Variants and ECG   Phenotypes for Cardiovascular Risk Prediction?,"Cardiovascular disease (CVD) prediction remains a tremendous challenge due to its multifactorial etiology and global burden of morbidity and mortality. Despite the growing availability of genomic and electrophysiological data, extracting biologically meaningful insights from such high-dimensional, noisy, and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has been applied effectively to predict structural variations in biological sequences. In this work, we explore the potential of fine-tuned LLMs to predict cardiac diseases and SNPs potentially leading to CVD risk using genetic markers derived from high-throughput genomic profiling. We investigate the effect of genetic patterns associated with cardiac conditions and evaluate how LLMs can learn latent biological relationships from structured and semi-structured genomic data obtained by mapping genetic aspects that are inherited from the family tree. By framing the problem as a Chain of Thought (CoT) reasoning task, the models are prompted to generate disease labels and articulate informed clinical deductions across diverse patient profiles and phenotypes. The findings highlight the promise of LLMs in contributing to early detection, risk assessment, and ultimately, the advancement of personalized medicine in cardiac care.",Niranjana Arun Menon; Iqra Farooq; Yulong Li; Sara Ahmed; Yutong Xie; Muhammad Awais; Imran Razzak,,2025-08-10T00:19:29Z,http://arxiv.org/abs/2508.07127v1
2508.21806v1,Suppression of errors in collectively coded information,"Modern life largely transmits genetic information from mother to daughter through the duplication of single physically intact molecules that encode information. However, copying an extended molecule requires highly processive copying machinery and high fidelity that scales with the genome size to avoid the error catastrophe. Here, we explore these fidelity requirements in an alternative architecture, the virtual circular genome, in which no one physical molecule encodes the full genetic information. Instead, information is encoded and transmitted in a collective of overlapping and interacting segments. Using a model experimental system of a complex mixture of DNA oligos that can partly anneal and extend off each other, we find that mutant oligomers are suppressed relative to a model without collective encoding. Through simulations and theory, we show that this suppression of mutants can be explained by competition for productive binding partners. As a consequence, information can be propagated robustly in a virtual circular genome even if the mutation rate is above the error catastrophe for a physically intact genome.",Martin J. Falk; Leon Zhou; Yoshiya J. Matsubara; Kabir Husain; Jack W. Szostak; Arvind Murugan,,2025-08-29T17:39:29Z,http://arxiv.org/abs/2508.21806v1
2503.20964v2,Active Hydrodynamic Theory of Euchromatin and Heterochromatin,"The genome contains genetic information essential for cell's life. The genome's spatial organization inside the cell nucleus is critical for its proper function including gene regulation. The two major genomic compartments -- euchromatin and heterochromatin -- contain largely transcriptionally active and silenced genes, respectively, and exhibit distinct dynamics. In this work, we present a hydrodynamic framework that describes the large-scale behavior of euchromatin and heterochromatin, and accounts for the interplay of mechanical forces, active processes, and nuclear confinement. Our model shows contractile stresses from cross-linking proteins lead to the formation of heterochromatin droplets via mechanically driven phase separation. These droplets grow, coalesce, and in nuclear confinement, wet the boundary. Active processes, such as gene transcription in euchromatin, introduce non-equilibrium fluctuations that drive long-range, coherent motions of chromatin as well as the nucleoplasm, and thus alter the genome's spatial organization. These fluctuations also indirectly deform heterochromatin droplets, by continuously changing their shape. Taken together, our findings reveal how active forces, mechanical stresses and hydrodynamic flows contribute to the genome's organization at large scales and provide a physical framework for understanding chromatin organization and dynamics in live cells.",S. Alex Rautu; Alexandra Zidovska; David Saintillan; Michael J. Shelley,,2025-03-26T20:07:13Z,http://arxiv.org/abs/2503.20964v2
2504.15934v1,Real-time raw signal genomic analysis using fully integrated memristor   hardware,"Advances in third-generation sequencing have enabled portable and real-time genomic sequencing, but real-time data processing remains a bottleneck, hampering on-site genomic analysis due to prohibitive time and energy costs. These technologies generate a massive amount of noisy analog signals that traditionally require basecalling and digital mapping, both demanding frequent and costly data movement on von Neumann hardware. To overcome these challenges, we present a memristor-based hardware-software co-design that processes raw sequencer signals directly in analog memory, effectively combining the separated basecalling and read mapping steps. Here we demonstrate, for the first time, end-to-end memristor-based genomic analysis in a fully integrated memristor chip. By exploiting intrinsic device noise for locality-sensitive hashing and implementing parallel approximate searches in content-addressable memory, we experimentally showcase on-site applications including infectious disease detection and metagenomic classification. Our experimentally-validated analysis confirms the effectiveness of this approach on real-world tasks, achieving a state-of-the-art 97.15% F1 score in virus raw signal mapping, with 51x speed up and 477x energy saving compared to implementation on a state-of-the-art ASIC. These results demonstrate that memristor-based in-memory computing provides a viable solution for integration with portable sequencers, enabling truly real-time on-site genomic analysis for applications ranging from pathogen surveillance to microbial community profiling.",Peiyi He; Shengbo Wang; Ruibin Mao; Sebastian Siegel; Giacomo Pedretti; Jim Ignowski; John Paul Strachan; Ruibang Luo; Can Li,,2025-04-22T14:22:34Z,http://arxiv.org/abs/2504.15934v1
2504.20328v1,Mantodea phylogenomics provides new insights into X-chromosome   progression and evolutionary radiation,"Background: Praying mantises, members of the order Mantodea, play important roles in agriculture, medicine, bionics, and entertainment. However, the scarcity of genomic resources has hindered extensive studies on mantis evolution and behaviour. Results: Here, we present the chromosome-scale reference genomes of five mantis species: the European mantis (Mantis religiosa), Chinese mantis (Tenodera sinensis), triangle dead leaf mantis (Deroplatys truncata), orchid mantis (Hymenopus coronatus), and metallic mantis (Metallyticus violaceus). We found that transposable element expansion is the major force governing genome size in Mantodea. Based on whole-alignments, we deduced that the Mantodea ancestor may have had only one X chromosome and that translocations between the X chromosome and an autosome may have occurred in the lineage of the superfamily Mantoidea. Furthermore, we found a lower evolutionary rate for the metallic mantis than for the other mantises. We also found that Mantodea underwent rapid radiation after the K-Pg mass extinction event, which could have contributed to the confusion in species classification. Conclusions: We present the chromosome-scale reference genomes of five mantis species to reveal the X-chromosome evolution, clarify the phylogeny relationship, and transposable element expansion.",Hangwei Liu; Lihong Lei; Fan Jiang; Bo Zhang; Hengchao Wang; Yutong Zhang; Anqi Wang; Hanbo Zhao; Guirong Wang; Wei Fan,,2025-04-29T00:36:14Z,http://arxiv.org/abs/2504.20328v1
2505.10017v1,Data mining of public genomic repositories: harnessing off-target reads   to expand microbial pathogen genomic resources,"As sequencing technologies become more affordable and genomic databases expand continuously, the reuse of publicly available sequencing data emerges as a powerful strategy for studying microbial pathogens. Indeed, raw sequencing reads generated for the study of a given organism often contain reads originating from the associated microbiota. This review explores how such off-target reads can be detected and used for the study of microbial pathogens. We present genomic data mining as a method to identify relevant sequencing runs from petabase-scale databases, highlighting recent methodological advances that allow efficient database querying. We then briefly outline methods designed to retrieve relevant data and associated metadata, and provide an overview of common downstream analysis pipelines. We discuss how such approaches have (i) expanded the known genetic diversity of microbial pathogens, (ii) enriched our understanding of their spatiotemporal distribution, and (iii) highlighted previously unrecognized ecological interactions involving microbial pathogens. However, these analyses often rely on the completeness and accuracy of accompanying metadata, which remain highly variable. We detail common pitfalls, including data contamination and metadata misannotations, and suggest strategies for result interpretation. Ultimately, while data mining cannot replace dedicated studies, it constitutes an essential and complementary tool for microbial pathogen research. Broader utility will depend on improved data standardization and systematic genomic monitoring across ecosystems.",Damien Richard; Nils Poulicard,UMR PHIM; UMR PHIM,2025-05-15T06:59:18Z,http://arxiv.org/abs/2505.10017v1
2507.07761v1,Widespread remote introgression in the grass genomes,"Genetic transfers are pervasive across both prokaryotes and eukaryotes, encompassing canonical genomic introgression between species or genera and horizontal gene transfer (HGT) across kingdoms. However, DNA transfer between phylogenetically distant species, here defined as remote introgression (RI), has remained poorly explored in evolutionary genomics. In this study, we present RIFinder, a novel phylogeny-based method for RI event detection, and apply it to a comprehensive dataset of 122 grass genomes. Our analysis identifies 622 RI events originating from 543 distinct homologous genes, revealing distinct characteristics among grass subfamilies. Specifically, the subfamily Pooideae exhibits the highest number of introgressed genes while Bambusoideae contains the lowest. Comparisons among accepted genes, their donor copies and native homologs demonstrate that introgressed genes undergo post-transfer localized adaptation, with significant functional enrichment in stress-response pathways. Notably, we identify a large Triticeae-derived segment in a Chloridoideae species Cleistogenes songorica, which is potentially associated with its exceptional drought tolerance. Furthermore, we provide compelling evidence that RI has contributed to the origin and diversification of biosynthetic gene clusters of gramine, a defensive alkaloid chemical, across grass species. Collectively, our study establishes a robust method for RI detection and highlights its critical role in adaptive evolution.",Yujie Huang; Shiyu Zhang; Hanyang Lin; Chenxu Liu; Zhefu Li; Kun Yang; Yutong Liu; Linfeng Jin; Chuanlong Lu; Yuan Cheng; Chaoyi Hu; Huifang Zhao; Guoping Zhang; Qian Qian; Longjiang Fan; Dongya Wu,,2025-07-10T13:37:42Z,http://arxiv.org/abs/2507.07761v1
2507.08542v1,CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA   Splice Site Detection and Pairing in Plant Genomes,"Circular RNAs (circRNAs) are important components of the non-coding RNA regulatory network. Previous circRNA identification primarily relies on high-throughput RNA sequencing (RNA-seq) data combined with alignment-based algorithms that detect back-splicing signals. However, these methods face several limitations: they can't predict circRNAs directly from genomic DNA sequences and relies heavily on RNA experimental data; they involve high computational costs due to complex alignment and filtering steps; and they are inefficient for large-scale or genome-wide circRNA prediction. The challenge is even greater in plants, where plant circRNA splice sites often lack the canonical GT-AG motif seen in human mRNA splicing, and no efficient deep learning model with strong generalization capability currently exists. Furthermore, the number of currently identified plant circRNAs is likely far lower than their true abundance. In this paper, we propose a deep learning framework named CircFormerMoE based on transformers and mixture-of experts for predicting circRNAs directly from plant genomic DNA. Our framework consists of two subtasks known as splicing site detection (SSD) and splicing site pairing (SSP). The model's effectiveness has been validated on gene data of 10 plant species. Trained on known circRNA instances, it is also capable of discovering previously unannotated circRNAs. In addition, we performed interpretability analyses on the trained model to investigate the sequence patterns contributing to its predictions. Our framework provides a fast and accurate computational method and tool for large-scale circRNA discovery in plants, laying a foundation for future research in plant functional genomics and non-coding RNA annotation.",Tianyou Jiang,,2025-07-11T12:43:17Z,http://arxiv.org/abs/2507.08542v1
2509.01020v1,"GeneTEK: Low-power, high-performance and scalable genome sequence   matching in FPGAs","The advent of next-generation sequencing (NGS) has revolutionized genomic research by enabling high-throughput data generation through parallel sequencing of a diverse range of organisms at significantly reduced costs. This breakthrough has unleashed a ""Cambrian explosion"" in genomic data volume and diversity. This volume of workloads places genomics among the top four big data challenges anticipated for this decade. In this context, pairwise sequence alignment represents a very time- and energy-consuming step in common bioinformatics pipelines. Speeding up this step requires the implementation of heuristic approaches, optimized algorithms, and/or hardware acceleration.   Whereas state-of-the-art CPU and GPU implementations have demonstrated significant performance gains, recent field programmable gate array (FPGA) implementations have shown improved energy efficiency. However, the latter often suffer from limited scalability due to constraints on hardware resources when aligning longer sequences. In this work, we present a scalable and flexible FPGA-based accelerator template that implements Myers's algorithm using high-level synthesis and a worker-based architecture. GeneTEK, an instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA, outperforms state-of-the-art CPU and GPU implementations in both speed and energy efficiency, while overcoming scalability limitations of current FPGA approaches. Specifically, GeneTEK achieves at least a 19.4% increase in execution speed and up to 62x reduction in energy consumption compared to leading CPU and GPU solutions, while fitting comparison matrices up to 72% larger compared to previous FPGA solutions. These results reaffirm the potential of FPGAs as an energy-efficient platform for scalable genomic workloads.",Elena Espinosa; Rubén Rodríguez Álvarez; José Miranda; Rafael Larrosa; Miguel Peón-Quirós; Oscar Plata; David Atienza,,2025-08-31T23:11:48Z,http://arxiv.org/abs/2509.01020v1
2501.04822v1,Curated loci prime editing (cliPE) for accessible multiplexed assays of   variant effect (MAVEs),"Multiplexed assays of variant effect (MAVEs) perform simultaneous characterization of many variants. Prime editing has been recently adopted for introducing many variants in their native genomic contexts. However, robust protocols and standards are limited, preventing widespread uptake. Herein, we describe curated loci prime editing (cliPE) which is an accessible, low-cost experimental pipeline to perform MAVEs using prime editing of a target gene, as well as a companion Shiny app (pegRNA Designer) to rapidly and easily design user-specific MAVE libraries.",Carina G Biar; Nicholas Bodkin; Gemma L Carvill; Jeffrey D Calhoun,,2025-01-08T20:21:40Z,http://arxiv.org/abs/2501.04822v1
2503.04490v2,Large Language Models in Bioinformatics: A Survey,"Large Language Models (LLMs) are revolutionizing bioinformatics, enabling advanced analysis of DNA, RNA, proteins, and single-cell data. This survey provides a systematic review of recent advancements, focusing on genomic sequence modeling, RNA structure prediction, protein function inference, and single-cell transcriptomics. Meanwhile, we also discuss several key challenges, including data scarcity, computational complexity, and cross-omics integration, and explore future directions such as multimodal learning, hybrid AI models, and clinical applications. By offering a comprehensive perspective, this paper underscores the transformative potential of LLMs in driving innovations in bioinformatics and precision medicine.",Zhenyu Wang; Zikang Wang; Jiyue Jiang; Pengan Chen; Xiangyu Shi; Yu Li,,2025-03-06T14:38:20Z,http://arxiv.org/abs/2503.04490v2
2505.07740v1,Pan-genome Analysis of Plastomes from Lamiales using PGR-TK,"Chloroplast sequences from the Lamiales order were analyzed using the Pangenome Research Toolkit (PGR-TK). Overall, most genera and families exhibited a high degree of sequence uniformity. However, at the genus level, Utricularia, Incarvillea, and Orobanche stood out as particularly divergent. At the family level, Orobanchaceae, Bignoniaceae and Lentibulariaceae displayed notably complex patterns in the generated plots. The PGR-TK algorithm successfully distinguished most genera within their respective families and often recognized misclassified plants.",Aadhavan Veerendra; Manoj Samanta,,2025-05-12T16:49:38Z,http://arxiv.org/abs/2505.07740v1
2505.11610v1,Foundation Models for AI-Enabled Biological Design,"This paper surveys foundation models for AI-enabled biological design, focusing on recent developments in applying large-scale, self-supervised models to tasks such as protein engineering, small molecule design, and genomic sequence design. Though this domain is evolving rapidly, this survey presents and discusses a taxonomy of current models and methods. The focus is on challenges and solutions in adapting these models for biological applications, including biological sequence modeling architectures, controllability in generation, and multi-modal integration. The survey concludes with a discussion of open problems and future directions, offering concrete next-steps to improve the quality of biological sequence generation.",Asher Moldwin; Amarda Shehu,,2025-05-16T18:17:37Z,http://arxiv.org/abs/2505.11610v1
2508.10058v1,A Structural Analysis of Population Graphs,"The format of graphing algorithms for genomic data has been a debate in recent biotechnology. In this paper, we discuss the construction of population graphs using said genomic data. We first examine the GENPOFAD distance measurement, developed by Joly et. al., and prove that this constitutes a metric function. We develop an algorithm to construct graphs to visualize the relationships between individuals in a population. We then provide a statistical analysis of these simulated population graphs, and show that they are distinct from randomly generated graphs, and also show differences from small-world graphs.",Kimberly Ayers; Maxwell Kooiker,,2025-08-12T21:51:08Z,http://arxiv.org/abs/2508.10058v1
2501.13366v1,Computationally Efficient Whole-Genome Signal Region Detection for   Quantitative and Binary Traits,"The identification of genetic signal regions in the human genome is critical for understanding the genetic architecture of complex traits and diseases. Numerous methods based on scan algorithms (i.e. QSCAN, SCANG, SCANG-STARR) have been developed to allow dynamic window sizes in whole-genome association studies. Beyond scan algorithms, we have recently developed the binary and re-search (BiRS) algorithm, which is more computationally efficient than scan-based methods and exhibits superior statistical power. However, the BiRS algorithm is based on two-sample mean test for binary traits, not accounting for multidimensional covariates or handling test statistics for non-binary outcomes. In this work, we present a distributed version of the BiRS algorithm (dBiRS) that incorporate a new infinity-norm test statistic based on summary statistics computed from a generalized linear model. The dBiRS algorithm accommodates regression-based statistics, allowing for the adjustment of covariates and the testing of both continuous and binary outcomes. This new framework enables parallel computing of block-wise results by aggregation through a central machine to ensure both detection accuracy and computational efficiency, and has theoretical guarantees for controlling family-wise error rates and false discovery rates while maintaining the power advantages of the original algorithm. Applying dBiRS to detect genetic regions associated with fluid intelligence and prospective memory using whole-exome sequencing data from the UK Biobank, we validate previous findings and identify numerous novel rare variants near newly implicated genes. These discoveries offer valuable insights into the genetic basis of cognitive performance and neurodegenerative disorders, highlighting the potential of dBiRS as a scalable and powerful tool for whole-genome signal region detection.",Wei Zhang; Fan Wang; Fang Yao,,2025-01-23T04:11:35Z,http://arxiv.org/abs/2501.13366v1
2501.15472v1,GiantHunter: Accurate detection of giant virus in metagenomic data using   reinforcement-learning and Monte Carlo tree search,"Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for their large genomes and extensive gene repertoires, which contribute to their widespread environmental presence and critical roles in processes such as host metabolic reprogramming and nutrient cycling. Metagenomic sequencing has emerged as a powerful tool for uncovering novel NCLDVs in environmental samples. However, identifying NCLDV sequences in metagenomic data remains challenging due to their high genomic diversity, limited reference genomes, and shared regions with other microbes. Existing alignment-based and machine learning methods struggle with achieving optimal trade-offs between sensitivity and precision. Results: In this work, we present GiantHunter, a reinforcement learning-based tool for identifying NCLDVs from metagenomic data. By employing a Monte Carlo tree search strategy, GiantHunter dynamically selects representative non-NCLDV sequences as the negative training data, enabling the model to establish a robust decision boundary. Benchmarking on rigorously designed experiments shows that GiantHunter achieves high precision while maintaining competitive sensitivity, improving the F1-score by 10% and reducing computational cost by 90% compared to the second-best method. To demonstrate its real-world utility, we applied GiantHunter to 60 metagenomic datasets collected from six cities along the Yangtze River, located both upstream and downstream of the Three Gorges Dam. The results reveal significant differences in NCLDV diversity correlated with proximity to the dam, likely influenced by reduced flow velocity caused by the dam. These findings highlight the potential of GiantSeeker to advance our understanding of NCLDVs and their ecological roles in diverse environments.",Fuchuan Qu; Cheng Peng; Jiaojiao Guan; Donglin Wang; Yanni Sun; Jiayu Shang,,2025-01-26T10:19:54Z,http://arxiv.org/abs/2501.15472v1
2503.09711v2,Genome evolution in an endangered freshwater mussel,"Nearly neutral theory predicts that evolutionary processes will differ in small populations compared to large populations, a key point of concern for endangered species. The nearly-neutral threshold, the span of neutral variation, and the adaptive potential from new mutations all differ depending on N_e. To determine how genomes respond in small populations, we have created a reference genome for a US federally endangered IUCN Red List freshwater mussel, Elliptio spinosa, and compare it to genetic variation for a common and successful relative, Elliptio crassidens. We find higher rates of background duplication rates in E. spinosa consistent with proposed theories of duplicate gene accumulation according to nearly-neutral processes. Along with these changes we observe fewer cases of adaptive gene family amplification in this endangered species. However, TE content is not consistent with nearly-neutral theory. We observe substantially less recent TE proliferation in the endangered species with over 500 Mb of newly copied TEs in Elliptio crassidens. These results suggest a more complex interplay between TEs and duplicate genes than previously proposed for small populations. They further suggest that TEs and duplications require greater attention in surveys of genomic health for endangered species.",Rebekah L. Rogers; John P. Wares; Jeffrey T. Garner,,2025-03-12T18:04:34Z,http://arxiv.org/abs/2503.09711v2
2503.11180v1,"Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction   for Rice Breeding with Small, Structured Datasets","Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding, enabling the identification of superior genotypes based on genomic data. Rice (Oryza sativa), one of the most important staple crops, faces challenges in improving yield and resilience due to the complex genetic architecture of agronomic traits and the limited sample size in breeding datasets. Current G2P prediction methods, such as GWAS and linear models, often fail to capture complex non-linear relationships between genotypes and phenotypes, leading to suboptimal prediction accuracy. Additionally, population stratification and overfitting are significant obstacles when models are applied to small datasets with diverse genetic backgrounds. This study introduces the Learnable Group Transform (LGT) method, which aims to overcome these challenges by combining the advantages of traditional linear models with advanced machine learning techniques. LGT utilizes a group-based transformation of genotype data to capture spatial relationships and genetic structures across diverse rice populations, offering flexibility to generalize even with limited data. Through extensive experiments on the Rice529 dataset, a panel of 529 rice accessions, LGT demonstrated substantial improvements in prediction accuracy for multiple agronomic traits, including yield and plant height, compared to state-of-the-art baselines such as linear models and recent deep learning approaches. Notably, LGT achieved an R^2 improvement of up to 15\% for yield prediction, significantly reducing error and demonstrating its ability to extract meaningful signals from high-dimensional, noisy genomic data. These results highlight the potential of LGT as a powerful tool for genomic prediction in rice breeding, offering a promising solution for accelerating the identification of high-yielding and resilient rice varieties.",Yunxuan Dong; Siyuan Chen; Jisen Zhang,,2025-03-14T08:27:19Z,http://arxiv.org/abs/2503.11180v1
2504.07298v1,CiMBA: Accelerating Genome Sequencing through On-Device Basecalling via   Compute-in-Memory,"As genome sequencing is finding utility in a wide variety of domains beyond the confines of traditional medical settings, its computational pipeline faces two significant challenges. First, the creation of up to 0.5 GB of data per minute imposes substantial communication and storage overheads. Second, the sequencing pipeline is bottlenecked at the basecalling step, consuming >40% of genome analysis time. A range of proposals have attempted to address these challenges, with limited success. We propose to address these challenges with a Compute-in-Memory Basecalling Accelerator (CiMBA), the first embedded ($\sim25$mm$^2$) accelerator capable of real-time, on-device basecalling, coupled with AnaLog (AL)-Dorado, a new family of analog focused basecalling DNNs. Our resulting hardware/software co-design greatly reduces data communication overhead, is capable of a throughput of 4.77 million bases per second, 24x that required for real-time operation, and achieves 17x/27x power/area efficiency over the best prior basecalling embedded accelerator while maintaining a high accuracy comparable to state-of-the-art software basecallers.",William Andrew Simon; Irem Boybat; Riselda Kodra; Elena Ferro; Gagandeep Singh; Mohammed Alser; Shubham Jain; Hsinyu Tsai; Geoffrey W. Burr; Onur Mutlu; Abu Sebastian,,2025-04-09T21:40:46Z,http://arxiv.org/abs/2504.07298v1
2505.12638v2,ChromFound: Towards A Universal Foundation Model for Single-Cell   Chromatin Accessibility Data,"The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present ChromFound, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.",Yifeng Jiao; Yuchen Liu; Yu Zhang; Xin Guo; Yushuai Wu; Chen Jiang; Jiyang Li; Hongwei Zhang; Limei Han; Xin Gao; Yuan Qi; Yuan Cheng,,2025-05-19T02:45:42Z,http://arxiv.org/abs/2505.12638v2
2505.13503v1,Ancestry-Adjusted Polygenic Risk Scores for Predicting Obesity Risk in   the Indonesian Population,"Obesity prevalence in Indonesian adults increased from 10.5% in 2007 to 23.4% in 2023. Studies showed that genetic predisposition significantly influences obesity susceptibility. To aid this, polygenic risk scores (PRS) help aggregate the effects of numerous genetic variants to assess genetic risk. However, 91% of genome-wide association studies (GWAS) involve European populations, limiting their applicability to Indonesians due to genetic diversity. This study aims to develop and validate an ancestry adjusted PRS for obesity in the Indonesian population using principal component analysis (PCA) method constructed from the 1000 Genomes Project data and our own genomic data from approximately 2,800 Indonesians. We calculate PRS for obesity using all races, then determine the first four principal components using ancestry-informative SNPs and develop a linear regression model to predict PRS based on these principal components. The raw PRS is adjusted by subtracting the predicted score to obtain an ancestry adjusted PRS for the Indonesian population. Our results indicate that the ancestry-adjusted PRS improves obesity risk prediction. Compared to the unadjusted PRS, the adjusted score improved classification performance with a 5% increase in area under the ROC curve (AUC). This approach underscores the importance of population-specific adjustments in genetic risk assessments to enable more effective personalized healthcare and targeted intervention strategies for diverse populations.",Jocelyn Verna Siswanto; Belinda Mutiara; Felicia Austin; Jonathan Susanto; Cathelyn Theophila Tan; Restu Unggul Kresnadi; Kezia Irene,,2025-05-16T09:06:17Z,http://arxiv.org/abs/2505.13503v1
2505.17257v3,JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model,"Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA",Qihao Duan; Bingding Huang; Zhenqiao Song; Irina Lehmann; Lei Gu; Roland Eils; Benjamin Wild,,2025-05-22T20:10:55Z,http://arxiv.org/abs/2505.17257v3
2505.20836v1,HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic   Sequence Modeling,"Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.",Hexiong Yang; Mingrui Chen; Huaibo Huang; Junxian Duan; Jie Cao; Zhen Zhou; Ran He,,2025-05-27T07:57:35Z,http://arxiv.org/abs/2505.20836v1
2506.19598v2,Training Flexible Models of Genetic Variant Effects from Functional   Annotations using Accelerated Linear Algebra,"To understand how genetic variants in human genomes manifest in phenotypes -- traits like height or diseases like asthma -- geneticists have sequenced and measured hundreds of thousands of individuals. Geneticists use this data to build models that predict how a genetic variant impacts phenotype given genomic features of the variant, like DNA accessibility or the presence of nearby DNA-bound proteins. As more data and features become available, one might expect predictive models to improve. Unfortunately, training these models is bottlenecked by the need to solve expensive linear algebra problems because variants in the genome are correlated with nearby variants, requiring inversion of large matrices. Previous methods have therefore been restricted to fitting small models, and fitting simplified summary statistics, rather than the full likelihood of the statistical model. In this paper, we leverage modern fast linear algebra techniques to develop DeepWAS (Deep genome Wide Association Studies), a method to train large and flexible neural network predictive models to optimize likelihood. Notably, we find that larger models only improve performance when using our full likelihood approach; when trained by fitting traditional summary statistics, larger models perform no better than small ones. We find larger models trained on more features make better predictions, potentially improving disease predictions and therapeutic target identification.",Alan N. Amin; Andres Potapczynski; Andrew Gordon Wilson,,2025-06-24T13:07:45Z,http://arxiv.org/abs/2506.19598v2
2508.04757v1,Embedding Is (Almost) All You Need: Retrieval-Augmented Inference for   Generalizable Genomic Prediction Tasks,"Large pre-trained DNA language models such as DNABERT-2, Nucleotide Transformer, and HyenaDNA have demonstrated strong performance on various genomic benchmarks. However, most applications rely on expensive fine-tuning, which works best when the training and test data share a similar distribution. In this work, we investigate whether task-specific fine-tuning is always necessary. We show that simple embedding-based pipelines that extract fixed representations from these models and feed them into lightweight classifiers can achieve competitive performance. In evaluation settings with different data distributions, embedding-based methods often outperform fine-tuning while reducing inference time by 10x to 20x. Our results suggest that embedding extraction is not only a strong baseline but also a more generalizable and efficient alternative to fine-tuning, especially for deployment in diverse or unseen genomic contexts. For example, in enhancer classification, HyenaDNA embeddings combined with zCurve achieve 0.68 accuracy (vs. 0.58 for fine-tuning), with an 88% reduction in inference time and over 8x lower carbon emissions (0.02 kg vs. 0.17 kg CO2). In non-TATA promoter classification, DNABERT-2 embeddings with zCurve or GC content reach 0.85 accuracy (vs. 0.89 with fine-tuning) with a 22x lower carbon footprint (0.02 kg vs. 0.44 kg CO2). These results show that embedding-based pipelines offer over 10x better carbon efficiency while maintaining strong predictive performance. The code is available here: https://github.com/NIRJHOR-DATTA/EMBEDDING-IS-ALMOST-ALL-YOU-NEED.",Nirjhor Datta; Swakkhar Shatabda; M Sohel Rahman,,2025-08-06T14:15:48Z,http://arxiv.org/abs/2508.04757v1
2508.14934v1,AGP: A Novel Arabidopsis thaliana Genomics-Phenomics Dataset and its   HyperGraph Baseline Benchmarking,"Understanding which genes control which traits in an organism remains one of the central challenges in biology. Despite significant advances in data collection technology, our ability to map genes to traits is still limited. This genome-to-phenome (G2P) challenge spans several problem domains, including plant breeding, and requires models capable of reasoning over high-dimensional, heterogeneous, and biologically structured data. Currently, however, many datasets solely capture genetic information or solely capture phenotype information. Additionally, phenotype data is very heterogeneous, which many datasets do not fully capture. The critical drawback is that these datasets are not integrated, that is, they do not link with each other to describe the same biological specimens. This limits machine learning models' ability to be informed on the various aspects of these specimens, impacting the breadth of correlations learned, and therefore their ability to make more accurate predictions. To address this gap, we present the Arabidopsis Genomics-Phenomics (AGP) Dataset, a curated multi-modal dataset linking gene expression profiles with phenotypic trait measurements in Arabidopsis thaliana, a model organism in plant biology. AGP supports tasks such as phenotype prediction and interpretable graph learning. In addition, we benchmark conventional regression and explanatory baselines, including a biologically-informed hypergraph baseline, to validate gene-trait associations. To the best of our knowledge, this is the first dataset that provides multi-modal gene information and heterogeneous trait or phenotype data for the same Arabidopsis thaliana specimens. With AGP, we aim to foster the research community towards accurately understanding the connection between genotypes and phenotypes using gene information, higher-order gene pairings, and trait data from several sources.",Manuel Serna-Aguilera; Fiona L. Goggin; Aranyak Goswami; Alexander Bucksch; Suxing Liu; Khoa Luu,,2025-08-19T21:21:23Z,http://arxiv.org/abs/2508.14934v1
2502.18758v1,Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear   Features,"Genotype-to-Phenotype prediction can promote advances in modern genomic research and crop improvement, guiding precision breeding and genomic selection. However, high-dimensional nonlinear features often hinder the accuracy of genotype-to-phenotype prediction by increasing computational complexity. The challenge also limits the predictive accuracy of traditional approaches. Therefore, effective solutions are needed to improve the accuracy of genotype-to-phenotype prediction. In our paper, we propose MLFformer. MLFformer is a Transformer-based architecture that incorporates the Fast Attention mechanism and a multilayer perceptron module to handle high-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism is utilized to handle computational complexity and enhance processing efficiency. In addition, the MLP structure further captures high-dimensional nonlinear features. Through experiments, the results show that MLFformer reduces the average MAPE by 7.73% compared to the vanilla Transformer. In univariate and multivariate prediction scenarios, MLFformer achieves the best predictive performance among all compared models.",Zeyuan Zhou; Siyuan Chen; Xinzhang Wu; Jisen Zhang; Yunxuan Dong,,2025-02-26T02:28:18Z,http://arxiv.org/abs/2502.18758v1
2503.14520v1,From de Bruijn graphs to variation graphs-relationships between   pangenome models,"Pangenomes serve as a framework for joint analysis of genomes of related organisms. Several pangenome models were proposed, offering different functionalities, applications provided by available tools, their efficiency etc. Among them, two graph-based models are particularly widely used: variation graphs and de Bruijn graphs. In the current paper we propose an axiomatization of the desirable properties of a graph representation of a collection of strings. We show the relationship between variation graphs satisfying these criteria and de Bruijn graphs. This relationship can be used to efficiently build a variation graph representing a given set of genomes, transfer annotations between both models, compare the results of analyzes based on each model etc.",Adam Cicherski; Norbert Dojer,,2025-03-14T15:23:52Z,http://arxiv.org/abs/2503.14520v1
2504.10330v1,Can genomic analysis actually estimate past population size?,"Genomic data can be used to reconstruct population size over thousands of generations, using a new class of algorithms (SMC methods). These analyses often show a recent decline in $N_e$ (effective size), which at face value implies a conservation or demographic crisis: a population crash and loss of genetic diversity. This interpretation is frequently mistaken. Here we outline how SMC methods work, why they generate this misleading signal, and suggest simple approaches for exploiting the rich information produced by these algorithms. In most species, genomic patterns reflect major changes in the species' range and subdivision over tens or hundreds of thousands of years. Consequently, collaboration between geneticists, palaeoecologists, palaeoclimatologists, and geologists is crucial for evaluating the outputs of SMC algorithms.",Janeesh K. Bansal; Richard A. Nichols,,2025-04-14T15:37:55Z,http://arxiv.org/abs/2504.10330v1
