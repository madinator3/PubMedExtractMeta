ARXIVID,Title,Abstract,Authors,Affiliations,Date_published,URL
2509.06234v1,Minimum-Cost Synthetic Genome Planning: An Algorithmic Framework,"As synthetic genomics scales toward the construction of increasingly larger genomes, computational strategies are needed to address technical feasibility. We introduce an algorithmic framework for the Minimum-Cost Synthetic Genome Planning problem, aiming to identify the most cost-effective strategy to assemble a target genome from a source genome through a combination of reuse, synthesis, and join operations. By comparing dynamic programming and greedy heuristic strategies under diverse cost regimes, we demonstrate how algorithmic choices influence the cost-efficiency of large-scale genome construction. In parallel, solving the Minimum-Cost Synthetic Genome Planning problem can help us better understand genome architecture and evolution. We applied our framework in case studies on viral genomes, including SARS-CoV-2, to examine how source-target genome similarity shapes construction costs. Our analyses revealed that conserved regions such as ORF1ab can be reconstructed cost-effectively from related templates, while highly variable regions such as the S (spike) gene are more reliant on DNA synthesis, highlighting the biological and economic trade-offs of genome design.",Michail Patsakis; Ioannis Mouratidis; Ilias Georgakopoulos-Soares,,2025-09-07T22:46:57Z,http://arxiv.org/abs/2509.06234v1
2505.08918v1,When repeats drive the vocabulary: a Byte-Pair Encoding analysis of T2T   primate genomes,"The emergence of telomere-to-telomere (T2T) genome assemblies has opened new avenues for comparative genomics, yet effective tokenization strategies for genomic sequences remain underexplored. In this pilot study, we apply Byte Pair Encoding (BPE) to nine T2T primate genomes including three human assemblies by training independent BPE tokenizers with a fixed vocabulary of 512,000 tokens using our custom tool, dnaBPE. Our analysis reveals that only 11,569 tokens are shared across all assemblies, while nearly 991,854 tokens are unique to a single genome, indicating a rapid decline in shared vocabulary with increasing assembly comparisons. Moreover, phylogenetic trees derived from token overlap failed to recapitulate established primate relationships, a discrepancy attributed to the disproportionate influence of species-specific high-copy repetitive elements. These findings underscore the dual nature of BPE tokenization: while it effectively compresses repetitive sequences, its sensitivity to high-copy elements limits its utility as a universal tool for comparative genomics. We discuss potential hybrid strategies and repeat-masking approaches to refine genomic tokenization, emphasizing the need for domain-specific adaptations in the development of large-scale genomic language models. The dnaBPE tool used in this study is open-source and available at https://github.com/aglabx/dnaBPE.",Marina Popova; Iaroslav Chelombitko; Aleksey Komissarov,,2025-05-13T19:27:58Z,http://arxiv.org/abs/2505.08918v1
2505.16680v1,Learning Genomic Structure from $k$-mers,"Sequencing a genome to determine an individual's DNA produces an enormous number of short nucleotide subsequences known as reads, which must be reassembled to reconstruct the full genome. We present a method for analyzing this type of data using contrastive learning, in which an encoder model is trained to produce embeddings that cluster together sequences from the same genomic region. The sequential nature of genomic regions is preserved in the form of trajectories through this embedding space. Trained solely to reflect the structure of the genome, the resulting model provides a general representation of $k$-mer sequences, suitable for a range of downstream tasks involving read data. We apply our framework to learn the structure of the $E.\ coli$ genome, and demonstrate its use in simulated ancient DNA (aDNA) read mapping and identification of structural variations. Furthermore, we illustrate the potential of using this type of model for metagenomic species identification. We show how incorporating a domain-specific noise model can enhance embedding robustness, and how a supervised contrastive learning setting can be adopted when a linear reference genome is available, by introducing a distance thresholding parameter $\Gamma$. The model can also be trained fully self-supervised on read data, enabling analysis without the need to construct a full genome assembly using specialized algorithms. Small prediction heads based on a pre-trained embedding are shown to perform on par with BWA-aln, the current gold standard approach for aDNA mapping, in terms of accuracy and runtime for short genomes. Given the method's favorable scaling properties with respect to total genome size, inference using our approach is highly promising for metagenomic applications and for mapping to genomes comparable in size to the human genome.",Filip Thor; Carl Nettelblad,,2025-05-22T13:46:18Z,http://arxiv.org/abs/2505.16680v1
2504.17860v1,"An Integrated Genomics Workflow Tool: Simulating Reads, Evaluating Read   Alignments, and Optimizing Variant Calling Algorithms","Next-generation sequencing (NGS) is a pivotal technique in genome sequencing due to its high throughput, rapid results, cost-effectiveness, and enhanced accuracy. Its significance extends across various domains, playing a crucial role in identifying genetic variations and exploring genomic complexity. NGS finds applications in diverse fields such as clinical genomics, comparative genomics, functional genomics, and metagenomics, contributing substantially to advancements in research, medicine, and scientific disciplines. Within the sphere of genomics data science, the execution of read simulation, mapping, and variant calling holds paramount importance for obtaining precise and dependable results. Given the plethora of tools available for these purposes, each employing distinct methodologies and options, a nuanced understanding of their intricacies becomes imperative for optimization. This research, situated at the intersection of data science and genomics, involves a meticulous assessment of various tools, elucidating their individual strengths and weaknesses through rigorous experimentation and analysis. This comprehensive evaluation has enabled the researchers to pinpoint the most accurate tools, reinforcing the alignment between the established workflow and the demonstrated efficacy of specific tools in the context of genomics data analysis. To meet these requirements, ""VarFind"", an open-source and freely accessible pipeline tool designed to automate the entire process has been introduced (VarFind GitHub repository: https://github.com/shanikawm/varfinder)",Fathima Nuzla Ismail; Shanika Amarasoma,,2025-04-24T18:05:08Z,http://arxiv.org/abs/2504.17860v1
2502.21125v1,Eukaryotes evade information storage-replication rate trade-off with   endosymbiont assistance leading to larger genomes,"Genome length varies widely among organisms, from compact genomes of prokaryotes to vast and complex genomes of eukaryotes. In this study, we theoretically identify the evolutionary pressures that may have driven this divergence in genome length. We use a parameter-free model to study genome length evolution under selection pressure to minimize replication time and maximize information storage capacity. We show that prokaryotes tend to reduce genome length, constrained by a single replication origin, while eukaryotes expand their genomes by incorporating multiple replication origins. We propose a connection between genome length and cellular energetics, suggesting that endosymbiotic organelles, mitochondria and chloroplasts, evolutionarily regulate the number of replication origins, thereby influencing genome length in eukaryotes. We show that the above two selection pressures also lead to strict equalization of the number of purines and their corresponding base-pairing pyrimidines within a single DNA strand, known as Chagraff's second parity rule, a hitherto unexplained observation in genomes of nearly all known species. This arises from the symmetrization of replichore length, another observation that has been shown to hold across species, which our model reproduces. The model also reproduces other experimentally observed phenomena, such as a general preference for deletions over insertions, and elongation and high variance of genome lengths under reduced selection pressure for replication rate, termed the C-value paradox. We highlight the possibility of regulation of the firing of latent replication origins in response to cues from the extracellular environment leading to the regulation of cell cycle rates in multicellular eukaryotes.",Parthasarathi Sahu; Sashikanta Barik; Koushik Ghosh; Hemachander Subramanian,,2025-02-28T15:01:36Z,http://arxiv.org/abs/2502.21125v1
2504.09060v1,Multimodal 3D Genome Pre-training,"Deep learning techniques have driven significant progress in various analytical tasks within 3D genomics in computational biology. However, a holistic understanding of 3D genomics knowledge remains underexplored. Here, we propose MIX-HIC, the first multimodal foundation model of 3D genome that integrates both 3D genome structure and epigenomic tracks, which obtains unified and comprehensive semantics. For accurate heterogeneous semantic fusion, we design the cross-modal interaction and mapping blocks for robust unified representation, yielding the accurate aggregation of 3D genome knowledge. Besides, we introduce the first large-scale dataset comprising over 1 million pairwise samples of Hi-C contact maps and epigenomic tracks for high-quality pre-training, enabling the exploration of functional implications in 3D genomics. Extensive experiments show that MIX-HIC can significantly surpass existing state-of-the-art methods in diverse downstream tasks. This work provides a valuable resource for advancing 3D genomics research.",Minghao Yang; Pengteng Li; Yan Liang; Qianyi Cai; Zhihang Zheng; Shichen Zhang; Pengfei Zhang; Zhi-An Huang; Hui Xiong,,2025-04-12T03:31:03Z,http://arxiv.org/abs/2504.09060v1
2505.07919v1,Revolutionising Bacterial Genomics: Graph-Based Strategies for Improved   Variant Identification,"A significant advancement in bioinformatics is using genome graph techniques to improve variation discovery across organisms. Traditional approaches, such as bwa mem, rely on linear reference genomes for genomic analyses but may introduce biases when applied to highly diverse bacterial genomes of the same species. Pangenome graphs provide an alternative paradigm for evaluating structural and minor variations within a graphical framework, including insertions, deletions, and single nucleotide polymorphisms. Pangenome graphs enhance the detection and interpretation of complex genetic variants by representing the full genetic diversity of a species. In this study, we present a robust and reliable bioinformatics pipeline utilising the PanGenome Graph Builder (PGGB) and the Variation Graph toolbox (vg giraffe) to align whole-genome sequencing data, call variants against a graph reference, and construct pangenomes from assembled genomes. Our results demonstrate that leveraging pangenome graphs over a single linear reference genome significantly improves mapping rates and variant calling accuracy for simulated and actual bacterial pathogens datasets.",Fathima Nuzla Ismail; Abira Sengupta,,2025-05-12T15:34:24Z,http://arxiv.org/abs/2505.07919v1
2505.14402v1,OmniGenBench: A Modular Platform for Reproducible Genomic Foundation   Models Benchmarking,"The code of nature, embedded in DNA and RNA genomes since the origin of life, holds immense potential to impact both humans and ecosystems through genome modeling. Genomic Foundation Models (GFMs) have emerged as a transformative approach to decoding the genome. As GFMs scale up and reshape the landscape of AI-driven genomics, the field faces an urgent need for rigorous and reproducible evaluation. We present OmniGenBench, a modular benchmarking platform designed to unify the data, model, benchmarking, and interpretability layers across GFMs. OmniGenBench enables standardized, one-command evaluation of any GFM across five benchmark suites, with seamless integration of over 31 open-source models. Through automated pipelines and community-extensible features, the platform addresses critical reproducibility challenges, including data transparency, model interoperability, benchmark fragmentation, and black-box interpretability. OmniGenBench aims to serve as foundational infrastructure for reproducible genomic AI research, accelerating trustworthy discovery and collaborative innovation in the era of genome-scale modeling.",Heng Yang; Jack Cole; Yuan Li; Renzhi Chen; Geyong Min; Ke Li,,2025-05-20T14:16:25Z,http://arxiv.org/abs/2505.14402v1
2503.19367v3,VGAT: A Cancer Survival Analysis Framework Transitioning from Generative   Visual Question Answering to Genomic Reconstruction,"Multimodal learning combining pathology images and genomic sequences enhances cancer survival analysis but faces clinical implementation barriers due to limited access to genomic sequencing in under-resourced regions. To enable survival prediction using only whole-slide images (WSI), we propose the Visual-Genomic Answering-Guided Transformer (VGAT), a framework integrating Visual Question Answering (VQA) techniques for genomic modality reconstruction. By adapting VQA's text feature extraction approach, we derive stable genomic representations that circumvent dimensionality challenges in raw genomic data. Simultaneously, a cluster-based visual prompt module selectively enhances discriminative WSI patches, addressing noise from unfiltered image regions. Evaluated across five TCGA datasets, VGAT outperforms existing WSI-only methods, demonstrating the viability of genomic-informed inference without sequencing. This approach bridges multimodal research and clinical feasibility in resource-constrained settings. The code link is https://github.com/CZZZZZZZZZZZZZZZZZ/VGAT.",Zizhi Chen; Minghao Han; Xukun Zhang; Shuwei Ma; Tao Liu; Xing Wei; Lihua Zhang,,2025-03-25T05:48:31Z,http://arxiv.org/abs/2503.19367v3
2503.23691v1,A Conceptual Framework for Human-AI Collaborative Genome Annotation,"Genome annotation is essential for understanding the functional elements within genomes. While automated methods are indispensable for processing large-scale genomic data, they often face challenges in accurately predicting gene structures and functions. Consequently, manual curation by domain experts remains crucial for validating and refining these predictions. These combined outcomes from automated tools and manual curation highlight the importance of integrating human expertise with AI capabilities to improve both the accuracy and efficiency of genome annotation. However, the manual curation process is inherently labor-intensive and time-consuming, making it difficult to scale for large datasets. To address these challenges, we propose a conceptual framework, Human-AI Collaborative Genome Annotation (HAICoGA), which leverages the synergistic partnership between humans and artificial intelligence to enhance human capabilities and accelerate the genome annotation process. Additionally, we explore the potential of integrating Large Language Models (LLMs) into this framework to support and augment specific tasks. Finally, we discuss emerging challenges and outline open research questions to guide further exploration in this area.",Xiaomei Li; Alex Whan; Meredith McNeil; David Starns; Jessica Irons; Samuel C. Andrew; Rad Suchecki,,2025-03-31T03:44:00Z,http://arxiv.org/abs/2503.23691v1
2503.03773v1,A Phylogenetic Approach to Genomic Language Modeling,"Genomic language models (gLMs) have shown mostly modest success in identifying evolutionarily constrained elements in mammalian genomes. To address this issue, we introduce a novel framework for training gLMs that explicitly models nucleotide evolution on phylogenetic trees using multispecies whole-genome alignments. Our approach integrates an alignment into the loss function during training but does not require it for making predictions, thereby enhancing the model's applicability. We applied this framework to train PhyloGPN, a model that excels at predicting functionally disruptive variants from a single sequence alone and demonstrates strong transfer learning capabilities.",Carlos Albors; Jianan Canal Li; Gonzalo Benegas; Chengzhong Ye; Yun S. Song,,2025-03-04T06:53:03Z,http://arxiv.org/abs/2503.03773v1
2509.05539v1,Investigating DNA words and their distributions across the tree of life,"The frequency distributions of DNA k-mers are shaped by fundamental biological processes and offer a window into genome structure and evolution. Inspired by analogies to natural language, prior studies have attempted to model genomic k-mer usage using Zipf's law, a rank-frequency law originally formulated for words in human language. However, the extent to which this law accurately captures the distribution of k-mers across diverse species remains unclear. Here, we systematically analyze k-mer frequency spectra across more than 225,000 genome assemblies spanning all three domains of life and viruses. We demonstrate that Zipf's law consistently underperforms in modeling k-mer distributions. In contrast, we propose the truncated power law and Zipf-Mandelbrot distributions, which provide substantially improved fits across taxonomic groups. We show that genome size and GC content influence model performance, with larger and GC-content imbalanced genomes yielding better adherence. Additionally, we perform an extensive analysis on vocabulary expansion and exhaustion across the same organisms using Heaps' law. We apply our modeling framework to evaluate simulated genomes generated by k-let preserving shuffling and deep generative language models. Our results reveal substantial differences between organismal genomes and their synthetic or shuffled counterparts, offering a novel approach to benchmark the biological plausibility of artificial genomes. Collectively, this work establishes new standards for modeling genomic k-mer distributions and provides insights relevant to synthetic biology, and evolutionary sequence analysis.",Charalampos Koilakos; Kimonas Provatas; Michail Patsakis; Aris Karatzikos; Ilias Georgakopoulos-Soares,,2025-09-05T23:32:30Z,http://arxiv.org/abs/2509.05539v1
2501.11982v1,WinPCA: A package for windowed principal component analysis,"Principal component analysis (PCA) is routinely used in population genetics to assess genetic structure. With chromosomal reference genomes and population-scale whole genome-sequencing becoming increasingly accessible, contemporary studies often include characterizations of the genomic landscape as it varies along chromosomes, commonly termed genome scans. While traditional summary statistics like FST and dXY remain integral to characterizing the genomic divergence profile, PCA fundamentally differs by providing single-sample resolution, thereby making results intuitively interpretable to help identify polymorphic inversions, introgression and other types of divergent sequence. Here, we introduce WinPCA, a user-friendly package to compute, polarize and visualize genetic principal components in windows along the genome. To accommodate low-coverage whole genome-sequencing datasets, WinPCA can optionally make use of PCAngsd methods to compute principal components in a genotype likelihood framework. WinPCA accepts variant data in either VCF or BEAGLE format and can generate rich plots for interactive data exploration and downstream presentation.",L. Moritz Blumer; Jeffrey M. Good; Richard Durbin,,2025-01-21T08:57:45Z,http://arxiv.org/abs/2501.11982v1
2502.16470v1,Bancroft: Genomics Acceleration Beyond On-Device Memory,"This paper presents Bancroft, a computational genomics acceleration platform that provides the illusion of practically infinite on-device memory capacity by compressing genomic data movement over PCIe. Bancroft introduces novel optimizations for efficient accelerator implementation to reference-based genome compression, including fixed-stride matching using cuckoo hashes and grouped header encoding, incorporated into a familiar interface supporting random accesses. We evaluate a prototype implementation of Bancroft on an affordable Alveo U50 FPGA equipped with 8 GB of HBM. Thanks to the orders of magnitude improvements in performance and resource efficiency of genomic compression, our prototype provides access to TBs of host-side genomic data at memory-class performance, measuring speeds over 30% of the on-device HBM bandwidth, an order of magnitude higher than conventional PCIe-limited architectures. Using a real-world pre-alignment filtering application, Bancroft demonstrates over 6x improvement over the conventional PCIe-attached architecture, achieving 30% of peak internal throughput of an accelerator with HBM, and 90% of the one with DDR4. Bancroft supports memory-class performance to practically infinite data capacity, using a small, fixed amount of HBM, making it an attractive solution to continued future scalability of computational genomics.",Se-Min Lim; Seongyoung Kang; Sang-Woo Jun,,2025-02-23T07:07:09Z,http://arxiv.org/abs/2502.16470v1
2504.20034v1,Pan-genome Analysis of Angiosperm Plastomes using PGR-TK,"We present a novel approach for taxonomic analysis of chloroplast genomes in angiosperms using the Pan-genome Research Toolkit (PGR-TK). Comparative plots generated by PGR-TK across diverse angiosperm genera reveal a wide range of structural complexity, from straightforward to highly intricate patterns. Notably, the characteristic quadripartite plastome structure, comprising the large single copy (LSC), small single copy (SSC), and inverted repeat (IR) regions, is clearly identifiable in over 75% of the genera analyzed. Our findings also underscore several occurrences of species mis-annotations in public genomic databases, which are readily detected through visual anomalies in the PGR-TK plots. While more complex plot patterns remain difficult to interpret, they likely reflect underlying biological variation or technical inconsistencies in genome assembly. Overall, this approach effectively integrates classical botanical visualization with modern molecular taxonomy, providing a powerful tool for genome-based classification in plant systematics.",Manoj P. Samanta,,2025-04-28T17:56:13Z,http://arxiv.org/abs/2504.20034v1
2506.01833v1,SPACE: Your Genomic Profile Predictor is a Powerful DNA Foundation Model,"Inspired by the success of unsupervised pre-training paradigms, researchers have applied these approaches to DNA pre-training. However, we argue that these approaches alone yield suboptimal results because pure DNA sequences lack sufficient information, since their functions are regulated by genomic profiles like chromatin accessibility. Here, we demonstrate that supervised training for genomic profile prediction serves as a more effective alternative to pure sequence pre-training. Furthermore, considering the multi-species and multi-profile nature of genomic profile prediction, we introduce our $\textbf{S}$pecies-$\textbf{P}$rofile $\textbf{A}$daptive $\textbf{C}$ollaborative $\textbf{E}$xperts (SPACE) that leverages Mixture of Experts (MoE) to better capture the relationships between DNA sequences across different species and genomic profiles, thereby learning more effective DNA representations. Through extensive experiments across various tasks, our model achieves state-of-the-art performance, establishing that DNA models trained with supervised genomic profiles serve as powerful DNA representation learners. The code is available at https://github.com/ZhuJiwei111/SPACE.",Zhao Yang; Jiwei Zhu; Bing Su,,2025-06-02T16:23:05Z,http://arxiv.org/abs/2506.01833v1
2507.02877v1,AuraGenome: An LLM-Powered Framework for On-the-Fly Reusable and   Scalable Circular Genome Visualizations,"Circular genome visualizations are essential for exploring structural variants and gene regulation. However, existing tools often require complex scripting and manual configuration, making the process time-consuming, error-prone, and difficult to learn. To address these challenges, we introduce AuraGenome, an LLM-powered framework for rapid, reusable, and scalable generation of multi-layered circular genome visualizations. AuraGenome combines a semantic-driven multi-agent workflow with an interactive visual analytics system. The workflow employs seven specialized LLM-driven agents, each assigned distinct roles such as intent recognition, layout planning, and code generation, to transform raw genomic data into tailored visualizations. The system supports multiple coordinated views tailored for genomic data, offering ring, radial, and chord-based layouts to represent multi-layered circular genome visualizations. In addition to enabling interactions and configuration reuse, the system supports real-time refinement and high-quality report export. We validate its effectiveness through two case studies and a comprehensive user study. AuraGenome is available at: https://github.com/Darius18/AuraGenome.",Chi Zhang; Yu Dong; Yang Wang; Yuetong Han; Guihua Shan; Bixia Tang,,2025-06-18T03:29:30Z,http://arxiv.org/abs/2507.02877v1
2503.09496v2,Robust Multimodal Survival Prediction with the Latent Differentiation   Conditional Variational AutoEncoder,"The integrative analysis of histopathological images and genomic data has received increasing attention for survival prediction of human cancers. However, the existing studies always hold the assumption that full modalities are available. As a matter of fact, the cost for collecting genomic data is high, which sometimes makes genomic data unavailable in testing samples. A common way of tackling such incompleteness is to generate the genomic representations from the pathology images. Nevertheless, such strategy still faces the following two challenges: (1) The gigapixel whole slide images (WSIs) are huge and thus hard for representation. (2) It is difficult to generate the genomic embeddings with diverse function categories in a unified generative framework. To address the above challenges, we propose a Conditional Latent Differentiation Variational AutoEncoder (LD-CVAE) for robust multimodal survival prediction, even with missing genomic data. Specifically, a Variational Information Bottleneck Transformer (VIB-Trans) module is proposed to learn compressed pathological representations from the gigapixel WSIs. To generate different functional genomic features, we develop a novel Latent Differentiation Variational AutoEncoder (LD-VAE) to learn the common and specific posteriors for the genomic embeddings with diverse functions. Finally, we use the product-of-experts technique to integrate the genomic common posterior and image posterior for the joint latent distribution estimation in LD-CVAE. We test the effectiveness of our method on five different cancer datasets, and the experimental results demonstrate its superiority in both complete and missing modality scenarios.",Junjie Zhou; Jiao Tang; Yingli Zuo; Peng Wan; Daoqiang Zhang; Wei Shao,,2025-03-12T15:58:37Z,http://arxiv.org/abs/2503.09496v2
2504.03732v2,SAGe: A Lightweight Algorithm-Architecture Co-Design for Mitigating the   Data Preparation Bottleneck in Large-Scale Genome Analysis,"Given the exponentially growing volumes of genomic data, there are extensive efforts to accelerate genome analysis. We demonstrate a major bottleneck that greatly limits and diminishes the benefits of state-of-the-art genome analysis accelerators: the data preparation bottleneck, where genomic data is stored in compressed form and needs to be decompressed and formatted first before an accelerator can operate on it. To mitigate this bottleneck, we propose SAGe, an algorithm-architecture co-design for highly-compressed storage and high-performance access of large-scale genomic data. SAGe overcomes the challenges of mitigating the data preparation bottleneck while maintaining high compression ratios (comparable to genomic-specific compression algorithms) at low hardware cost. This is enabled by leveraging key features of genomic datasets to co-design (i) a new (de)compression algorithm, (ii) hardware, (iii) storage data layout, and (iv) interface commands to access storage. SAGe stores data in structures that can be rapidly interpreted and decompressed by efficient streaming accesses and lightweight hardware. To achieve high compression ratios using only these lightweight structures, SAGe exploits unique features of genomic data. We show that SAGe can be seamlessly integrated with a broad range of genome analysis hardware accelerators to mitigate their data preparation bottlenecks. Our results demonstrate that SAGe improves the average end-to-end performance and energy efficiency of two state-of-the-art genome analysis accelerators by 3.0x-32.1x and 18.8x-49.6x, respectively, compared to when the accelerators rely on state-of-the-art decompression tools.",Nika Mansouri Ghiasi; Talu Güloglu; Harun Mustafa; Can Firtina; Konstantina Koliogeorgi; Konstantinos Kanellopoulos; Haiyu Mao; Rakesh Nadig; Mohammad Sadrosadati; Jisung Park; Onur Mutlu,,2025-03-31T23:36:26Z,http://arxiv.org/abs/2504.03732v2
2507.08060v1,MicroTrace: A Lightweight R Tool for SNP-Based Pathogen Clustering in   Outbreak Detection,"MicroTrace is an open-source R tool that performs SNP-based hierarchical clustering to detect potential transmission clusters from pathogen whole-genome sequencing (WGS) data. Designed for epidemiologists, microbiologists, and genomic surveillance teams, it processes SNP distance matrices and outputs dendrograms and cluster tables with optional metadata integration. MicroTrace enables reproducible outbreak detection workflows with minimal setup.",Kaitao Lai,,2025-07-10T15:41:53Z,http://arxiv.org/abs/2507.08060v1
2506.17766v1,Improving Genomic Models via Task-Specific Self-Pretraining,"Pretraining DNA language models (DNALMs) on the full human genome is resource-intensive, yet often considered necessary for strong downstream performance. Inspired by recent findings in NLP and long-context modeling, we explore an alternative: self-pretraining on task-specific, unlabeled data. Using the BEND benchmark, we show that DNALMs trained with self-pretraining match or exceed the performance of models trained from scratch under identical compute. While genome-scale pretraining may still offer higher absolute performance, task-specific self-pretraining provides a practical and compute-efficient strategy for building stronger supervised baselines.",Sohan Mupparapu; Parameswari Krishnamurthy; Ratish Puduppully,,2025-06-21T17:19:21Z,http://arxiv.org/abs/2506.17766v1
2501.16982v1,"Human Genome Book: Words, Sentences and Paragraphs","Since the completion of the human genome sequencing project in 2001, significant progress has been made in areas such as gene regulation editing and protein structure prediction. However, given the vast amount of genomic data, the segments that can be fully annotated and understood remain relatively limited. If we consider the genome as a book, constructing its equivalents of words, sentences, and paragraphs has been a long-standing and popular research direction. Recently, studies on transfer learning in large language models have provided a novel approach to this challenge.Multilingual transfer ability, which assesses how well models fine-tuned on a source language can be applied to other languages, has been extensively studied in multilingual pre-trained models. Similarly, the transfer of natural language capabilities to ""DNA language"" has also been validated. Building upon these findings, we first trained a foundational model capable of transferring linguistic capabilities from English to DNA sequences. Using this model, we constructed a vocabulary of DNA words and mapped DNA words to their English equivalents.Subsequently, we fine-tuned this model using English datasets for paragraphing and sentence segmentation to develop models capable of segmenting DNA sequences into sentences and paragraphs. Leveraging these models, we processed the GRCh38.p14 human genome by segmenting, tokenizing, and organizing it into a ""book"" comprised of genomic ""words,"" ""sentences,"" and ""paragraphs."" Additionally, based on the DNA-to-English vocabulary mapping, we created an ""English version"" of the genomic book. This study offers a novel perspective for understanding the genome and provides exciting possibilities for developing innovative tools for DNA search, generation, and analysis.",Wang Liang,,2025-01-23T04:39:24Z,http://arxiv.org/abs/2501.16982v1
2503.16565v1,Gene42: Long-Range Genomic Foundation Model With Dense Attention,"We introduce Gene42, a novel family of Genomic Foundation Models (GFMs) designed to manage context lengths of up to 192,000 base pairs (bp) at a single-nucleotide resolution. Gene42 models utilize a decoder-only (LLaMA-style) architecture with a dense self-attention mechanism. Initially trained on fixed-length sequences of 4,096 bp, our models underwent continuous pretraining to extend the context length to 192,000 bp. This iterative extension allowed for the comprehensive processing of large-scale genomic data and the capture of intricate patterns and dependencies within the human genome. Gene42 is the first dense attention model capable of handling such extensive long context lengths in genomics, challenging state-space models that often rely on convolutional operators among other mechanisms. Our pretrained models exhibit notably low perplexity values and high reconstruction accuracy, highlighting their strong ability to model genomic data. Extensive experiments on various genomic benchmarks have demonstrated state-of-the-art performance across multiple tasks, including biotype classification, regulatory region identification, chromatin profiling prediction, variant pathogenicity prediction, and species classification. The models are publicly available at huggingface.co/inceptionai.",Kirill Vishniakov; Boulbaba Ben Amor; Engin Tekin; Nancy A. ElNaker; Karthik Viswanathan; Aleksandr Medvedev; Aahan Singh; Maryam Nadeem; Mohammad Amaan Sayeed; Praveenkumar Kanithi; Tiago Magalhaes; Natalia Vassilieva; Dwarikanath Mahapatra; Marco Pimentel; and Shadab Khan,,2025-03-20T07:10:04Z,http://arxiv.org/abs/2503.16565v1
2504.06304v2,Leveraging State Space Models in Long Range Genomics,"Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.",Matvei Popov; Aymen Kallala; Anirudha Ramesh; Narimane Hennouni; Shivesh Khaitan; Rick Gentry; Alain-Sam Cohen,,2025-04-07T18:34:06Z,http://arxiv.org/abs/2504.06304v2
2506.19097v1,Quantum Gradient Optimized Drug Repurposing Prototype for Omics Data,This paper presents a novel quantum-enhanced prototype for drug repurposing and addresses the challenge of managing massive genomics data in precision medicine.,Don Roosan; Saif Nirzhor; Rubayat Khan; Fahmida Hai,,2025-06-23T20:17:55Z,http://arxiv.org/abs/2506.19097v1
2508.13191v1,NucEL: Single-Nucleotide ELECTRA-Style Genomic Pre-training for   Efficient and Interpretable Representations,"Pre-training large language models on genomic sequences is a powerful approach for learning biologically meaningful representations. Masked language modeling (MLM) methods, such as DNABERT and Nucleotide Transformer (NT), achieve strong performance but suffer from partial token supervision, pre-training/fine-tuning mismatches, and high computational costs. We introduce NucEL, the first ELECTRA-style pre-training framework for genomic foundation models, addressing these limitations. Using a discriminator to identify tokens altered by a generator, NucEL provides comprehensive token-level supervision across all sequence positions, improving efficiency over the partial supervision of MLM. Incorporating ModernBERT's hybrid local-global attention and flash attention, NucEL offers an optimized BERT architecture for genomic modeling. Unlike 6-mer tokenization, NucEL uses single-nucleotide tokens for fine-grained resolution, boosting both efficiency and interpretability. Pre-trained on the human genome, NucEL achieves state-of-the-art results on diverse downstream tasks -- regulatory element identification (e.g., promoters, enhancers), transcription factor binding prediction, open chromatin classification, and histone modification profiling -- surpassing similarly sized MLM-based models and rivaling models 25x larger, such as NT. Ablation studies highlight optimal tokenization and masking strategies for ELECTRA-style DNA pre-training. Attention analysis reveals NucEL's superior capture of biologically relevant motifs compared to NT, providing insights into hierarchical learning and regulatory element modeling. These findings demonstrate ELECTRA-style pre-training as an efficient, effective strategy for genomic representation learning with broad implications for genomic research.",Ke Ding; Brian Parker; Jiayu Wen,,2025-08-15T12:34:51Z,http://arxiv.org/abs/2508.13191v1
2502.15109v4,Social Genome: Grounded Social Reasoning Abilities of Multimodal Models,"Social reasoning abilities are crucial for AI systems to effectively interpret and respond to multimodal human communication and interaction within social contexts. We introduce SOCIAL GENOME, the first benchmark for fine-grained, grounded social reasoning abilities of multimodal models. SOCIAL GENOME contains 272 videos of interactions and 1,486 human-annotated reasoning traces related to inferences about these interactions. These traces contain 5,777 reasoning steps that reference evidence from visual cues, verbal cues, vocal cues, and external knowledge (contextual knowledge external to videos). SOCIAL GENOME is also the first modeling challenge to study external knowledge in social reasoning. SOCIAL GENOME computes metrics to holistically evaluate semantic and structural qualities of model-generated social reasoning traces. We demonstrate the utility of SOCIAL GENOME through experiments with state-of-the-art models, identifying performance gaps and opportunities for future research to improve the grounded social reasoning abilities of multimodal models.",Leena Mathur; Marian Qian; Paul Pu Liang; Louis-Philippe Morency,,2025-02-21T00:05:40Z,http://arxiv.org/abs/2502.15109v4
2503.20451v1,Agptools: a utility suite for editing genome assemblies,"The AGP format is a tab-separated table format describing how components of a genome assembly fit together. A standard submission format for genome assemblies is a fasta file giving the sequence of contigs along with an AGP file showing how these components are assembled into larger pieces like scaffolds or chromosomes. For this reason, many scaffolding software pipelines output assemblies in this format. However, although many programs for assembling and scaffolding genomes read and write this format, there is currently no published software for making edits to AGP files when performing assembly curation. We present agptools, a suite of command-line programs that can perform common operations on AGP files, such as breaking and joining sequences, inverting pieces of assembly components, assembling contigs into larger sequences based on an AGP file, and transforming between coordinate systems of different assembly layouts. Additionally, agptools includes an API that writers of other software packages can use to read, write, and manipulate AGP files within their own programs. Agptools gives bioinformaticians a simple, robust, and reproducible way to edit genome assemblies that avoids the shortfalls of other methods for editing AGP files.",Edward S. Ricemeyer; Rachel A. Carroll; Wesley C. Warren,,2025-03-26T11:26:21Z,http://arxiv.org/abs/2503.20451v1
2506.00082v1,An AI-powered Knowledge Hub for Potato Functional Genomics,"Potato functional genomics lags due to unsystematic gene information curation, gene identifier inconsistencies across reference genome versions, and the increasing volume of research publications. To address these limitations, we developed the Potato Knowledge Hub (http://www.potato-ai.top), leveraging Large Language Models (LLMs) and a systematically curated collection of over 3,200 high-quality potato research papers spanning over 120 years. This platform integrates two key modules: a functional gene database containing 2,571 literature-reported genes, meticulously mapped to the latest DMv8.1 reference genome with resolved nomenclature discrepancies and links to original publications; and a potato knowledge base. The knowledge base, built using a Retrieval-Augmented Generation (RAG) architecture, accurately answers research queries with literature citations, mitigating LLM ""hallucination."" Users can interact with the hub via a natural language AI agent, ""Potato Research Assistant,"" for querying specialized knowledge, retrieving gene information, and extracting sequences. The continuously updated Potato Knowledge Hub aims to be a comprehensive resource, fostering advancements in potato functional genomics and supporting breeding programs.",Jia Yuxin; Li Jinye; Jia Yudong; Li Futing; Su Xiaoqi; Luo Jilin; Dong Yarui; Sun Chunyan; Cui Qinghan; Wang Li; Li Axiu; Shang Yi; Zhu Yujuan; Huang Sanwen,,2025-05-30T03:09:59Z,http://arxiv.org/abs/2506.00082v1
2506.00597v1,Processing-in-memory for genomics workloads,"Low-cost, high-throughput DNA and RNA sequencing (HTS) data is the main workforce for the life sciences. Genome sequencing is now becoming a part of Predictive, Preventive, Personalized, and Participatory (termed 'P4') medicine. All genomic data are currently processed in energy-hungry computer clusters and centers, necessitating data transfer, consuming substantial energy, and wasting valuable time. Therefore, there is a need for fast, energy-efficient, and cost-efficient technologies that enable genomics research without requiring data centers and cloud platforms. We recently started the BioPIM Project to leverage the emerging processing-in-memory (PIM) technologies to enable energy and cost-efficient analysis of bioinformatics workloads. The BioPIM Project focuses on co-designing algorithms and data structures commonly used in genomics with several PIM architectures for the highest cost, energy, and time savings benefit.",William Andrew Simon; Leonid Yavits; Konstantina Koliogeorgi; Yann Falevoz; Yoshihiro Shibuya; Dominique Lavenier; Irem Boybat; Klea Zambaku; Berkan Şahin; Mohammad Sadrosadati; Onur Mutlu; Abu Sebastian; Rayan Chikhi; The BioPIM Consortium; Can Alkan,,2025-05-31T15:13:06Z,http://arxiv.org/abs/2506.00597v1
2506.19324v1,Memory-Augmented Incomplete Multimodal Survival Prediction via   Cross-Slide and Gene-Attentive Hypergraph Learning,"Multimodal pathology-genomic analysis is critical for cancer survival prediction. However, existing approaches predominantly integrate formalin-fixed paraffin-embedded (FFPE) slides with genomic data, while neglecting the availability of other preservation slides, such as Fresh Froze (FF) slides. Moreover, as the high-resolution spatial nature of pathology data tends to dominate the cross-modality fusion process, it hinders effective multimodal fusion and leads to modality imbalance challenges between pathology and genomics. These methods also typically require complete data modalities, limiting their clinical applicability with incomplete modalities, such as missing either pathology or genomic data. In this paper, we propose a multimodal survival prediction framework that leverages hypergraph learning to effectively integrate multi-WSI information and cross-modality interactions between pathology slides and genomics data while addressing modality imbalance. In addition, we introduce a memory mechanism that stores previously learned paired pathology-genomic features and dynamically compensates for incomplete modalities. Experiments on five TCGA datasets demonstrate that our model outperforms advanced methods by over 2.3% in C-Index. Under incomplete modality scenarios, our approach surpasses pathology-only (3.3%) and gene-only models (7.9%). Code: https://github.com/MCPathology/M2Surv",Mingcheng Qu; Guang Yang; Donglin Di; Yue Gao; Tonghua Su; Yang Song; Lei Fan,,2025-06-24T05:31:13Z,http://arxiv.org/abs/2506.19324v1
2507.11950v1,RNAMunin: A Deep Machine Learning Model for Non-coding RNA Discovery,"Functional annotation of microbial genomes is often biased toward protein-coding genes, leaving a vast, unexplored landscape of non-coding RNAs (ncRNAs) that are critical for regulating bacterial and archaeal physiology, stress response and metabolism. Identifying ncRNAs directly from genomic sequence is a paramount challenge in bioinformatics and biology, essential for understanding the complete regulatory potential of an organism. This paper presents RNAMunin, a machine learning (ML) model that is capable of finding ncRNAs using genomic sequence alone. It is also computationally viable for large sequence datasets such as long read metagenomic assemblies with contigs totaling multiple Gbp. RNAMunin is trained on Rfam sequences extracted from approximately 60 Gbp of long read metagenomes from 16 San Francisco Estuary samples. We know of no other model that can detect ncRNAs based solely on genomic sequence at this scale. Since RNAMunin only requires genomic sequence as input, we do not need for an ncRNA to be transcribed to find it, i.e., we do not need transcriptomics data. We wrote this manuscript in a narrative style in order to best convey how RNAMunin was developed and how it works in detail. Unlike almost all current ML models, at approximately 1M parameters, RNAMunin is very small and very fast.",Lauren Lui; Torben Nielsen,,2025-07-16T06:33:50Z,http://arxiv.org/abs/2507.11950v1
2508.09406v1,CAKL: Commutative algebra k-mer learning of genomics,"Despite the availability of various sequence analysis models, comparative genomic analysis remains a challenge in genomics, genetics, and phylogenetics. Commutative algebra, a fundamental tool in algebraic geometry and number theory, has rarely been used in data and biological sciences. In this study, we introduce commutative algebra k-mer learning (CAKL) as the first-ever nonlinear algebraic framework for analyzing genomic sequences. CAKL bridges between commutative algebra, algebraic topology, combinatorics, and machine learning to establish a new mathematical paradigm for comparative genomic analysis. We evaluate its effectiveness on three tasks -- genetic variant identification, phylogenetic tree analysis, and viral genome classification -- typically requiring alignment-based, alignment-free, and machine-learning approaches, respectively. Across eleven datasets, CAKL outperforms five state-of-the-art sequence analysis methods, particularly in viral classification, and maintains stable predictive accuracy as dataset size increases, underscoring its scalability and robustness. This work ushers in a new era in commutative algebraic data analysis and learning.",Faisal Suwayyid; Yuta Hozumi; Hongsong Feng; Mushal Zia; JunJie Wee; Guo-Wei Wei,,2025-08-13T00:43:32Z,http://arxiv.org/abs/2508.09406v1
2501.02284v3,"Origin of $α$-satellite repeat arrays from mitochondrial molecular   fossils -- sequential insertion, expansion, and evolution in the nuclear   genome","Alpha satellite DNA is large tandem arrays of 150-400 bp units, and its origin remains an evolutionary mystery. In this research, we identified 1,545 alpha-satellite-like (SatL) repeat units in the nuclear genome of jewel wasp Nasonia vitripennis. Among them, thirty-nine copies of SatL were organized in two palindromic arrays in mitochondria, resulting in a 50% increase in the genome size. Strikingly, genomic neighborhood analyses of 1,516 nuclear SatL repeats revealed that they are located in NuMT (nuclear mitochondrial DNA) regions, and SatL phylogeny matched perfectly with mitochondrial genes and NuMT pseudogenes. These results support that SatL arrays originated from ten independent mitochondria insertion events into the nuclear genome within the last 500,000 years, after divergence from its sister species N. giraulti. Dramatic repeat GC-percent elevation (from 33.9% to 50.4%) is a hallmark of rapid SatL sequence evolution in mitochondria due to GC-biased gene conversion facilitated by the palindromic sequence pairing of the two mitochondrial SatL arrays. The nuclear SatL repeat arrays underwent substantial copy number expansion, from 12-15 (SatL1) to over 400 copies (SatL4). The oldest SatL4B array consists of four types of repeat units derived from deletions in the AT-rich region of ancestral repeats, and complex high-order structures have evolved through duplications. We also discovered similar repeat insertions into the nuclear genome of Muscidifurax, suggesting this mechanism can be common in insects. This is the first report of the mitochondrial origin of nuclear satellite sequences, and our findings shed new light on the origin and evolution of satellite DNA.",Yihang Zhou,,2025-01-04T13:22:30Z,http://arxiv.org/abs/2501.02284v3
2501.07737v1,Multi-megabase scale genome interpretation with genetic language models,"Understanding how molecular changes caused by genetic variation drive disease risk is crucial for deciphering disease mechanisms. However, interpreting genome sequences is challenging because of the vast size of the human genome, and because its consequences manifest across a wide range of cells, tissues and scales -- spanning from molecular to whole organism level. Here, we present Phenformer, a multi-scale genetic language model that learns to generate mechanistic hypotheses as to how differences in genome sequence lead to disease-relevant changes in expression across cell types and tissues directly from DNA sequences of up to 88 million base pairs. Using whole genome sequencing data from more than 150 000 individuals, we show that Phenformer generates mechanistic hypotheses about disease-relevant cell and tissue types that match literature better than existing state-of-the-art methods, while using only sequence data. Furthermore, disease risk predictors enriched by Phenformer show improved prediction performance and generalisation to diverse populations. Accurate multi-megabase scale interpretation of whole genomes without additional experimental data enables both a deeper understanding of molecular mechanisms involved in disease and improved disease risk prediction at the level of individuals.",Frederik Träuble; Lachlan Stuart; Andreas Georgiou; Pascal Notin; Arash Mehrjou; Ron Schwessinger; Mathieu Chevalley; Kim Branson; Bernhard Schölkopf; Cornelia van Duijn; Debora Marks; Patrick Schwab,,2025-01-13T23:00:40Z,http://arxiv.org/abs/2501.07737v1
2502.07272v3,GENERator: A Long-Context Generative Genomic Foundation Model,"Advancements in DNA sequencing technologies have significantly improved our ability to decode genomic sequences. However, the prediction and interpretation of these sequences remain challenging due to the intricate nature of genetic material. Large language models (LLMs) have introduced new opportunities for biological sequence analysis. Recent developments in genomic language models have underscored the potential of LLMs in deciphering DNA sequences. Nonetheless, existing models often face limitations in robustness and application scope, primarily due to constraints in model structure and training data scale. To address these limitations, we present GENERator, a generative genomic foundation model featuring a context length of 98k base pairs (bp) and 1.2B parameters. Trained on an expansive dataset comprising 386B bp of eukaryotic DNA, the GENERator demonstrates state-of-the-art performance across both established and newly proposed benchmarks. The model adheres to the central dogma of molecular biology, accurately generating protein-coding sequences that translate into proteins structurally analogous to known families. It also shows significant promise in sequence optimization, particularly through the prompt-responsive generation of enhancer sequences with specific activity profiles. These capabilities position the GENERator as a pivotal tool for genomic research and biotechnological advancement, enhancing our ability to interpret and predict complex biological systems and enabling precise genomic interventions. Implementation details and supplementary resources are available at https://github.com/GenerTeam/GENERator.",Wei Wu; Qiuyi Li; Mingyang Li; Kun Fu; Fuli Feng; Jieping Ye; Hui Xiong; Zheng Wang,,2025-02-11T05:39:49Z,http://arxiv.org/abs/2502.07272v3
2503.02997v1,"Enabling Fast, Accurate, and Efficient Real-Time Genome Analysis via New   Algorithms and Techniques","The advent of high-throughput sequencing technologies has revolutionized genome analysis by enabling the rapid and cost-effective sequencing of large genomes. Despite these advancements, the increasing complexity and volume of genomic data present significant challenges related to accuracy, scalability, and computational efficiency. These challenges are mainly due to various forms of unwanted and unhandled variations in sequencing data, collectively referred to as noise. In this dissertation, we address these challenges by providing a deep understanding of different types of noise in genomic data and developing techniques to mitigate the impact of noise on genome analysis.   First, we introduce BLEND, a noise-tolerant hashing mechanism that quickly identifies both exactly matching and highly similar sequences with arbitrary differences using a single lookup of their hash values. Second, to enable scalable and accurate analysis of noisy raw nanopore signals, we propose RawHash, a novel mechanism that effectively reduces noise in raw nanopore signals and enables accurate, real-time analysis by proposing the first hash-based similarity search technique for raw nanopore signals. Third, we extend the capabilities of RawHash with RawHash2, an improved mechanism that 1) provides a better understanding of noise in raw nanopore signals to reduce it more effectively and 2) improves the robustness of mapping decisions. Fourth, we explore the broader implications and new applications of raw nanopore signal analysis by introducing Rawsamble, the first mechanism for all-vs-all overlapping of raw signals using hash-based search. Rawsamble enables the construction of de novo assemblies directly from raw signals without basecalling, which opens up new directions and uses for raw nanopore signal analysis.",Can Firtina,,2025-03-04T20:44:37Z,http://arxiv.org/abs/2503.02997v1
2508.17211v1,Liquid-liquid phase separation enables highly selective viral genome   packaging,"In many viruses, hundreds of proteins assemble an outer shell (capsid) around the viral nucleic acid to form an infectious virion. How the assembly process selects the viral genome amidst a vast excess of diverse cellular nucleic acids is poorly understood. It has recently been discovered that many viruses perform assembly and genome packaging within liquid-liquid phase separated biomolecular condensates inside the host cell. However, the role of condensates in genome packaging is poorly understood. Here, we construct equilibrium and dynamical rate equation models for condensate-coupled assembly and genome packaging. We show that when the viral genome and capsid proteins favorably partition into the condensate, assembly rates, yields, and packaging efficiencies can increase by orders of magnitude. Selectivity is further enhanced by the condensate when capsid proteins are translated during assembly and packaging. Our results suggest that viral condensates provide a mechanism to ensure robust and highly selective assembly of virions around viral genomes. More broadly, our results may apply to other types of selective co-assembly processes that occur within biomolecular condensates, and suggest that liquid-liquid phase-separated condensates could be exploited for selective encapsulation of microscopic cargo in human-engineered systems.",Layne B. Frechette; Michael F. Hagan,,2025-08-24T04:51:01Z,http://arxiv.org/abs/2508.17211v1
2502.07749v1,Whole-Genome Phenotype Prediction with Machine Learning: Open Problems   in Bacterial Genomics,"How can we identify causal genetic mechanisms that govern bacterial traits? Initial efforts entrusting machine learning models to handle the task of predicting phenotype from genotype return high accuracy scores. However, attempts to extract any meaning from the predictive models are found to be corrupted by falsely identified ""causal"" features. Relying solely on pattern recognition and correlations is unreliable, significantly so in bacterial genomics settings where high-dimensionality and spurious associations are the norm. Though it is not yet clear whether we can overcome this hurdle, significant efforts are being made towards discovering potential high-risk bacterial genetic variants. In view of this, we set up open problems surrounding phenotype prediction from bacterial whole-genome datasets and extending those to learning causal effects, and discuss challenges that impact the reliability of a machine's decision-making when faced with datasets of this nature.",Tamsin James; Ben Williamson; Peter Tino; Nicole Wheeler,,2025-02-11T18:25:14Z,http://arxiv.org/abs/2502.07749v1
2504.07065v1,Enhancing Downstream Analysis in Genome Sequencing: Species   Classification While Basecalling,"The ability to quickly and accurately identify microbial species in a sample, known as metagenomic profiling, is critical across various fields, from healthcare to environmental science. This paper introduces a novel method to profile signals coming from sequencing devices in parallel with determining their nucleotide sequences, a process known as basecalling, via a multi-objective deep neural network for simultaneous basecalling and multi-class genome classification. We introduce a new loss strategy where losses for basecalling and classification are back-propagated separately, with model weights combined for the shared layers, and a pre-configured ranking strategy allowing top-K species accuracy, giving users flexibility to choose between higher accuracy or higher speed at identifying the species. We achieve state-of-the-art basecalling accuracies, while classification accuracies meet and exceed the results of state-of-the-art binary classifiers, attaining an average of 92.5%/98.9% accuracy at identifying the top-1/3 species among a total of 17 genomes in the Wick bacterial dataset. The work presented here has implications for future studies in metagenomic profiling by accelerating the bottleneck step of matching the DNA sequence to the correct genome.",Riselda Kodra; Hadjer Benmeziane; Irem Boybat; William Andrew Simon,,2025-04-09T17:30:43Z,http://arxiv.org/abs/2504.07065v1
2504.11340v1,Organisation and dynamics of individual DNA segments in topologically   complex genomes,"Capturing the physical organisation and dynamics of genomic regions is one of the major open challenges in biology. The kinetoplast DNA (kDNA) is a topologically complex genome, made by thousands of DNA (mini and maxi) circles interlinked into a two-dimensional Olympic network. The organisation and dynamics of these DNA circles are poorly understood. In this paper, we show that dCas9 linked to Quantum Dots can efficiently label different classes of DNA minicircles in kDNA. We use this method to study the distribution and dynamics of different classes of DNA minicircles within the network. We discover that maxicircles display a preference to localise at the periphery of the network and that they undergo subdiffusive dynamics. From the latter, we can also quantify the effective network stiffness, confirming previous indirect estimations via AFM. Our method could be used more generally, to quantify the location, dynamics and material properties of genomic regions in other complex genomes, such as that of bacteria, and to study their behaviour in the presence of DNA-binding proteins.",Saminathan Ramakrishnan; Auro Varat Patnaik; Guglielmo Grillo; Luca Tubiana; Davide Michieletto,,2025-04-15T16:12:54Z,http://arxiv.org/abs/2504.11340v1
2505.07188v1,Securing Genomic Data Against Inference Attacks in Federated Learning   Environments,"Federated Learning (FL) offers a promising framework for collaboratively training machine learning models across decentralized genomic datasets without direct data sharing. While this approach preserves data locality, it remains susceptible to sophisticated inference attacks that can compromise individual privacy. In this study, we simulate a federated learning setup using synthetic genomic data and assess its vulnerability to three key attack vectors: Membership Inference Attack (MIA), Gradient-Based Membership Inference Attack, and Label Inference Attack (LIA). Our experiments reveal that Gradient-Based MIA achieves the highest effectiveness, with a precision of 0.79 and F1-score of 0.87, underscoring the risk posed by gradient exposure in federated updates. Additionally, we visualize comparative attack performance through radar plots and quantify model leakage across clients. The findings emphasize the inadequacy of na\""ive FL setups in safeguarding genomic privacy and motivate the development of more robust privacy-preserving mechanisms tailored to the unique sensitivity of genomic data.",Chetan Pathade; Shubham Patil,,2025-05-12T02:36:50Z,http://arxiv.org/abs/2505.07188v1
2506.00662v1,Uncertainty-Aware Genomic Classification of Alzheimer's Disease: A   Transformer-Based Ensemble Approach with Monte Carlo Dropout,"INTRODUCTION: Alzheimer's disease (AD) is genetically complex, complicating robust classification from genomic data. METHODS: We developed a transformer-based ensemble model (TrUE-Net) using Monte Carlo Dropout for uncertainty estimation in AD classification from whole-genome sequencing (WGS). We combined a transformer that preserves single-nucleotide polymorphism (SNP) sequence structure with a concurrent random forest using flattened genotypes. An uncertainty threshold separated samples into an uncertain (high-variance) group and a more certain (low-variance) group. RESULTS: We analyzed 1050 individuals, holding out half for testing. Overall accuracy and area under the receiver operating characteristic (ROC) curve (AUC) were 0.6514 and 0.6636, respectively. Excluding the uncertain group improved accuracy from 0.6263 to 0.7287 (10.24% increase) and F1 from 0.5843 to 0.8205 (23.62% increase). DISCUSSION: Monte Carlo Dropout-driven uncertainty helps identify ambiguous cases that may require further clinical evaluation, thus improving reliability in AD genomic classification.",Taeho Jo; Eun Hye Lee; Alzheimer's Disease Sequencing Project,,2025-05-31T18:20:49Z,http://arxiv.org/abs/2506.00662v1
2506.02212v1,"Leveraging Natural Language Processing to Unravel the Mystery of Life: A   Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics","Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.",Ella Rannon; David Burstein,,2025-06-02T19:54:03Z,http://arxiv.org/abs/2506.02212v1
2506.11896v1,GlobDB: A comprehensive species-dereplicated microbial genome resource,"Over the past years, substantial numbers of microbial species' genomes have been deposited outside of conventional INSDC databases. The GlobDB aggregates 14 independent genomic catalogues to provide a comprehensive database of species-dereplicated microbial genomes, with consistent taxonomy, annotations, and additional analysis resources. The GlobDB is available at https://globdb.org/.",Daan R. Speth; Nick Pullen; Samuel T. N. Aroney; Benjamin L. Coltman; Jay T. Osvatic; Ben J. Woodcroft; Thomas Rattei; Michael Wagner,"Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria; Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria; Centre for Microbiome Research School of Biomedical Sciences, Queensland University of Technology, Translational Research Institute, Woolloongabba, Australia; Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria; Joint Microbiome Facility of the Medical University of Vienna and the University of Vienna, Vienna, Austria; Centre for Microbiome Research School of Biomedical Sciences, Queensland University of Technology, Translational Research Institute, Woolloongabba, Australia; Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria; Centre for Microbiology and Environmental Systems Science, University of Vienna, Vienna, Austria",2025-06-13T15:43:15Z,http://arxiv.org/abs/2506.11896v1
2507.05452v1,Topological Sequence Analysis of Genomes: Delta Complex approaches,"Algebraic topology has been widely applied to point cloud data to capture geometric shapes and topological structures. However, its application to genome sequence analysis remains rare. In this work, we propose topological sequence analysis (TSA) techniques by constructing $\Delta$-complexes and classifying spaces, leading to persistent homology, and persistent path homology on genome sequences. We also develop $\Delta$-complex-based persistent Laplacians to facilitate the topological spectral analysis of genome sequences. Finally, we demonstrate the utility of the proposed TSA approaches in phylogenetic analysis using Ebola virus sequences and whole bacterial genomes. The present TSA methods are more efficient than earlier TSA model, k-mer topology, and thus have a potential to be applied to other time-consuming sequential data analyses, such as those in linguistics, literature, music, media, and social contexts.",Jian Liu; Li Shen; Dong Chen; Guo-Wei Wei,,2025-07-07T20:10:31Z,http://arxiv.org/abs/2507.05452v1
2506.10886v2,"S3Mirror: Making Genomic Data Transfers Fast, Reliable, and Observable   with DBOS","To meet the needs of a large pharmaceutical organization, we set out to create S3Mirror - an application for transferring large genomic sequencing datasets between S3 buckets quickly, reliably, and observably. We used the DBOS Transact durable execution framework to achieve these goals and benchmarked the performance and cost of the application. S3Mirror is an open source DBOS Python application that can run in a variety of environments, including DBOS Cloud Pro, where it runs as much as 40x faster than AWS DataSync at a fraction of the cost. Moreover, S3Mirror is resilient to failures and allows for real-time filewise observability of ongoing and past transfers.",Steven Vasquez-Grinnell; Alex Poliakov,,2025-06-12T16:50:04Z,http://arxiv.org/abs/2506.10886v2
2501.02045v1,METAGENE-1: Metagenomic Foundation Model for Pandemic Monitoring,"We pretrain METAGENE-1, a 7-billion-parameter autoregressive transformer model, which we refer to as a metagenomic foundation model, on a novel corpus of diverse metagenomic DNA and RNA sequences comprising over 1.5 trillion base pairs. This dataset is sourced from a large collection of human wastewater samples, processed and sequenced using deep metagenomic (next-generation) sequencing methods. Unlike genomic models that focus on individual genomes or curated sets of specific species, the aim of METAGENE-1 is to capture the full distribution of genomic information present within this wastewater, to aid in tasks relevant to pandemic monitoring and pathogen detection. We carry out byte-pair encoding (BPE) tokenization on our dataset, tailored for metagenomic sequences, and then pretrain our model. In this paper, we first detail the pretraining dataset, tokenization strategy, and model architecture, highlighting the considerations and design choices that enable the effective modeling of metagenomic data. We then show results of pretraining this model on our metagenomic dataset, providing details about our losses, system metrics, and training stability over the course of pretraining. Finally, we demonstrate the performance of METAGENE-1, which achieves state-of-the-art results on a set of genomic benchmarks and new evaluations focused on human-pathogen detection and genomic sequence embedding, showcasing its potential for public health applications in pandemic monitoring, biosurveillance, and early detection of emerging health threats.",Ollie Liu; Sami Jaghouar; Johannes Hagemann; Shangshang Wang; Jason Wiemels; Jeff Kaufman; Willie Neiswanger,,2025-01-03T18:44:43Z,http://arxiv.org/abs/2501.02045v1
2501.04941v1,MyESL: Sparse learning in molecular evolution and phylogenetic analysis,"Evolutionary sparse learning (ESL) uses a supervised machine learning approach, Least Absolute Shrinkage and Selection Operator (LASSO), to build models explaining the relationship between a hypothesis and the variation across genomic features (e.g., sites) in sequence alignments. ESL employs sparsity between and within the groups of genomic features (e.g., genomic loci) by using sparse-group LASSO. Although some software packages are available for performing sparse group LASSO, we found them less well-suited for processing and analyzing genome-scale data containing millions of features, such as bases. MyESL software fills the need for open-source software for conducting ESL analyses with facilities to pre-process the input hypotheses and large alignments, make LASSO flexible and computationally efficient, and post-process the output model to produce different metrics useful in functional or evolutionary genomics. MyESL can take phylogenetic trees and sequence alignments as input and transform them into numeric responses and features, respecetively. The model outputs are processed into user-friendly text and graphical files. The computational core of MyESL is written in C++, which offers model building with or without group sparsity, while the pre- and post-processing of inputs and model outputs is performed using customized functions written in Python. One of its applications in phylogenomics showcases the utility of MyESL. Our analysis of empirical genome-scale datasets shows that MyESL can build evolutionary models quickly and efficiently on a personal desktop, while other computational packages were unable due to their prohibitive requirements of computational resources and time. MyESL is available for Python environments on Linux and distributed as a standalone application for Windows and macOS. It is available from https://github.com/kumarlabgit/MyESL.",Maxwell Sanderford; Sudip Sharma; Glen Stecher; Jun Liu; Jieping Ye; Sudhir Kumar,,2025-01-09T03:16:16Z,http://arxiv.org/abs/2501.04941v1
2501.11217v1,CoverM: Read alignment statistics for metagenomics,"Genome-centric analysis of metagenomic samples is a powerful method for understanding the function of microbial communities. Calculating read coverage is a central part of analysis, enabling differential coverage binning for recovery of genomes and estimation of microbial community composition. Coverage is determined by processing read alignments to reference sequences of either contigs or genomes. Per-reference coverage is typically calculated in an ad-hoc manner, with each software package providing its own implementation and specific definition of coverage. Here we present a unified software package CoverM which calculates several coverage statistics for contigs and genomes in an ergonomic and flexible manner. It uses 'Mosdepth arrays' for computational efficiency and avoids unnecessary I/O overhead by calculating coverage statistics from streamed read alignment results. CoverM is free software available at https://github.com/wwood/coverm. CoverM is implemented in Rust, with Python (https://github.com/apcamargo/pycoverm) and Julia (https://github.com/JuliaBinaryWrappers/CoverM_jll.jl) interfaces.",Samuel T. N. Aroney; Rhys J. P. Newell; Jakob N. Nissen; Antonio Pedro Camargo; Gene W. Tyson; Ben J. Woodcroft,"Centre for Microbiome Research, School of Biomedical Sciences, Queensland University of Technology; Centre for Microbiome Research, School of Biomedical Sciences, Queensland University of Technology; The Novo Nordisk Foundation Center for Protein Research, University of Copenhagen; Departamento de Genética e Evolução, Instituto de Biologia, Universidade Estadual de Campinas; Centre for Microbiome Research, School of Biomedical Sciences, Queensland University of Technology; Centre for Microbiome Research, School of Biomedical Sciences, Queensland University of Technology",2025-01-20T01:43:39Z,http://arxiv.org/abs/2501.11217v1
2502.03499v1,Omni-DNA: A Unified Genomic Foundation Model for Cross-Modal and   Multi-Task Learning,"Large Language Models (LLMs) demonstrate remarkable generalizability across diverse tasks, yet genomic foundation models (GFMs) still require separate finetuning for each downstream application, creating significant overhead as model sizes grow. Moreover, existing GFMs are constrained by rigid output formats, limiting their applicability to various genomic tasks. In this work, we revisit the transformer-based auto-regressive models and introduce Omni-DNA, a family of cross-modal multi-task models ranging from 20 million to 1 billion parameters. Our approach consists of two stages: (i) pretraining on DNA sequences with next token prediction objective, and (ii) expanding the multi-modal task-specific tokens and finetuning for multiple downstream tasks simultaneously. When evaluated on the Nucleotide Transformer and GB benchmarks, Omni-DNA achieves state-of-the-art performance on 18 out of 26 tasks. Through multi-task finetuning, Omni-DNA addresses 10 acetylation and methylation tasks at once, surpassing models trained on each task individually. Finally, we design two complex genomic tasks, DNA2Function and Needle-in-DNA, which map DNA sequences to textual functional descriptions and images, respectively, indicating Omni-DNA's cross-modal capabilities to broaden the scope of genomic applications. All the models are available through https://huggingface.co/collections/zehui127",Zehui Li; Vallijah Subasri; Yifei Shen; Dongsheng Li; Yiren Zhao; Guy-Bart Stan; Caihua Shan,,2025-02-05T09:20:52Z,http://arxiv.org/abs/2502.03499v1
2503.15377v1,Genomic data processing with GenomeFlow,"Advances in genome sequencing technologies generate massive amounts of sequence data that are increasingly analyzed and shared through public repositories. On-demand infrastructure services on cloud computing platforms enable the processing of such large-scale genomic sequence data in distributed processing environments with a significant reduction in analysis time. However, parallel processing on cloud computing platforms presents many challenges to researchers, even skillful bioinformaticians. In particular, it is difficult to design a computing architecture optimized to reduce the cost of computing and disk storage as genomic data analysis pipelines often employ many heterogeneous tools with different resource requirements. To address these issues, we developed GenomeFlow, a tool for automated development of computing architecture and resource optimization on Google Cloud Platform, which allows users to process a large number of samples at minimal cost. We outline multiple use cases of GenomeFlow demonstrating its utility to significantly reduce computing time and cost associated with analyzing genomic and transcriptomic data from hundreds to tens of thousands of samples from several consortia. Here, we describe a step-by-step protocol on how to use GenomeFlow for a common genomic data processing task. We introduce this example protocol geared toward a bioinformatician with little experience in cloud computing.",Junseok Park; Eduardo A. Maury; Changhoon Oh; Donghoon Shin; Danielle Denisko; Eunjung Alice Lee,,2025-03-19T16:13:05Z,http://arxiv.org/abs/2503.15377v1
2505.08071v1,NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome   Assembly,"De novo assembly enables investigations of unknown genomes, paving the way for personalized medicine and disease management. However, it faces immense computational challenges arising from the excessive data volumes and algorithmic complexity.   While state-of-the-art de novo assemblers utilize distributed systems for extreme-scale genome assembly, they demand substantial computational and memory resources. They also fail to address the inherent challenges of de novo assembly, including a large memory footprint, memory-bound behavior, and irregular data patterns stemming from complex, interdependent data structures. Given these challenges, de novo assembly merits a custom hardware solution, though existing approaches have not fully addressed the limitations.   We propose NMP-PaK, a hardware-software co-design that accelerates scalable de novo genome assembly through near-memory processing (NMP). Our channel-level NMP architecture addresses memory bottlenecks while providing sufficient scratchpad space for processing elements. Customized processing elements maximize parallelism while efficiently handling large data structures that are both dynamic and interdependent. Software optimizations include customized batch processing to reduce the memory footprint and hybrid CPU-NMP processing to address hardware underutilization caused by irregular data patterns.   NMP-PaK conducts the same genome assembly while incurring a 14X smaller memory footprint compared to the state-of-the-art de novo assembly. Moreover, NMP-PaK delivers a 16X performance improvement over the CPU baseline, with a 2.4X reduction in memory operations. Consequently, NMP-PaK achieves 8.3X greater throughput than state-of-the-art de novo assembly under the same resource constraints, showcasing its superior computational efficiency.",Heewoo Kim; Sanjay Sri Vallabh Singapuram; Haojie Ye; Joseph Izraelevitz; Trevor Mudge; Ronald Dreslinski; Nishil Talati,,2025-05-12T21:17:20Z,http://arxiv.org/abs/2505.08071v1
2507.19229v1,TrinityDNA: A Bio-Inspired Foundational Model for Efficient   Long-Sequence DNA Modeling,"The modeling of genomic sequences presents unique challenges due to their length and structural complexity. Traditional sequence models struggle to capture long-range dependencies and biological features inherent in DNA. In this work, we propose TrinityDNA, a novel DNA foundational model designed to address these challenges. The model integrates biologically informed components, including Groove Fusion for capturing DNA's structural features and Gated Reverse Complement (GRC) to handle the inherent symmetry of DNA sequences. Additionally, we introduce a multi-scale attention mechanism that allows the model to attend to varying levels of sequence dependencies, and an evolutionary training strategy that progressively adapts the model to both prokaryotic and eukaryotic genomes. TrinityDNA provides a more accurate and efficient approach to genomic sequence modeling, offering significant improvements in gene function prediction, regulatory mechanism discovery, and other genomics applications. Our model bridges the gap between machine learning techniques and biological insights, paving the way for more effective analysis of genomic data. Additionally, we introduced a new DNA long-sequence CDS annotation benchmark to make evaluations more comprehensive and oriented toward practical applications.",Qirong Yang; Yucheng Guo; Zicheng Liu; Yujie Yang; Qijin Yin; Siyuan Li; Shaomin Ji; Linlin Chao; Xiaoming Zhang; Stan Z. Li,,2025-07-25T12:55:30Z,http://arxiv.org/abs/2507.19229v1
2507.21648v1,Hyperbolic Genome Embeddings,"Current approaches to genomic sequence modeling often struggle to align the inductive biases of machine learning models with the evolutionarily-informed structure of biological systems. To this end, we formulate a novel application of hyperbolic CNNs that exploits this structure, enabling more expressive DNA sequence representations. Our strategy circumvents the need for explicit phylogenetic mapping while discerning key properties of sequences pertaining to core functional and regulatory behavior. Across 37 out of 42 genome interpretation benchmark datasets, our hyperbolic models outperform their Euclidean equivalents. Notably, our approach even surpasses state-of-the-art performance on seven GUE benchmark datasets, consistently outperforming many DNA language models while using orders of magnitude fewer parameters and avoiding pretraining. Our results include a novel set of benchmark datasets--the Transposable Elements Benchmark--which explores a major but understudied component of the genome with deep evolutionary significance. We further motivate our work by exploring how our hyperbolic models recognize genomic signal under various data-generating conditions and by constructing an empirical method for interpreting the hyperbolicity of dataset embeddings. Throughout these assessments, we find persistent evidence highlighting the potential of our hyperbolic framework as a robust paradigm for genome representation learning. Our code and benchmark datasets are available at https://github.com/rrkhan/HGE.",Raiyan R. Khan; Philippe Chlenski; Itsik Pe'er,,2025-07-29T10:06:17Z,http://arxiv.org/abs/2507.21648v1
2501.04718v1,Knowledge-Guided Biomarker Identification for Label-Free Single-Cell   RNA-Seq Data: A Reinforcement Learning Perspective,"Gene panel selection aims to identify the most informative genomic biomarkers in label-free genomic datasets. Traditional approaches, which rely on domain expertise, embedded machine learning models, or heuristic-based iterative optimization, often introduce biases and inefficiencies, potentially obscuring critical biological signals. To address these challenges, we present an iterative gene panel selection strategy that harnesses ensemble knowledge from existing gene selection algorithms to establish preliminary boundaries or prior knowledge, which guide the initial search space. Subsequently, we incorporate reinforcement learning through a reward function shaped by expert behavior, enabling dynamic refinement and targeted selection of gene panels. This integration mitigates biases stemming from initial boundaries while capitalizing on RL's stochastic adaptability. Comprehensive comparative experiments, case studies, and downstream analyses demonstrate the effectiveness of our method, highlighting its improved precision and efficiency for label-free biomarker discovery. Our results underscore the potential of this approach to advance single-cell genomics data analysis.",Meng Xiao; Weiliang Zhang; Xiaohan Huang; Hengshu Zhu; Min Wu; Xiaoli Li; Yuanchun Zhou,,2025-01-02T07:57:41Z,http://arxiv.org/abs/2501.04718v1
2501.07606v1,Heuristics based on Adjacency Graph Packing for DCJ Distance Considering   Intergenic Regions,"In this work, we explore heuristics for the Adjacency Graph Packing problem, which can be applied to the Double Cut and Join (DCJ) Distance Problem. The DCJ is a rearrangement operation and the distance problem considering it is a well established method for genome comparison. Our heuristics will use the structure called adjacency graph adapted to include information about intergenic regions, multiple copies of genes in the genomes, and multiple circular or linear chromosomes. The only required property from the genomes is that it must be possible to turn one into the other with DCJ operations. We propose one greedy heuristic and one heuristic based on Genetic Algorithms. Our experimental tests in artificial genomes show that the use of heuristics is capable of finding good results that are superior to a simpler random strategy.",Gabriel Siqueira; Alexsandro Oliveira Alexandrino; Andre Rodrigues Oliveira; Zanoni Dias,,2025-01-11T18:57:46Z,http://arxiv.org/abs/2501.07606v1
2501.08193v1,Modeling Quantum Machine Learning for Genomic Data Analysis,"Quantum Machine Learning (QML) continues to evolve, unlocking new opportunities for diverse applications. In this study, we investigate and evaluate the applicability of QML models for binary classification of genome sequence data by employing various feature mapping techniques. We present an open-source, independent Qiskit-based implementation to conduct experiments on a benchmark genomic dataset. Our simulations reveal that the interplay between feature mapping techniques and QML algorithms significantly influences performance. Notably, the Pegasos Quantum Support Vector Classifier (Pegasos-QSVC) exhibits high sensitivity, particularly excelling in recall metrics, while Quantum Neural Networks (QNN) achieve the highest training accuracy across all feature maps. However, the pronounced variability in classifier performance, dependent on feature mapping, highlights the risk of overfitting to localized output distributions in certain scenarios. This work underscores the transformative potential of QML for genomic data classification while emphasizing the need for continued advancements to enhance the robustness and accuracy of these methodologies.",Navneet Singh; Shiva Raj Pokhrel,,2025-01-14T15:14:26Z,http://arxiv.org/abs/2501.08193v1
2502.04067v1,Generalised Bayesian distance-based phylogenetics for the genomics era,"As whole genomes become widely available, maximum likelihood and Bayesian phylogenetic methods are demonstrating their limits in meeting the escalating computational demands. Conversely, distance-based phylogenetic methods are efficient, but are rarely favoured due to their inferior performance. Here, we extend distance-based phylogenetics using an entropy-based likelihood of the evolution among pairs of taxa, allowing for fast Bayesian inference in genome-scale datasets. We provide evidence of a close link between the inference criteria used in distance methods and Felsenstein's likelihood, such that the methods are expected to have comparable performance in practice. Using the entropic likelihood, we perform Bayesian inference on three phylogenetic benchmark datasets and find that estimates closely correspond with previous inferences. We also apply this rapid inference approach to a 60-million-site alignment from 363 avian taxa, covering most avian families. The method has outstanding performance and reveals substantial uncertainty in the avian diversification events immediately after the K-Pg transition event. The entropic likelihood allows for efficient Bayesian phylogenetic inference, accommodating the analysis demands of the genomic era.",Matthew J. Penn; Neil Scheidwasser; Mark P. Khurana; Christl A. Donnelly; David A. Duchêne; Samir Bhatt,,2025-02-06T13:24:29Z,http://arxiv.org/abs/2502.04067v1
2503.09312v2,Terrier: A Deep Learning Repeat Classifier,"Repetitive DNA sequences underpin genome architecture and evolutionary processes, yet they remain challenging to classify accurately. Terrier is a deep learning model designed to overcome these challenges by classifying repetitive DNA sequences using a publicly available, curated repeat sequence library trained under the RepeatMasker schema. Poor representation of taxa within repeat databases often limits the classification accuracy and reproducibility of current repeat annotation methods, limiting our understanding of repeat evolution and function. Terrier overcomes these challenges by leveraging deep learning for improved accuracy. Trained on Repbase, which includes over 100,000 repeat families -- four times more than Dfam -- Terrier maps 97.1% of Repbase sequences to RepeatMasker categories, offering the most comprehensive classification system available. When benchmarked against DeepTE, TERL, and TEclass2 in model organisms (rice, fruit flies, humans, and mice), Terrier achieved superior accuracy while classifying a broader range of sequences. Further validation in non-model amphibian, flatworm and Northern krill genomes highlights its effectiveness in improving classification in non-model species, facilitating research on repeat-driven evolution, genomic instability, and phenotypic variation.",Robert Turnbull; Neil D. Young; Edoardo Tescari; Lee F. Skerratt; Tiffany A. Kosch,,2025-03-12T12:03:26Z,http://arxiv.org/abs/2503.09312v2
2503.10713v1,HiCMamba: Enhancing Hi-C Resolution and Identifying 3D Genome Structures   with State Space Modeling,"Hi-C technology measures genome-wide interaction frequencies, providing a powerful tool for studying the 3D genomic structure within the nucleus. However, high sequencing costs and technical challenges often result in Hi-C data with limited coverage, leading to imprecise estimates of chromatin interaction frequencies. To address this issue, we present a novel deep learning-based method HiCMamba to enhance the resolution of Hi-C contact maps using a state space model. We adopt the UNet-based auto-encoder architecture to stack the proposed holistic scan block, enabling the perception of both global and local receptive fields at multiple scales. Experimental results demonstrate that HiCMamba outperforms state-of-the-art methods while significantly reducing computational resources. Furthermore, the 3D genome structures, including topologically associating domains (TADs) and loops, identified in the contact maps recovered by HiCMamba are validated through associated epigenomic features. Our work demonstrates the potential of a state space model as foundational frameworks in the field of Hi-C resolution enhancement.",Minghao Yang; Zhi-An Huang; Zhihang Zheng; Yuqiao Liu; Shichen Zhang; Pengfei Zhang; Hui Xiong; Shaojun Tang,,2025-03-13T03:04:02Z,http://arxiv.org/abs/2503.10713v1
2505.00598v2,Fast and Low-Cost Genomic Foundation Models via Outlier Removal,"To address the challenge of scarce computational resources in genomic modeling, we introduce GERM, a genomic foundation model with strong compression performance and fast adaptability. GERM improves upon models like DNABERT-2 by eliminating outliers that hinder low-rank adaptation and post-training quantization, enhancing both efficiency and robustness. We replace the vanilla attention layer with an outlier-free mechanism inspired by associative memory models. By removing outliers during both pre-training and fine-tuning, this approach accelerates adaptation, reduces computational costs, and enhances quantization robustness within acceptable loss margins. Additionally, we propose GERM-T, a strategy that employs small-step continual learning within the outlier-free framework, leveraging original checkpoints to avoid retraining from scratch. Empirically, GERM improves fine-tuning performance by 37.98% and quantization by 64.34% over the baseline model. It also reduces average kurtosis by 92.14% and maximum infinity norm by 82.77%. Compared to leading methods, GERM consistently delivers superior performance, offering a practical solution for genomic modeling in resource-constrained settings. Code is available at https://github.com/MAGICS-LAB/GERM.",Haozheng Luo; Chenghao Qiu; Maojiang Su; Zhihan Zhou; Zoe Mehta; Guo Ye; Jerry Yao-Chieh Hu; Han Liu,,2025-05-01T15:31:09Z,http://arxiv.org/abs/2505.00598v2
2505.03377v1,Gene finding revisited: improved robustness through structured decoding   from learned embeddings,"Gene finding is the task of identifying the locations of coding sequences within the vast amount of genetic code contained in the genome. With an ever increasing quantity of raw genome sequences, gene finding is an important avenue towards understanding the genetic information of (novel) organisms, as well as learning shared patterns across evolutionarily diverse species. The current state of the art are graphical models usually trained per organism and requiring manually curated datasets. However, these models lack the flexibility to incorporate deep learning representation learning techniques that have in recent years been transformative in the analysis of pro tein sequences, and which could potentially help gene finders exploit the growing number of the sequenced genomes to expand performance across multiple organisms. Here, we propose a novel approach, combining learned embeddings of raw genetic sequences with exact decoding using a latent conditional random field. We show that the model achieves performance matching the current state of the art, while increasing training robustness, and removing the need for manually fitted length distributions. As language models for DNA improve, this paves the way for more performant cross-organism gene-finders.",Frederikke I. Marin; Dennis Pultz; Wouter Boomsma,,2025-05-06T09:53:15Z,http://arxiv.org/abs/2505.03377v1
2505.11997v2,Multimodal Cancer Survival Analysis via Hypergraph Learning with   Cross-Modality Rebalance,"Multimodal pathology-genomic analysis has become increasingly prominent in cancer survival prediction. However, existing studies mainly utilize multi-instance learning to aggregate patch-level features, neglecting the information loss of contextual and hierarchical details within pathology images. Furthermore, the disparity in data granularity and dimensionality between pathology and genomics leads to a significant modality imbalance. The high spatial resolution inherent in pathology data renders it a dominant role while overshadowing genomics in multimodal integration. In this paper, we propose a multimodal survival prediction framework that incorporates hypergraph learning to effectively capture both contextual and hierarchical details from pathology images. Moreover, it employs a modality rebalance mechanism and an interactive alignment fusion strategy to dynamically reweight the contributions of the two modalities, thereby mitigating the pathology-genomics imbalance. Quantitative and qualitative experiments are conducted on five TCGA datasets, demonstrating that our model outperforms advanced methods by over 3.4\% in C-Index performance.",Mingcheng Qu; Guang Yang; Donglin Di; Tonghua Su; Yue Gao; Yang Song; Lei Fan,,2025-05-17T13:16:54Z,http://arxiv.org/abs/2505.11997v2
2505.19461v1,Fluctuations in DNA Packing Density Drive the Spatial Segregation   between Euchromatin and Heterochromatin,"In the crowded eukaryotic nucleus, euchromatin and heterochromatin segregate into distinct compartments, a phenomenon often attributed to homotypic interactions mediated by liquid liquid phase separation of chromatin associated proteins. Here, we revisit genome compartmentalization by examining the role of in vivo DNA packing density fluctuations driven by ATP dependent chromatin remodelers. Leveraging DNA accessibility data, we develop a polymer based model that captures these fluctuations and successfully reproduces genome wide compartment patterns observed in HiC data, without invoking homotypic interactions. Further analysis reveals that density fluctuations in a crowded nuclear environment elevate the system energy, while euchromatin heterochromatin segregation facilitates energy dissipation, offering a thermodynamic advantage for spontaneous compartment formation. These findings suggest that euchromatin heterochromatin segregation may arise through a non equilibrium, self organizing process, providing new insights into genome organization.",Luming Meng; Boping Liu; Qiong Luo,,2025-05-26T03:34:57Z,http://arxiv.org/abs/2505.19461v1
2505.19501v2,Toward Scientific Reasoning in LLMs: Training from Expert Discussions   via Reinforcement Learning,"We investigate how to teach large language models (LLMs) to perform scientific reasoning by leveraging expert discussions as a learning signal. Focusing on the genomics domain, we develop an automated pipeline to extract trainable data and introduce Genome-Bench, a new benchmark constructed from over a decade of scientific forum discussions on genome engineering. Our pipeline transforms raw interactions into a reinforcement learning-friendly multiple-choice questions format, supported by 3000+ high-quality question-answer pairs spanning foundational biology, experimental troubleshooting, tool usage, and beyond. We fine-tune an LLM using RL with a rule-based reward signal derived from the synthetic MCQ dataset to enhance domain-specific reasoning. Our results show that reinforcement learning from scientific discussions improves model performance by over 15% compared to the base model on Genome-Bench, narrowing the gap between open-source LLMs and expert-level reasoning. To our knowledge, this is the first end-to-end pipeline for teaching LLMs to reason from scientific discussions, with promising potential for generalization across scientific domains beyond biology.",Ming Yin; Yuanhao Qu; Ling Yang; Le Cong; Mengdi Wang,,2025-05-26T04:28:46Z,http://arxiv.org/abs/2505.19501v2
2506.00673v1,DuAL-Net: A Hybrid Framework for Alzheimer's Disease Prediction from   Whole-Genome Sequencing via Local SNP Windows and Global Annotations,"Alzheimer's disease (AD) dementia is the most common form of dementia. With the emergence of disease-modifying therapies, predicting disease risk before symptom onset has become critical. We introduce DuAL-Net, a hybrid deep learning framework for AD dementia prediction using whole genome sequencing (WGS) data. DuAL-Net integrates two components: local probability modeling, which segments the genome into non-overlapping windows, and global annotation-based modeling, which annotates SNPs and reorganizes WGS input to capture long-range functional relationships. Both employ out-of-fold stacking with TabNet and Random Forest classifiers. Final predictions combine local and global probabilities using an optimized weighting parameter alpha. We analyzed WGS data from 1,050 individuals (443 cognitively normal, 607 AD dementia) using five-fold cross-validation. DuAL-Net achieved an AUC of 0.671 using top-ranked SNPs, representing 35.0% and 20.3% higher performance than bottom-ranked and randomly selected SNPs, respectively. ROC analysis demonstrated strong positive correlation between SNP prioritization rank and predictive power. The model identified known AD-associated SNPs as top contributors alongside potentially novel variants. DuAL-Net presents a promising framework improving both predictive accuracy and biological interpretability. The framework and web implementation offer an accessible platform for broader research applications.",Eun Hye Lee; Taeho Jo,,2025-05-31T18:53:19Z,http://arxiv.org/abs/2506.00673v1
2506.00821v1,SafeGenes: Evaluating the Adversarial Robustness of Genomic Foundation   Models,"Genomic Foundation Models (GFMs), such as Evolutionary Scale Modeling (ESM), have demonstrated significant success in variant effect prediction. However, their adversarial robustness remains largely unexplored. To address this gap, we propose SafeGenes: a framework for Secure analysis of genomic foundation models, leveraging adversarial attacks to evaluate robustness against both engineered near-identical adversarial Genes and embedding-space manipulations. In this study, we assess the adversarial vulnerabilities of GFMs using two approaches: the Fast Gradient Sign Method (FGSM) and a soft prompt attack. FGSM introduces minimal perturbations to input sequences, while the soft prompt attack optimizes continuous embeddings to manipulate model predictions without modifying the input tokens. By combining these techniques, SafeGenes provides a comprehensive assessment of GFM susceptibility to adversarial manipulation. Targeted soft prompt attacks led to substantial performance degradation, even in large models such as ESM1b and ESM1v. These findings expose critical vulnerabilities in current foundation models, opening new research directions toward improving their security and robustness in high-stakes genomic applications such as variant effect prediction.",Huixin Zhan; Jason H. Moore,,2025-06-01T03:54:03Z,http://arxiv.org/abs/2506.00821v1
2506.12986v1,Improving spliced alignment by modeling splice sites with deep learning,"Motivation: Spliced alignment refers to the alignment of messenger RNA (mRNA) or protein sequences to eukaryotic genomes. It plays a critical role in gene annotation and the study of gene functions. Accurate spliced alignment demands sophisticated modeling of splice sites, but current aligners use simple models, which may affect their accuracy given dissimilar sequences.   Results: We implemented minisplice to learn splice signals with a one-dimensional convolutional neural network (1D-CNN) and trained a model with 7,026 parameters for vertebrate and insect genomes. It captures conserved splice signals across phyla and reveals GC-rich introns specific to mammals and birds. We used this model to estimate the empirical splicing probability for every GT and AG in genomes, and modified minimap2 and miniprot to leverage pre-computed splicing probability during alignment. Evaluation on human long-read RNA-seq data and cross-species protein datasets showed our method greatly improves the junction accuracy especially for noisy long RNA-seq reads and proteins of distant homology.   Availability and implementation: https://github.com/lh3/minisplice",Siying Yang; Neng Huang; Heng Li,,2025-06-15T22:57:33Z,http://arxiv.org/abs/2506.12986v1
2507.04111v1,Quantum computing for genomics: conceptual challenges and practical   perspectives,"We assess the potential of quantum computing to accelerate computation of central tasks in genomics, focusing on often-neglected theoretical limitations. We discuss state-of-the-art challenges of quantum search, optimization, and machine learning algorithms. Examining database search with Grover's algorithm, we show that the expected speedup vanishes under realistic assumptions. For combinatorial optimization prevalent in genomics, we discuss the limitations of theoretical complexity in practice and suggest carefully identifying problems genuinely suited for quantum acceleration. Given the competition from excellent classical approximate solvers, quantum computing could offer a speedup in the near future only for a specific subset of hard enough tasks in assembly, gene selection, and inference. These tasks need to be characterized by core optimization problems that are particularly challenging for classical methods while requiring relatively limited variables. We emphasize rigorous empirical validation through runtime scaling analysis to avoid misleading claims of quantum advantage. Finally, we discuss the problem of trainability and data-loading in quantum machine learning. This work advocates for a balanced perspective on quantum computing in genomics, guiding future research toward targeted applications and robust validation.",Aurora Maurizio; Guglielmo Mazzola,,2025-07-05T17:41:10Z,http://arxiv.org/abs/2507.04111v1
2508.02061v1,A Bayesian approach to model uncertainty in single-cell genomic data,"Network models provide a powerful framework for analysing single-cell count data, facilitating the characterisation of cellular identities, disease mechanisms, and developmental trajectories. However, uncertainty modeling in unsupervised learning with genomic data remains insufficiently explored. Conventional clustering methods assign a singular identity to each cell, potentially obscuring transitional states during differentiation or mutation. This study introduces a variational Bayesian framework for clustering and analysing single-cell genomic data, employing a Bayesian Gaussian mixture model to estimate the probabilistic association of cells with distinct clusters. This approach captures cellular transitions, yielding biologically coherent insights into neurogenesis and breast cancer progression. The inferred clustering probabilities enable further analyses, including Differential Expression Analysis and pseudotime analysis. Furthermore, we propose utilising the misclustering rate and Area Under the Curve in clustering scRNA-seq data as an innovative metric to quantitatively evaluate overall clustering performance. This methodological advancement enhances the resolution of single-cell data analysis, enabling a more nuanced characterisation of dynamic cellular identities in development and disease.",Shanshan Ren; Thomas E. Bartlett; Lina Gerontogianni; Swati Chandna,,2025-08-04T05:00:15Z,http://arxiv.org/abs/2508.02061v1
2508.07127v1,How Effectively Can Large Language Models Connect SNP Variants and ECG   Phenotypes for Cardiovascular Risk Prediction?,"Cardiovascular disease (CVD) prediction remains a tremendous challenge due to its multifactorial etiology and global burden of morbidity and mortality. Despite the growing availability of genomic and electrophysiological data, extracting biologically meaningful insights from such high-dimensional, noisy, and sparsely annotated datasets remains a non-trivial task. Recently, LLMs has been applied effectively to predict structural variations in biological sequences. In this work, we explore the potential of fine-tuned LLMs to predict cardiac diseases and SNPs potentially leading to CVD risk using genetic markers derived from high-throughput genomic profiling. We investigate the effect of genetic patterns associated with cardiac conditions and evaluate how LLMs can learn latent biological relationships from structured and semi-structured genomic data obtained by mapping genetic aspects that are inherited from the family tree. By framing the problem as a Chain of Thought (CoT) reasoning task, the models are prompted to generate disease labels and articulate informed clinical deductions across diverse patient profiles and phenotypes. The findings highlight the promise of LLMs in contributing to early detection, risk assessment, and ultimately, the advancement of personalized medicine in cardiac care.",Niranjana Arun Menon; Iqra Farooq; Yulong Li; Sara Ahmed; Yutong Xie; Muhammad Awais; Imran Razzak,,2025-08-10T00:19:29Z,http://arxiv.org/abs/2508.07127v1
2508.21806v1,Suppression of errors in collectively coded information,"Modern life largely transmits genetic information from mother to daughter through the duplication of single physically intact molecules that encode information. However, copying an extended molecule requires highly processive copying machinery and high fidelity that scales with the genome size to avoid the error catastrophe. Here, we explore these fidelity requirements in an alternative architecture, the virtual circular genome, in which no one physical molecule encodes the full genetic information. Instead, information is encoded and transmitted in a collective of overlapping and interacting segments. Using a model experimental system of a complex mixture of DNA oligos that can partly anneal and extend off each other, we find that mutant oligomers are suppressed relative to a model without collective encoding. Through simulations and theory, we show that this suppression of mutants can be explained by competition for productive binding partners. As a consequence, information can be propagated robustly in a virtual circular genome even if the mutation rate is above the error catastrophe for a physically intact genome.",Martin J. Falk; Leon Zhou; Yoshiya J. Matsubara; Kabir Husain; Jack W. Szostak; Arvind Murugan,,2025-08-29T17:39:29Z,http://arxiv.org/abs/2508.21806v1
2503.20964v2,Active Hydrodynamic Theory of Euchromatin and Heterochromatin,"The genome contains genetic information essential for cell's life. The genome's spatial organization inside the cell nucleus is critical for its proper function including gene regulation. The two major genomic compartments -- euchromatin and heterochromatin -- contain largely transcriptionally active and silenced genes, respectively, and exhibit distinct dynamics. In this work, we present a hydrodynamic framework that describes the large-scale behavior of euchromatin and heterochromatin, and accounts for the interplay of mechanical forces, active processes, and nuclear confinement. Our model shows contractile stresses from cross-linking proteins lead to the formation of heterochromatin droplets via mechanically driven phase separation. These droplets grow, coalesce, and in nuclear confinement, wet the boundary. Active processes, such as gene transcription in euchromatin, introduce non-equilibrium fluctuations that drive long-range, coherent motions of chromatin as well as the nucleoplasm, and thus alter the genome's spatial organization. These fluctuations also indirectly deform heterochromatin droplets, by continuously changing their shape. Taken together, our findings reveal how active forces, mechanical stresses and hydrodynamic flows contribute to the genome's organization at large scales and provide a physical framework for understanding chromatin organization and dynamics in live cells.",S. Alex Rautu; Alexandra Zidovska; David Saintillan; Michael J. Shelley,,2025-03-26T20:07:13Z,http://arxiv.org/abs/2503.20964v2
2504.15934v1,Real-time raw signal genomic analysis using fully integrated memristor   hardware,"Advances in third-generation sequencing have enabled portable and real-time genomic sequencing, but real-time data processing remains a bottleneck, hampering on-site genomic analysis due to prohibitive time and energy costs. These technologies generate a massive amount of noisy analog signals that traditionally require basecalling and digital mapping, both demanding frequent and costly data movement on von Neumann hardware. To overcome these challenges, we present a memristor-based hardware-software co-design that processes raw sequencer signals directly in analog memory, effectively combining the separated basecalling and read mapping steps. Here we demonstrate, for the first time, end-to-end memristor-based genomic analysis in a fully integrated memristor chip. By exploiting intrinsic device noise for locality-sensitive hashing and implementing parallel approximate searches in content-addressable memory, we experimentally showcase on-site applications including infectious disease detection and metagenomic classification. Our experimentally-validated analysis confirms the effectiveness of this approach on real-world tasks, achieving a state-of-the-art 97.15% F1 score in virus raw signal mapping, with 51x speed up and 477x energy saving compared to implementation on a state-of-the-art ASIC. These results demonstrate that memristor-based in-memory computing provides a viable solution for integration with portable sequencers, enabling truly real-time on-site genomic analysis for applications ranging from pathogen surveillance to microbial community profiling.",Peiyi He; Shengbo Wang; Ruibin Mao; Sebastian Siegel; Giacomo Pedretti; Jim Ignowski; John Paul Strachan; Ruibang Luo; Can Li,,2025-04-22T14:22:34Z,http://arxiv.org/abs/2504.15934v1
2504.20328v1,Mantodea phylogenomics provides new insights into X-chromosome   progression and evolutionary radiation,"Background: Praying mantises, members of the order Mantodea, play important roles in agriculture, medicine, bionics, and entertainment. However, the scarcity of genomic resources has hindered extensive studies on mantis evolution and behaviour. Results: Here, we present the chromosome-scale reference genomes of five mantis species: the European mantis (Mantis religiosa), Chinese mantis (Tenodera sinensis), triangle dead leaf mantis (Deroplatys truncata), orchid mantis (Hymenopus coronatus), and metallic mantis (Metallyticus violaceus). We found that transposable element expansion is the major force governing genome size in Mantodea. Based on whole-alignments, we deduced that the Mantodea ancestor may have had only one X chromosome and that translocations between the X chromosome and an autosome may have occurred in the lineage of the superfamily Mantoidea. Furthermore, we found a lower evolutionary rate for the metallic mantis than for the other mantises. We also found that Mantodea underwent rapid radiation after the K-Pg mass extinction event, which could have contributed to the confusion in species classification. Conclusions: We present the chromosome-scale reference genomes of five mantis species to reveal the X-chromosome evolution, clarify the phylogeny relationship, and transposable element expansion.",Hangwei Liu; Lihong Lei; Fan Jiang; Bo Zhang; Hengchao Wang; Yutong Zhang; Anqi Wang; Hanbo Zhao; Guirong Wang; Wei Fan,,2025-04-29T00:36:14Z,http://arxiv.org/abs/2504.20328v1
2505.10017v1,Data mining of public genomic repositories: harnessing off-target reads   to expand microbial pathogen genomic resources,"As sequencing technologies become more affordable and genomic databases expand continuously, the reuse of publicly available sequencing data emerges as a powerful strategy for studying microbial pathogens. Indeed, raw sequencing reads generated for the study of a given organism often contain reads originating from the associated microbiota. This review explores how such off-target reads can be detected and used for the study of microbial pathogens. We present genomic data mining as a method to identify relevant sequencing runs from petabase-scale databases, highlighting recent methodological advances that allow efficient database querying. We then briefly outline methods designed to retrieve relevant data and associated metadata, and provide an overview of common downstream analysis pipelines. We discuss how such approaches have (i) expanded the known genetic diversity of microbial pathogens, (ii) enriched our understanding of their spatiotemporal distribution, and (iii) highlighted previously unrecognized ecological interactions involving microbial pathogens. However, these analyses often rely on the completeness and accuracy of accompanying metadata, which remain highly variable. We detail common pitfalls, including data contamination and metadata misannotations, and suggest strategies for result interpretation. Ultimately, while data mining cannot replace dedicated studies, it constitutes an essential and complementary tool for microbial pathogen research. Broader utility will depend on improved data standardization and systematic genomic monitoring across ecosystems.",Damien Richard; Nils Poulicard,UMR PHIM; UMR PHIM,2025-05-15T06:59:18Z,http://arxiv.org/abs/2505.10017v1
2507.07761v1,Widespread remote introgression in the grass genomes,"Genetic transfers are pervasive across both prokaryotes and eukaryotes, encompassing canonical genomic introgression between species or genera and horizontal gene transfer (HGT) across kingdoms. However, DNA transfer between phylogenetically distant species, here defined as remote introgression (RI), has remained poorly explored in evolutionary genomics. In this study, we present RIFinder, a novel phylogeny-based method for RI event detection, and apply it to a comprehensive dataset of 122 grass genomes. Our analysis identifies 622 RI events originating from 543 distinct homologous genes, revealing distinct characteristics among grass subfamilies. Specifically, the subfamily Pooideae exhibits the highest number of introgressed genes while Bambusoideae contains the lowest. Comparisons among accepted genes, their donor copies and native homologs demonstrate that introgressed genes undergo post-transfer localized adaptation, with significant functional enrichment in stress-response pathways. Notably, we identify a large Triticeae-derived segment in a Chloridoideae species Cleistogenes songorica, which is potentially associated with its exceptional drought tolerance. Furthermore, we provide compelling evidence that RI has contributed to the origin and diversification of biosynthetic gene clusters of gramine, a defensive alkaloid chemical, across grass species. Collectively, our study establishes a robust method for RI detection and highlights its critical role in adaptive evolution.",Yujie Huang; Shiyu Zhang; Hanyang Lin; Chenxu Liu; Zhefu Li; Kun Yang; Yutong Liu; Linfeng Jin; Chuanlong Lu; Yuan Cheng; Chaoyi Hu; Huifang Zhao; Guoping Zhang; Qian Qian; Longjiang Fan; Dongya Wu,,2025-07-10T13:37:42Z,http://arxiv.org/abs/2507.07761v1
2507.08542v1,CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA   Splice Site Detection and Pairing in Plant Genomes,"Circular RNAs (circRNAs) are important components of the non-coding RNA regulatory network. Previous circRNA identification primarily relies on high-throughput RNA sequencing (RNA-seq) data combined with alignment-based algorithms that detect back-splicing signals. However, these methods face several limitations: they can't predict circRNAs directly from genomic DNA sequences and relies heavily on RNA experimental data; they involve high computational costs due to complex alignment and filtering steps; and they are inefficient for large-scale or genome-wide circRNA prediction. The challenge is even greater in plants, where plant circRNA splice sites often lack the canonical GT-AG motif seen in human mRNA splicing, and no efficient deep learning model with strong generalization capability currently exists. Furthermore, the number of currently identified plant circRNAs is likely far lower than their true abundance. In this paper, we propose a deep learning framework named CircFormerMoE based on transformers and mixture-of experts for predicting circRNAs directly from plant genomic DNA. Our framework consists of two subtasks known as splicing site detection (SSD) and splicing site pairing (SSP). The model's effectiveness has been validated on gene data of 10 plant species. Trained on known circRNA instances, it is also capable of discovering previously unannotated circRNAs. In addition, we performed interpretability analyses on the trained model to investigate the sequence patterns contributing to its predictions. Our framework provides a fast and accurate computational method and tool for large-scale circRNA discovery in plants, laying a foundation for future research in plant functional genomics and non-coding RNA annotation.",Tianyou Jiang,,2025-07-11T12:43:17Z,http://arxiv.org/abs/2507.08542v1
2509.01020v1,"GeneTEK: Low-power, high-performance and scalable genome sequence   matching in FPGAs","The advent of next-generation sequencing (NGS) has revolutionized genomic research by enabling high-throughput data generation through parallel sequencing of a diverse range of organisms at significantly reduced costs. This breakthrough has unleashed a ""Cambrian explosion"" in genomic data volume and diversity. This volume of workloads places genomics among the top four big data challenges anticipated for this decade. In this context, pairwise sequence alignment represents a very time- and energy-consuming step in common bioinformatics pipelines. Speeding up this step requires the implementation of heuristic approaches, optimized algorithms, and/or hardware acceleration.   Whereas state-of-the-art CPU and GPU implementations have demonstrated significant performance gains, recent field programmable gate array (FPGA) implementations have shown improved energy efficiency. However, the latter often suffer from limited scalability due to constraints on hardware resources when aligning longer sequences. In this work, we present a scalable and flexible FPGA-based accelerator template that implements Myers's algorithm using high-level synthesis and a worker-based architecture. GeneTEK, an instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA, outperforms state-of-the-art CPU and GPU implementations in both speed and energy efficiency, while overcoming scalability limitations of current FPGA approaches. Specifically, GeneTEK achieves at least a 19.4% increase in execution speed and up to 62x reduction in energy consumption compared to leading CPU and GPU solutions, while fitting comparison matrices up to 72% larger compared to previous FPGA solutions. These results reaffirm the potential of FPGAs as an energy-efficient platform for scalable genomic workloads.",Elena Espinosa; Rubén Rodríguez Álvarez; José Miranda; Rafael Larrosa; Miguel Peón-Quirós; Oscar Plata; David Atienza,,2025-08-31T23:11:48Z,http://arxiv.org/abs/2509.01020v1
2501.04822v1,Curated loci prime editing (cliPE) for accessible multiplexed assays of   variant effect (MAVEs),"Multiplexed assays of variant effect (MAVEs) perform simultaneous characterization of many variants. Prime editing has been recently adopted for introducing many variants in their native genomic contexts. However, robust protocols and standards are limited, preventing widespread uptake. Herein, we describe curated loci prime editing (cliPE) which is an accessible, low-cost experimental pipeline to perform MAVEs using prime editing of a target gene, as well as a companion Shiny app (pegRNA Designer) to rapidly and easily design user-specific MAVE libraries.",Carina G Biar; Nicholas Bodkin; Gemma L Carvill; Jeffrey D Calhoun,,2025-01-08T20:21:40Z,http://arxiv.org/abs/2501.04822v1
2503.04490v2,Large Language Models in Bioinformatics: A Survey,"Large Language Models (LLMs) are revolutionizing bioinformatics, enabling advanced analysis of DNA, RNA, proteins, and single-cell data. This survey provides a systematic review of recent advancements, focusing on genomic sequence modeling, RNA structure prediction, protein function inference, and single-cell transcriptomics. Meanwhile, we also discuss several key challenges, including data scarcity, computational complexity, and cross-omics integration, and explore future directions such as multimodal learning, hybrid AI models, and clinical applications. By offering a comprehensive perspective, this paper underscores the transformative potential of LLMs in driving innovations in bioinformatics and precision medicine.",Zhenyu Wang; Zikang Wang; Jiyue Jiang; Pengan Chen; Xiangyu Shi; Yu Li,,2025-03-06T14:38:20Z,http://arxiv.org/abs/2503.04490v2
2505.07740v1,Pan-genome Analysis of Plastomes from Lamiales using PGR-TK,"Chloroplast sequences from the Lamiales order were analyzed using the Pangenome Research Toolkit (PGR-TK). Overall, most genera and families exhibited a high degree of sequence uniformity. However, at the genus level, Utricularia, Incarvillea, and Orobanche stood out as particularly divergent. At the family level, Orobanchaceae, Bignoniaceae and Lentibulariaceae displayed notably complex patterns in the generated plots. The PGR-TK algorithm successfully distinguished most genera within their respective families and often recognized misclassified plants.",Aadhavan Veerendra; Manoj Samanta,,2025-05-12T16:49:38Z,http://arxiv.org/abs/2505.07740v1
2505.11610v1,Foundation Models for AI-Enabled Biological Design,"This paper surveys foundation models for AI-enabled biological design, focusing on recent developments in applying large-scale, self-supervised models to tasks such as protein engineering, small molecule design, and genomic sequence design. Though this domain is evolving rapidly, this survey presents and discusses a taxonomy of current models and methods. The focus is on challenges and solutions in adapting these models for biological applications, including biological sequence modeling architectures, controllability in generation, and multi-modal integration. The survey concludes with a discussion of open problems and future directions, offering concrete next-steps to improve the quality of biological sequence generation.",Asher Moldwin; Amarda Shehu,,2025-05-16T18:17:37Z,http://arxiv.org/abs/2505.11610v1
2508.10058v1,A Structural Analysis of Population Graphs,"The format of graphing algorithms for genomic data has been a debate in recent biotechnology. In this paper, we discuss the construction of population graphs using said genomic data. We first examine the GENPOFAD distance measurement, developed by Joly et. al., and prove that this constitutes a metric function. We develop an algorithm to construct graphs to visualize the relationships between individuals in a population. We then provide a statistical analysis of these simulated population graphs, and show that they are distinct from randomly generated graphs, and also show differences from small-world graphs.",Kimberly Ayers; Maxwell Kooiker,,2025-08-12T21:51:08Z,http://arxiv.org/abs/2508.10058v1
2501.13366v1,Computationally Efficient Whole-Genome Signal Region Detection for   Quantitative and Binary Traits,"The identification of genetic signal regions in the human genome is critical for understanding the genetic architecture of complex traits and diseases. Numerous methods based on scan algorithms (i.e. QSCAN, SCANG, SCANG-STARR) have been developed to allow dynamic window sizes in whole-genome association studies. Beyond scan algorithms, we have recently developed the binary and re-search (BiRS) algorithm, which is more computationally efficient than scan-based methods and exhibits superior statistical power. However, the BiRS algorithm is based on two-sample mean test for binary traits, not accounting for multidimensional covariates or handling test statistics for non-binary outcomes. In this work, we present a distributed version of the BiRS algorithm (dBiRS) that incorporate a new infinity-norm test statistic based on summary statistics computed from a generalized linear model. The dBiRS algorithm accommodates regression-based statistics, allowing for the adjustment of covariates and the testing of both continuous and binary outcomes. This new framework enables parallel computing of block-wise results by aggregation through a central machine to ensure both detection accuracy and computational efficiency, and has theoretical guarantees for controlling family-wise error rates and false discovery rates while maintaining the power advantages of the original algorithm. Applying dBiRS to detect genetic regions associated with fluid intelligence and prospective memory using whole-exome sequencing data from the UK Biobank, we validate previous findings and identify numerous novel rare variants near newly implicated genes. These discoveries offer valuable insights into the genetic basis of cognitive performance and neurodegenerative disorders, highlighting the potential of dBiRS as a scalable and powerful tool for whole-genome signal region detection.",Wei Zhang; Fan Wang; Fang Yao,,2025-01-23T04:11:35Z,http://arxiv.org/abs/2501.13366v1
2501.15472v1,GiantHunter: Accurate detection of giant virus in metagenomic data using   reinforcement-learning and Monte Carlo tree search,"Motivation: Nucleocytoplasmic large DNA viruses (NCLDVs) are notable for their large genomes and extensive gene repertoires, which contribute to their widespread environmental presence and critical roles in processes such as host metabolic reprogramming and nutrient cycling. Metagenomic sequencing has emerged as a powerful tool for uncovering novel NCLDVs in environmental samples. However, identifying NCLDV sequences in metagenomic data remains challenging due to their high genomic diversity, limited reference genomes, and shared regions with other microbes. Existing alignment-based and machine learning methods struggle with achieving optimal trade-offs between sensitivity and precision. Results: In this work, we present GiantHunter, a reinforcement learning-based tool for identifying NCLDVs from metagenomic data. By employing a Monte Carlo tree search strategy, GiantHunter dynamically selects representative non-NCLDV sequences as the negative training data, enabling the model to establish a robust decision boundary. Benchmarking on rigorously designed experiments shows that GiantHunter achieves high precision while maintaining competitive sensitivity, improving the F1-score by 10% and reducing computational cost by 90% compared to the second-best method. To demonstrate its real-world utility, we applied GiantHunter to 60 metagenomic datasets collected from six cities along the Yangtze River, located both upstream and downstream of the Three Gorges Dam. The results reveal significant differences in NCLDV diversity correlated with proximity to the dam, likely influenced by reduced flow velocity caused by the dam. These findings highlight the potential of GiantSeeker to advance our understanding of NCLDVs and their ecological roles in diverse environments.",Fuchuan Qu; Cheng Peng; Jiaojiao Guan; Donglin Wang; Yanni Sun; Jiayu Shang,,2025-01-26T10:19:54Z,http://arxiv.org/abs/2501.15472v1
2503.09711v2,Genome evolution in an endangered freshwater mussel,"Nearly neutral theory predicts that evolutionary processes will differ in small populations compared to large populations, a key point of concern for endangered species. The nearly-neutral threshold, the span of neutral variation, and the adaptive potential from new mutations all differ depending on N_e. To determine how genomes respond in small populations, we have created a reference genome for a US federally endangered IUCN Red List freshwater mussel, Elliptio spinosa, and compare it to genetic variation for a common and successful relative, Elliptio crassidens. We find higher rates of background duplication rates in E. spinosa consistent with proposed theories of duplicate gene accumulation according to nearly-neutral processes. Along with these changes we observe fewer cases of adaptive gene family amplification in this endangered species. However, TE content is not consistent with nearly-neutral theory. We observe substantially less recent TE proliferation in the endangered species with over 500 Mb of newly copied TEs in Elliptio crassidens. These results suggest a more complex interplay between TEs and duplicate genes than previously proposed for small populations. They further suggest that TEs and duplications require greater attention in surveys of genomic health for endangered species.",Rebekah L. Rogers; John P. Wares; Jeffrey T. Garner,,2025-03-12T18:04:34Z,http://arxiv.org/abs/2503.09711v2
2503.11180v1,"Learnable Group Transform: Enhancing Genotype-to-Phenotype Prediction   for Rice Breeding with Small, Structured Datasets","Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding, enabling the identification of superior genotypes based on genomic data. Rice (Oryza sativa), one of the most important staple crops, faces challenges in improving yield and resilience due to the complex genetic architecture of agronomic traits and the limited sample size in breeding datasets. Current G2P prediction methods, such as GWAS and linear models, often fail to capture complex non-linear relationships between genotypes and phenotypes, leading to suboptimal prediction accuracy. Additionally, population stratification and overfitting are significant obstacles when models are applied to small datasets with diverse genetic backgrounds. This study introduces the Learnable Group Transform (LGT) method, which aims to overcome these challenges by combining the advantages of traditional linear models with advanced machine learning techniques. LGT utilizes a group-based transformation of genotype data to capture spatial relationships and genetic structures across diverse rice populations, offering flexibility to generalize even with limited data. Through extensive experiments on the Rice529 dataset, a panel of 529 rice accessions, LGT demonstrated substantial improvements in prediction accuracy for multiple agronomic traits, including yield and plant height, compared to state-of-the-art baselines such as linear models and recent deep learning approaches. Notably, LGT achieved an R^2 improvement of up to 15\% for yield prediction, significantly reducing error and demonstrating its ability to extract meaningful signals from high-dimensional, noisy genomic data. These results highlight the potential of LGT as a powerful tool for genomic prediction in rice breeding, offering a promising solution for accelerating the identification of high-yielding and resilient rice varieties.",Yunxuan Dong; Siyuan Chen; Jisen Zhang,,2025-03-14T08:27:19Z,http://arxiv.org/abs/2503.11180v1
2504.07298v1,CiMBA: Accelerating Genome Sequencing through On-Device Basecalling via   Compute-in-Memory,"As genome sequencing is finding utility in a wide variety of domains beyond the confines of traditional medical settings, its computational pipeline faces two significant challenges. First, the creation of up to 0.5 GB of data per minute imposes substantial communication and storage overheads. Second, the sequencing pipeline is bottlenecked at the basecalling step, consuming >40% of genome analysis time. A range of proposals have attempted to address these challenges, with limited success. We propose to address these challenges with a Compute-in-Memory Basecalling Accelerator (CiMBA), the first embedded ($\sim25$mm$^2$) accelerator capable of real-time, on-device basecalling, coupled with AnaLog (AL)-Dorado, a new family of analog focused basecalling DNNs. Our resulting hardware/software co-design greatly reduces data communication overhead, is capable of a throughput of 4.77 million bases per second, 24x that required for real-time operation, and achieves 17x/27x power/area efficiency over the best prior basecalling embedded accelerator while maintaining a high accuracy comparable to state-of-the-art software basecallers.",William Andrew Simon; Irem Boybat; Riselda Kodra; Elena Ferro; Gagandeep Singh; Mohammed Alser; Shubham Jain; Hsinyu Tsai; Geoffrey W. Burr; Onur Mutlu; Abu Sebastian,,2025-04-09T21:40:46Z,http://arxiv.org/abs/2504.07298v1
2505.12638v2,ChromFound: Towards A Universal Foundation Model for Single-Cell   Chromatin Accessibility Data,"The advent of single-cell Assay for Transposase-Accessible Chromatin using sequencing (scATAC-seq) offers an innovative perspective for deciphering regulatory mechanisms by assembling a vast repository of single-cell chromatin accessibility data. While foundation models have achieved significant success in single-cell transcriptomics, there is currently no foundation model for scATAC-seq that supports zero-shot high-quality cell identification and comprehensive multi-omics analysis simultaneously. Key challenges lie in the high dimensionality and sparsity of scATAC-seq data, as well as the lack of a standardized schema for representing open chromatin regions (OCRs). Here, we present ChromFound, a foundation model tailored for scATAC-seq. ChromFound utilizes a hybrid architecture and genome-aware tokenization to effectively capture genome-wide long contexts and regulatory signals from dynamic chromatin landscapes. Pretrained on 1.97 million cells from 30 tissues and 6 disease conditions, ChromFound demonstrates broad applicability across 6 diverse tasks. Notably, it achieves robust zero-shot performance in generating universal cell representations and exhibits excellent transferability in cell type annotation and cross-omics prediction. By uncovering enhancer-gene links undetected by existing computational methods, ChromFound offers a promising framework for understanding disease risk variants in the noncoding genome.",Yifeng Jiao; Yuchen Liu; Yu Zhang; Xin Guo; Yushuai Wu; Chen Jiang; Jiyang Li; Hongwei Zhang; Limei Han; Xin Gao; Yuan Qi; Yuan Cheng,,2025-05-19T02:45:42Z,http://arxiv.org/abs/2505.12638v2
2505.13503v1,Ancestry-Adjusted Polygenic Risk Scores for Predicting Obesity Risk in   the Indonesian Population,"Obesity prevalence in Indonesian adults increased from 10.5% in 2007 to 23.4% in 2023. Studies showed that genetic predisposition significantly influences obesity susceptibility. To aid this, polygenic risk scores (PRS) help aggregate the effects of numerous genetic variants to assess genetic risk. However, 91% of genome-wide association studies (GWAS) involve European populations, limiting their applicability to Indonesians due to genetic diversity. This study aims to develop and validate an ancestry adjusted PRS for obesity in the Indonesian population using principal component analysis (PCA) method constructed from the 1000 Genomes Project data and our own genomic data from approximately 2,800 Indonesians. We calculate PRS for obesity using all races, then determine the first four principal components using ancestry-informative SNPs and develop a linear regression model to predict PRS based on these principal components. The raw PRS is adjusted by subtracting the predicted score to obtain an ancestry adjusted PRS for the Indonesian population. Our results indicate that the ancestry-adjusted PRS improves obesity risk prediction. Compared to the unadjusted PRS, the adjusted score improved classification performance with a 5% increase in area under the ROC curve (AUC). This approach underscores the importance of population-specific adjustments in genetic risk assessments to enable more effective personalized healthcare and targeted intervention strategies for diverse populations.",Jocelyn Verna Siswanto; Belinda Mutiara; Felicia Austin; Jonathan Susanto; Cathelyn Theophila Tan; Restu Unggul Kresnadi; Kezia Irene,,2025-05-16T09:06:17Z,http://arxiv.org/abs/2505.13503v1
2505.17257v3,JanusDNA: A Powerful Bi-directional Hybrid DNA Foundation Model,"Large language models (LLMs) have revolutionized natural language processing and are increasingly applied to other sequential data types, including genetic sequences. However, adapting LLMs to genomics presents significant challenges. Capturing complex genomic interactions requires modeling long-range dependencies within DNA sequences, where interactions often span over 10,000 base pairs, even within a single gene, posing substantial computational burdens under conventional model architectures and training paradigms. Moreover, standard LLM training approaches are suboptimal for DNA: autoregressive training, while efficient, supports only unidirectional understanding. However, DNA is inherently bidirectional, e.g., bidirectional promoters regulate transcription in both directions and account for nearly 11% of human gene expression. Masked language models (MLMs) allow bidirectional understanding but are inefficient, as only masked tokens contribute to the loss per step. To address these limitations, we introduce JanusDNA, the first bidirectional DNA foundation model built upon a novel pretraining paradigm that combines the optimization efficiency of autoregressive modeling with the bidirectional comprehension of masked modeling. JanusDNA adopts a hybrid Mamba, Attention and Mixture of Experts (MoE) architecture, combining long-range modeling of Attention with efficient sequential learning of Mamba. MoE layers further scale model capacity via sparse activation while keeping computational cost low. Notably, JanusDNA processes up to 1 million base pairs at single nucleotide resolution on a single 80GB GPU. Extensive experiments and ablations show JanusDNA achieves new SOTA results on three genomic representation benchmarks, outperforming models with 250x more activated parameters. Code: https://github.com/Qihao-Duan/JanusDNA",Qihao Duan; Bingding Huang; Zhenqiao Song; Irina Lehmann; Lei Gu; Roland Eils; Benjamin Wild,,2025-05-22T20:10:55Z,http://arxiv.org/abs/2505.17257v3
2505.20836v1,HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic   Sequence Modeling,"Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.",Hexiong Yang; Mingrui Chen; Huaibo Huang; Junxian Duan; Jie Cao; Zhen Zhou; Ran He,,2025-05-27T07:57:35Z,http://arxiv.org/abs/2505.20836v1
2506.19598v2,Training Flexible Models of Genetic Variant Effects from Functional   Annotations using Accelerated Linear Algebra,"To understand how genetic variants in human genomes manifest in phenotypes -- traits like height or diseases like asthma -- geneticists have sequenced and measured hundreds of thousands of individuals. Geneticists use this data to build models that predict how a genetic variant impacts phenotype given genomic features of the variant, like DNA accessibility or the presence of nearby DNA-bound proteins. As more data and features become available, one might expect predictive models to improve. Unfortunately, training these models is bottlenecked by the need to solve expensive linear algebra problems because variants in the genome are correlated with nearby variants, requiring inversion of large matrices. Previous methods have therefore been restricted to fitting small models, and fitting simplified summary statistics, rather than the full likelihood of the statistical model. In this paper, we leverage modern fast linear algebra techniques to develop DeepWAS (Deep genome Wide Association Studies), a method to train large and flexible neural network predictive models to optimize likelihood. Notably, we find that larger models only improve performance when using our full likelihood approach; when trained by fitting traditional summary statistics, larger models perform no better than small ones. We find larger models trained on more features make better predictions, potentially improving disease predictions and therapeutic target identification.",Alan N. Amin; Andres Potapczynski; Andrew Gordon Wilson,,2025-06-24T13:07:45Z,http://arxiv.org/abs/2506.19598v2
2508.04757v1,Embedding Is (Almost) All You Need: Retrieval-Augmented Inference for   Generalizable Genomic Prediction Tasks,"Large pre-trained DNA language models such as DNABERT-2, Nucleotide Transformer, and HyenaDNA have demonstrated strong performance on various genomic benchmarks. However, most applications rely on expensive fine-tuning, which works best when the training and test data share a similar distribution. In this work, we investigate whether task-specific fine-tuning is always necessary. We show that simple embedding-based pipelines that extract fixed representations from these models and feed them into lightweight classifiers can achieve competitive performance. In evaluation settings with different data distributions, embedding-based methods often outperform fine-tuning while reducing inference time by 10x to 20x. Our results suggest that embedding extraction is not only a strong baseline but also a more generalizable and efficient alternative to fine-tuning, especially for deployment in diverse or unseen genomic contexts. For example, in enhancer classification, HyenaDNA embeddings combined with zCurve achieve 0.68 accuracy (vs. 0.58 for fine-tuning), with an 88% reduction in inference time and over 8x lower carbon emissions (0.02 kg vs. 0.17 kg CO2). In non-TATA promoter classification, DNABERT-2 embeddings with zCurve or GC content reach 0.85 accuracy (vs. 0.89 with fine-tuning) with a 22x lower carbon footprint (0.02 kg vs. 0.44 kg CO2). These results show that embedding-based pipelines offer over 10x better carbon efficiency while maintaining strong predictive performance. The code is available here: https://github.com/NIRJHOR-DATTA/EMBEDDING-IS-ALMOST-ALL-YOU-NEED.",Nirjhor Datta; Swakkhar Shatabda; M Sohel Rahman,,2025-08-06T14:15:48Z,http://arxiv.org/abs/2508.04757v1
2508.14934v1,AGP: A Novel Arabidopsis thaliana Genomics-Phenomics Dataset and its   HyperGraph Baseline Benchmarking,"Understanding which genes control which traits in an organism remains one of the central challenges in biology. Despite significant advances in data collection technology, our ability to map genes to traits is still limited. This genome-to-phenome (G2P) challenge spans several problem domains, including plant breeding, and requires models capable of reasoning over high-dimensional, heterogeneous, and biologically structured data. Currently, however, many datasets solely capture genetic information or solely capture phenotype information. Additionally, phenotype data is very heterogeneous, which many datasets do not fully capture. The critical drawback is that these datasets are not integrated, that is, they do not link with each other to describe the same biological specimens. This limits machine learning models' ability to be informed on the various aspects of these specimens, impacting the breadth of correlations learned, and therefore their ability to make more accurate predictions. To address this gap, we present the Arabidopsis Genomics-Phenomics (AGP) Dataset, a curated multi-modal dataset linking gene expression profiles with phenotypic trait measurements in Arabidopsis thaliana, a model organism in plant biology. AGP supports tasks such as phenotype prediction and interpretable graph learning. In addition, we benchmark conventional regression and explanatory baselines, including a biologically-informed hypergraph baseline, to validate gene-trait associations. To the best of our knowledge, this is the first dataset that provides multi-modal gene information and heterogeneous trait or phenotype data for the same Arabidopsis thaliana specimens. With AGP, we aim to foster the research community towards accurately understanding the connection between genotypes and phenotypes using gene information, higher-order gene pairings, and trait data from several sources.",Manuel Serna-Aguilera; Fiona L. Goggin; Aranyak Goswami; Alexander Bucksch; Suxing Liu; Khoa Luu,,2025-08-19T21:21:23Z,http://arxiv.org/abs/2508.14934v1
2502.18758v1,Genotype-to-Phenotype Prediction in Rice with High-Dimensional Nonlinear   Features,"Genotype-to-Phenotype prediction can promote advances in modern genomic research and crop improvement, guiding precision breeding and genomic selection. However, high-dimensional nonlinear features often hinder the accuracy of genotype-to-phenotype prediction by increasing computational complexity. The challenge also limits the predictive accuracy of traditional approaches. Therefore, effective solutions are needed to improve the accuracy of genotype-to-phenotype prediction. In our paper, we propose MLFformer. MLFformer is a Transformer-based architecture that incorporates the Fast Attention mechanism and a multilayer perceptron module to handle high-dimensional nonlinear features. In MLFformer, the Fast Attention mechanism is utilized to handle computational complexity and enhance processing efficiency. In addition, the MLP structure further captures high-dimensional nonlinear features. Through experiments, the results show that MLFformer reduces the average MAPE by 7.73% compared to the vanilla Transformer. In univariate and multivariate prediction scenarios, MLFformer achieves the best predictive performance among all compared models.",Zeyuan Zhou; Siyuan Chen; Xinzhang Wu; Jisen Zhang; Yunxuan Dong,,2025-02-26T02:28:18Z,http://arxiv.org/abs/2502.18758v1
2503.14520v1,From de Bruijn graphs to variation graphs-relationships between   pangenome models,"Pangenomes serve as a framework for joint analysis of genomes of related organisms. Several pangenome models were proposed, offering different functionalities, applications provided by available tools, their efficiency etc. Among them, two graph-based models are particularly widely used: variation graphs and de Bruijn graphs. In the current paper we propose an axiomatization of the desirable properties of a graph representation of a collection of strings. We show the relationship between variation graphs satisfying these criteria and de Bruijn graphs. This relationship can be used to efficiently build a variation graph representing a given set of genomes, transfer annotations between both models, compare the results of analyzes based on each model etc.",Adam Cicherski; Norbert Dojer,,2025-03-14T15:23:52Z,http://arxiv.org/abs/2503.14520v1
2504.10330v1,Can genomic analysis actually estimate past population size?,"Genomic data can be used to reconstruct population size over thousands of generations, using a new class of algorithms (SMC methods). These analyses often show a recent decline in $N_e$ (effective size), which at face value implies a conservation or demographic crisis: a population crash and loss of genetic diversity. This interpretation is frequently mistaken. Here we outline how SMC methods work, why they generate this misleading signal, and suggest simple approaches for exploiting the rich information produced by these algorithms. In most species, genomic patterns reflect major changes in the species' range and subdivision over tens or hundreds of thousands of years. Consequently, collaboration between geneticists, palaeoecologists, palaeoclimatologists, and geologists is crucial for evaluating the outputs of SMC algorithms.",Janeesh K. Bansal; Richard A. Nichols,,2025-04-14T15:37:55Z,http://arxiv.org/abs/2504.10330v1
2505.02195v2,Scalable Genomic Context Analysis with GCsnap2 on HPC Clusters,"GCsnap2 Cluster is a scalable, high performance tool for genomic context analysis, developed to overcome the limitations of its predecessor, GCsnap1 Desktop. Leveraging distributed computing with mpi4py[.]futures, GCsnap2 Cluster achieved a 22x improvement in execution time and can now perform genomic context analysis for hundreds of thousands of input sequences in HPC clusters. Its modular architecture enables the creation of task-specific workflows and flexible deployment in various computational environments, making it well suited for bioinformatics studies of large-scale datasets. This work highlights the potential for applying similar approaches to solve scalability challenges in other scientific domains that rely on large-scale data analysis pipelines.",Reto Krummenacher; Osman Seckin Simsek; Michèle Leemann; Leila T. Alexander; Torsten Schwede; Florina M. Ciorba; Joana Pereira,,2025-05-04T17:30:44Z,http://arxiv.org/abs/2505.02195v2
2505.09873v1,Deep Learning and Explainable AI: New Pathways to Genetic Insights,"Deep learning-based AI models have been extensively applied in genomics, achieving remarkable success across diverse applications. As these models gain prominence, there exists an urgent need for interpretability methods to establish trustworthiness in model-driven decisions. For genetic researchers, interpretable insights derived from these models hold significant value in providing novel perspectives for understanding biological processes. Current interpretability analyses in genomics predominantly rely on intuition and experience rather than rigorous theoretical foundations. In this review, we systematically categorize interpretability methods into input-based and model-based approaches, while critically evaluating their limitations through concrete biological application scenarios. Furthermore, we establish theoretical underpinnings to elucidate the origins of these constraints through formal mathematical demonstrations, aiming to assist genetic researchers in better understanding and designing models in the future. Finally, we provide feasible suggestions for future research on interpretability in the field of genetics.",Chenyu Wang; Chaoying Zuo; Zihan Su; Yuhang Xing; Lu Li; Maojun Wang; Zeyu Zhang,,2025-05-15T00:37:03Z,http://arxiv.org/abs/2505.09873v1
2505.10983v1,GenoArmory: A Unified Evaluation Framework for Adversarial Attacks on   Genomic Foundation Models,"We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GenoArmory. Unlike existing GFM benchmarks, GenoArmory offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Additionally, we introduce GenoAdv, a new adversarial sample dataset designed to improve GFM safety. Empirically, classification models exhibit greater robustness to adversarial perturbations compared to generative models, highlighting the impact of task type on model vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features.",Haozheng Luo; Chenghao Qiu; Yimin Wang; Shang Wu; Jiahao Yu; Han Liu; Binghui Wang; Yan Chen,,2025-05-16T08:29:56Z,http://arxiv.org/abs/2505.10983v1
2505.15866v1,Multi-omic Causal Discovery using Genotypes and Gene Expression,"Causal discovery in multi-omic datasets is crucial for understanding the bigger picture of gene regulatory mechanisms, but remains challenging due to high dimensionality, differentiation of direct from indirect relationships, and hidden confounders. We introduce GENESIS (GEne Network inference from Expression SIgnals and SNPs), a constraint-based algorithm that leverages the natural causal precedence of genotypes to infer ancestral relationships in transcriptomic data. Unlike traditional causal discovery methods that start with a fully connected graph, GENESIS initialises an empty ancestrality matrix and iteratively populates it with direct, indirect or non-causal relationships using a series of provably sound marginal and conditional independence tests. By integrating genotypes as fixed causal anchors, GENESIS provides a principled ``head start'' to classical causal discovery algorithms, restricting the search space to biologically plausible edges. We test GENESIS on synthetic and real-world genomic datasets. This framework offers a powerful avenue for uncovering causal pathways in complex traits, with promising applications to functional genomics, drug discovery, and precision medicine.",Stephen Asiedu; David Watson,,2025-05-21T11:52:23Z,http://arxiv.org/abs/2505.15866v1
2505.23289v1,Intermediate State Formation of Topologically Associated Chromatin   Domains using Quantum Annealing,"Topologically Associating Chromatic Domains are spatially distinct chromatin regions that regulate transcription by segregating active and inactive genomic elements. Empirical studies show that their formation correlates with local patterns of epigenetic markers, yet the precise mechanisms linking 1D epigenetic landscapes to 3D chromatin folding remain unclear. Recent models represent chromatin as a spin system, where nucleosomes are treated as discrete-state variables coupled by interaction strengths derived from genomic and epigenomic data. Classical samplers struggle with these models due to high frustration and dense couplings. Here, we present a quantum annealing (QA) approach to efficiently sample chromatin states, embedding an epigenetic Ising model into the topology of D-Wave quantum processors.",Tobias Kempe; S. M. Ali Tabei; Mohammad H. Ansari,,2025-05-29T09:40:39Z,http://arxiv.org/abs/2505.23289v1
2506.09076v2,A Probabilistic Framework for Imputing Genetic Distances in   Spatiotemporal Pathogen Models,"Pathogen genome data offers valuable structure for spatial models, but its utility is limited by incomplete sequencing coverage. We propose a probabilistic framework for inferring genetic distances between unsequenced cases and known sequences within defined transmission chains, using time-aware evolutionary distance modeling. The method estimates pairwise divergence from collection dates and observed genetic distances, enabling biologically plausible imputation grounded in observed divergence patterns, without requiring sequence alignment or known transmission chains. Applied to highly pathogenic avian influenza A/H5 cases in wild birds in the United States, this approach supports scalable, uncertainty-aware augmentation of genomic datasets and enhances the integration of evolutionary information into spatiotemporal modeling workflows.",Haley Stone; Jing Du; Hao Xue; Matthew Scotch; David Heslop; Andreas Züfle; Chandini Raina MacIntyre; Flora Salim,,2025-06-10T02:41:46Z,http://arxiv.org/abs/2506.09076v2
2506.10271v3,Evaluating DNA function understanding in genomic language models using   evolutionarily implausible sequences,"Genomic language models (gLMs) hold promise for generating novel, functional DNA sequences for synthetic biology. However, realizing this potential requires models to go beyond evolutionary plausibility and understand how DNA sequence encodes gene expression and regulation. We introduce a benchmark called Nullsettes, which assesses how well models can predict in silico loss-of-function (LOF) mutations, in synthetic expression cassettes with little evolutionary precedent. Testing 12 state-of-the-art gLMs, we find that most fail to consistently detect these strong LOF mutations. All models show a sharp drop in predictive accuracy as the likelihood assigned to the original (nonmutant) sequence decreases, suggesting that gLMs rely heavily on pattern-matching to their evolutionary prior rather than on any mechanistic understanding of gene expression. Our findings highlight fundamental limitations in how gLMs generalize to engineered, non-natural sequences, and underscore the need for benchmarks and modeling strategies that prioritize functional understanding.",Shiyu Jiang; Xuyin Liu; Zitong Jerry Wang,,2025-06-12T01:28:04Z,http://arxiv.org/abs/2506.10271v3
2506.18940v1,eccDNAMamba: A Pre-Trained Model for Ultra-Long eccDNA Sequence Analysis,"Extrachromosomal circular DNA (eccDNA) plays key regulatory roles and contributes to oncogene overexpression in cancer through high-copy amplification and long-range interactions. Despite advances in modeling, no pre-trained models currently support full-length circular eccDNA for downstream analysis. Existing genomic models are either limited to single-nucleotide resolution or hindered by the inefficiency of the quadratic attention mechanism. Here, we introduce eccDNAMamba, the first bidirectional state-space encoder tailored for circular DNA sequences. It combines forward and reverse passes for full-context representation learning with linear-time complexity, and preserves circular structure through a novel augmentation strategy. Tested on two real-world datasets, eccDNAMamba achieves strong classification performance and scales to sequences up to 200 Kbp, offering a robust and efficient framework for modeling circular genomes. Our codes are available at https://github.com/zzq1zh/GenAI-Lab.",Zhenke Liu; Jien Li; Ziqi Zhang,,2025-06-22T17:50:57Z,http://arxiv.org/abs/2506.18940v1
2506.22963v1,CN-SBM: Categorical Block Modelling For Primary and Residual Copy Number   Variation,"Cancer is a genetic disorder whose clonal evolution can be monitored by tracking noisy genome-wide copy number variants. We introduce the Copy Number Stochastic Block Model (CN-SBM), a probabilistic framework that jointly clusters samples and genomic regions based on discrete copy number states using a bipartite categorical block model. Unlike models relying on Gaussian or Poisson assumptions, CN-SBM respects the discrete nature of CNV calls and captures subpopulation-specific patterns through block-wise structure. Using a two-stage approach, CN-SBM decomposes CNV data into primary and residual components, enabling detection of both large-scale chromosomal alterations and finer aberrations. We derive a scalable variational inference algorithm for application to large cohorts and high-resolution data. Benchmarks on simulated and real datasets show improved model fit over existing methods. Applied to TCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and structured residual variation, aiding patient stratification in survival analysis. These results establish CN-SBM as an interpretable, scalable framework for CNV analysis with direct relevance for tumor heterogeneity and prognosis.",Kevin Lam; William Daniels; J Maxwell Douglas; Daniel Lai; Samuel Aparicio; Benjamin Bloem-Reddy; Yongjin Park,,2025-06-28T17:45:45Z,http://arxiv.org/abs/2506.22963v1
2507.07486v1,Sparse Autoencoders Reveal Interpretable Structure in Small Gene   Language Models,"Sparse autoencoders (SAEs) have recently emerged as a powerful tool for interpreting the internal representations of large language models (LLMs), revealing latent latent features with semantical meaning. This interpretability has also proven valuable in biological domains: applying SAEs to protein language models uncovered meaningful features related to protein structure and function. More recently, SAEs have been used to analyze genomics-focused models such as Evo 2, identifying interpretable features in gene sequences. However, it remains unclear whether SAEs can extract meaningful representations from small gene language models, which have fewer parameters and potentially less expressive capacity. To address it, we propose applying SAEs to the activations of a small gene language model. We demonstrate that even small-scale models encode biologically relevant genomic features, such as transcription factor binding motifs, that SAEs can effectively uncover. Our findings suggest that compact gene language models are capable of learning structured genomic representations, and that SAEs offer a scalable approach for interpreting gene models across various model sizes.",Haoxiang Guan; Jiyan He; Jie Zhang,,2025-07-10T07:13:54Z,http://arxiv.org/abs/2507.07486v1
2507.08043v1,Topological Sequence Analysis of Genomes: Category Approaches,"Sequence data, such as DNA, RNA, and protein sequences, exhibit intricate, multi-scale structures that pose significant challenges for conventional analysis methods, particularly those relying on alignment or purely statistical representations. In this work, we introduce category-based topological sequence analysis (CTSA ) of genomes. CTSA models a sequence as a resolution category, capturing its hierarchical structure through a categorical construction. Substructure complexes are then derived from this categorical representation, and their persistent homology is computed to extract multi-scale topological features. Our models depart from traditional alignment-free approaches by incorporating structured mathematical formalisms rooted in sequence topology. The resulting topological signatures provide informative representations across a variety of tasks, including the phylogenetic analysis of SARS-CoV-2 variants and the prediction of protein-nucleic acid binding affinities. Comparative studies were carried out against six state-of-the-art methods. Experimental results demonstrate that CTSA achieves excellent and consistent performance in these tasks, suggesting its general applicability and robustness. Beyond sequence analysis, the proposed framework opens new directions for the integration of categorical and homological theories for biological sequence analysis.",Jian Liu; Li Shen; Mushal Zia; Guo-Wei Wei,,2025-07-09T22:00:04Z,http://arxiv.org/abs/2507.08043v1
2507.09754v2,Explainable AI in Genomics: Transcription Factor Binding Site Prediction   with Mixture of Experts,"Transcription Factor Binding Site (TFBS) prediction is crucial for understanding gene regulation and various biological processes. This study introduces a novel Mixture of Experts (MoE) approach for TFBS prediction, integrating multiple pre-trained Convolutional Neural Network (CNN) models, each specializing in different TFBS patterns. We evaluate the performance of our MoE model against individual expert models on both in-distribution and out-of-distribution (OOD) datasets, using six randomly selected transcription factors (TFs) for OOD testing. Our results demonstrate that the MoE model achieves competitive or superior performance across diverse TF binding sites, particularly excelling in OOD scenarios. The Analysis of Variance (ANOVA) statistical test confirms the significance of these performance differences. Additionally, we introduce ShiftSmooth, a novel attribution mapping technique that provides more robust model interpretability by considering small shifts in input sequences. Through comprehensive explainability analysis, we show that ShiftSmooth offers superior attribution for motif discovery and localization compared to traditional Vanilla Gradient methods. Our work presents an efficient, generalizable, and interpretable solution for TFBS prediction, potentially enabling new discoveries in genome biology and advancing our understanding of transcriptional regulation.",Aakash Tripathi; Ian E. Nielsen; Muhammad Umer; Ravi P. Ramachandran; Ghulam Rasool,,2025-07-13T19:21:41Z,http://arxiv.org/abs/2507.09754v2
2507.13785v1,MorphoNAS: Embryogenic Neural Architecture Search Through   Morphogen-Guided Development,"While biological neural networks develop from compact genomes using relatively simple rules, modern artificial neural architecture search methods mostly involve explicit and routine manual work. In this paper, we introduce MorphoNAS (Morphogenetic Neural Architecture Search), a system able to deterministically grow neural networks through morphogenetic self-organization inspired by the Free Energy Principle, reaction-diffusion systems, and gene regulatory networks. In MorphoNAS, simple genomes encode just morphogens dynamics and threshold-based rules of cellular development. Nevertheless, this leads to self-organization of a single progenitor cell into complex neural networks, while the entire process is built on local chemical interactions. Our evolutionary experiments focused on two different domains: structural targeting, in which MorphoNAS system was able to find fully successful genomes able to generate predefined random graph configurations (8-31 nodes); and functional performance on the CartPole control task achieving low complexity 6-7 neuron solutions when target network size minimization evolutionary pressure was applied. The evolutionary process successfully balanced between quality of of the final solutions and neural architecture search effectiveness. Overall, our findings suggest that the proposed MorphoNAS method is able to grow complex specific neural architectures, using simple developmental rules, which suggests a feasible biological route to adaptive and efficient neural architecture search.",Mykola Glybovets; Sergii Medvid,,2025-07-18T09:57:49Z,http://arxiv.org/abs/2507.13785v1
2508.04739v1,CodonMoE: DNA Language Models for mRNA Analyses,"Genomic language models (gLMs) face a fundamental efficiency challenge: either maintain separate specialized models for each biological modality (DNA and RNA) or develop large multi-modal architectures. Both approaches impose significant computational burdens - modality-specific models require redundant infrastructure despite inherent biological connections, while multi-modal architectures demand massive parameter counts and extensive cross-modality pretraining. To address this limitation, we introduce CodonMoE (Adaptive Mixture of Codon Reformative Experts), a lightweight adapter that transforms DNA language models into effective RNA analyzers without RNA-specific pretraining. Our theoretical analysis establishes CodonMoE as a universal approximator at the codon level, capable of mapping arbitrary functions from codon sequences to RNA properties given sufficient expert capacity. Across four RNA prediction tasks spanning stability, expression, and regulation, DNA models augmented with CodonMoE significantly outperform their unmodified counterparts, with HyenaDNA+CodonMoE series achieving state-of-the-art results using 80% fewer parameters than specialized RNA models. By maintaining sub-quadratic complexity while achieving superior performance, our approach provides a principled path toward unifying genomic language modeling, leveraging more abundant DNA data and reducing computational overhead while preserving modality-specific performance advantages.",Shiyi Du; Litian Liang; Jiayi Li; Carl Kingsford,,2025-08-06T01:40:12Z,http://arxiv.org/abs/2508.04739v1
2508.05511v1,Adaptive Parallel Downloader for Large Genomic Datasets,"Modern next-generation sequencing (NGS) projects routinely generate terabytes of data, which researchers commonly download from public repositories such as SRA or ENA. Existing download tools often employ static concurrency settings, leading to inefficient bandwidth utilization and prolonged download times due to their inability to adapt to dynamic network conditions. We introduce FastBioDL, a parallel file downloader designed for large biological datasets, featuring an adaptive concurrency controller. FastBioDL frames the download process as an online optimization problem, utilizing a utility function and gradient descent to adjust the number of concurrent socket streams in real-time dynamically. This approach maximizes download throughput while minimizing resource overhead. Comprehensive evaluations on public genomic datasets demonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art tools. Moreover, in high-speed network experiments, its adaptive design was up to $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP or FTP downloads on the client side, FastBioDL provides a robust and efficient solution for large-scale genomic data acquisition, democratizing high-performance data retrieval for researchers without requiring specialized commercial software or protocols.",Rasman Mubtasim Swargo; Engin Arslan; Md Arifuzzaman,,2025-08-07T15:46:38Z,http://arxiv.org/abs/2508.05511v1
2508.06184v1,Scalable Quantum State Preparation for Encoding Genomic Data with Matrix   Product States,"As quantum computing hardware advances, the need for algorithms that facilitate the loading of classical data into the quantum states of these devices has become increasingly important. This study presents a method for producing scalable quantum circuits to encode genomic data using the Matrix Product State (MPS) formalism. The method is illustrated by encoding the genome of the bacteriophage $\Phi X174$ into a 15-qubit state, and analysing the trade-offs between MPS bond dimension, reconstruction error, and the resulting circuit complexity. This study proposes methods for optimising encoding circuits with standard benchmark datasets for the emerging field of quantum bioinformatics. The results for circuit generation and simulation on HPC and on current quantum hardware demonstrate the viability and utility of the encoding.",Floyd M. Creevey; Hitham T. Hassan; James McCafferty; Lloyd C. L. Hollenberg; Sergii Strelchuk,,2025-08-08T10:02:53Z,http://arxiv.org/abs/2508.06184v1
2508.10703v1,GenOM: Ontology Matching with Description Generation and Large Language   Model,"Ontology matching (OM) plays an essential role in enabling semantic interoperability and integration across heterogeneous knowledge sources, particularly in the biomedical domain which contains numerous complex concepts related to diseases and pharmaceuticals. This paper introduces GenOM, a large language model (LLM)-based ontology alignment framework, which enriches the semantic representations of ontology concepts via generating textual definitions, retrieves alignment candidates with an embedding model, and incorporates exact matching-based tools to improve precision. Extensive experiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often achieve competitive performance, surpassing many baselines including traditional OM systems and recent LLM-based methods. Further ablation studies confirm the effectiveness of semantic enrichment and few-shot prompting, highlighting the framework's robustness and adaptability.",Yiping Song; Jiaoyan Chen; Renate A. Schmidt,,2025-08-14T14:48:09Z,http://arxiv.org/abs/2508.10703v1
2508.20130v1,Artificial Intelligence for CRISPR Guide RNA Design: Explainable Models   and Off-Target Safety,"CRISPR-based genome editing has revolutionized biotechnology, yet optimizing guide RNA (gRNA) design for efficiency and safety remains a critical challenge. Recent advances (2020--2025, updated to reflect current year if needed) demonstrate that artificial intelligence (AI), especially deep learning, can markedly improve the prediction of gRNA on-target activity and identify off-target risks. In parallel, emerging explainable AI (XAI) techniques are beginning to illuminate the black-box nature of these models, offering insights into sequence features and genomic contexts that drive Cas enzyme performance. Here we review how state-of-the-art machine learning models are enhancing gRNA design for CRISPR systems, highlight strategies for interpreting model predictions, and discuss new developments in off-target prediction and safety assessment. We emphasize breakthroughs from top-tier journals that underscore an interdisciplinary convergence of AI and genome editing to enable more efficient, specific, and clinically viable CRISPR applications.",Alireza Abbaszadeh; Armita Shahlai,,2025-08-26T13:34:15Z,http://arxiv.org/abs/2508.20130v1
2506.10931v2,MARS: Processing-In-Memory Acceleration of Raw Signal Genome Analysis   Inside the Storage Subsystem,"Raw signal genome analysis (RSGA) has emerged as a promising approach to enable real-time genome analysis by directly analyzing raw electrical signals. However, rapid advancements in sequencing technologies make it increasingly difficult for software-based RSGA to match the throughput of raw signal generation. This paper demonstrates that while hardware acceleration techniques can significantly accelerate RSGA, the high volume of genomic data shifts the performance and energy bottleneck from computation to I/O data movement. As sequencing throughput increases, I/O overhead becomes the main contributor to both runtime and energy consumption. Therefore, there is a need to design a high-performance, energy-efficient system for RSGA that can both alleviate the data movement bottleneck and provide large acceleration capabilities. We propose MARS, a storage-centric system that leverages the heterogeneous resources within modern storage systems (e.g., storage-internal DRAM, storage controller, flash chips) alongside their large storage capacity to tackle both data movement and computational overheads of RSGA in an area-efficient and low-cost manner. MARS accelerates RSGA through a novel hardware/software co-design approach. First, MARS modifies the RSGA pipeline via two filtering mechanisms and a quantization scheme, reducing hardware demands and optimizing for in-storage execution. Second, MARS accelerates the RSGA steps directly within the storage by leveraging both Processing-Near-Memory and Processing-Using-Memory paradigms. Third, MARS orchestrates the execution of all steps to fully exploit in-storage parallelism and minimize data movement. Our evaluation shows that MARS outperforms basecalling-based software and hardware-accelerated state-of-the-art read mapping pipelines by 93x and 40x, on average across different datasets, while reducing their energy consumption by 427x and 72x.",Melina Soysal; Konstantina Koliogeorgi; Can Firtina; Nika Mansouri Ghiasi; Rakesh Nadig; Haiyu Mao; Geraldo F. Oliveira; Yu Liang; Klea Zambaku; Mohammad Sadrosadati; Onur Mutlu,,2025-06-12T17:38:12Z,http://arxiv.org/abs/2506.10931v2
2507.05265v1,BMFM-DNA: A SNP-aware DNA foundation model to capture variant effects,"Large language models (LLMs) trained on text demonstrated remarkable results on natural language processing (NLP) tasks. These models have been adapted to decipher the language of DNA, where sequences of nucleotides act as ""words"" that encode genomic functions. However, the genome differs fundamentally from natural language, as it lacks clearly defined words or a consistent grammar. Although DNA language models (DNALMs) such as DNABERT, GENA-LM have achieved high level of performance on genome-related biological tasks, these models do not encode biological functions in the presence of sequence variations. To address this problem, we pre-train foundation models that effectively integrate sequence variations, in particular Single Nucleotide Polymorphisms (SNPs), as they underlie important biological functions. Specifically, we use ModernBERT to pre-train two different Biomedical Foundation Models (BMFM), namely, BMFM-DNA-REF in which the model is trained with sequences of varying lengths along with their reverse complements derived from the reference genome and BMFM-DNA-SNP in which the model is trained with sequences created using a novel representation scheme that encodes sequence variations. Our findings indicate that integrating sequence variations into DNALMs helps capture the biological functions as seen in improvements on all fine-tuning tasks. To explore the model's practical utility, we experimented with various strategies for SNP imputation on promoter detection task introduced in DNABERT-2. However, we acknowledge that the current benchmarks are limited in their ability to fully evaluate these models. To enable more comprehensive assessment in the future and encourage community contributions, we release our models through HuggingFace and the code to reproduce the results at https://github.com/BiomedSciAI/biomed-multi-omic",Hongyang Li; Sanjoy Dey; Bum Chul Kwon; Michael Danziger; Michal Rosen-Tzvi; Jianying Hu; James Kozloski; Ching-Huei Tsou; Bharath Dandala; Pablo Meyer,,2025-06-26T13:56:32Z,http://arxiv.org/abs/2507.05265v1
2501.02028v2,Selecting ChIP-seq Normalization Methods from the Perspective of their   Technical Conditions,"Chromatin immunoprecipitation with high-throughput sequencing (ChIP-seq) provides insights into both the genomic location occupied by the protein of interest and the difference in DNA occupancy between experimental states. Given that ChIP-seq data is collected experimentally, an important step for determining regions with differential DNA occupancy between states is between-sample normalization. While between-sample normalization is crucial for downstream differential binding analysis, the technical conditions underlying between-sample normalization methods have yet to be examined for ChIP-seq. We identify three important technical conditions underlying ChIP-seq between-sample normalization methods: balanced differential DNA occupancy, equal total DNA occupancy, and equal background binding across states. To illustrate satisfying the selected normalization method's technical conditions for downstream differential binding analysis, we simulate ChIP-seq read count data where different combinations of the technical conditions are violated. We then externally verify our simulation results using experimental data. Based on our findings, we suggest that researchers use their understanding of the ChIP-seq experiment at hand to guide their choice of between-sample normalization method. Alternatively, researchers can use a high-confidence peakset, which is the intersection of the differentially bound peaksets obtained from using different between-sample normalization methods. In our two experimental analyses, roughly half of the called peaks were called as differentially bound for every normalization method. High-confidence peaks are less sensitive to choice of between-sample normalization method and could be a more robust basis for identifying genomic regions with differential DNA occupancy between experimental states when there is uncertainty about which technical conditions are satisfied.",Sara Colando; Danae Schulz; Johanna Hardin,,2025-01-03T06:24:06Z,http://arxiv.org/abs/2501.02028v2
2501.07043v1,From trees to traits: A review of advances in PhyloG2P methods and   future directions,"Mapping genotypes to phenotypes (G2P) is a fundamental goal in biology. So called PhyloG2P methods are a relatively new set of tools that leverage replicated evolution in phylogenetically independent lineages to identify genomic regions associated with traits of interest. Here, we review recent developments in PhyloG2P methods, focusing on three key areas: methods based on replicated amino acid substitutions, methods detecting changes in evolutionary rates, and methods analysing gene duplication and loss. We discuss how the definition and measurement of traits impacts the utility of these methods, arguing that focusing on simple rather than compound traits will lead to more meaningful genotype-phenotype associations. We advocate for the use of methods that work with continuous traits directly rather than collapsing them to binary representations. We examine the strengths and limitations of different approaches to modeling genetic replication, highlighting the importance of explicit modeling of evolutionary processes. Finally, we outline promising future directions, including the integration of population-level variation, as well as epigenetic and environmental information. No one method is likely to identify all genomic regions of interest, so we encourage users to apply multiple methods that are capable of detecting a wide range of associations. The overall aim of this review is to provide practitioners a roadmap for understanding and applying PhyloG2P methods.",Arlie R. Macdonald; Maddie E. James; Jonathan D. Mitchell; Barbara R. Holland,,2025-01-13T03:51:07Z,http://arxiv.org/abs/2501.07043v1
2501.11831v1,Genomic Analysis of Date Palm Fruit Size Traits and Identification of   Candidate Genes through GWAS,"The commercial value of economically significant fruits, including date palm fruit (dates), is influenced by various factors, such as biochemical composition and morphological features like size, shape, and visual appearance, which are key determinants of their quality and market value. Dates are typically consumed at the dry stage (Tamar), during which they exhibit a wide range of physical characteristics, such as color, length, weight, and skin appearance. Understanding the genetic basis of these traits is crucial for improving crop quality and breeding new cultivars. In this study, we integrated a genome dataset from highly diverse date cultivars with phenotypes of dry fruit such as length, width, area, and weight, identifying multiple significant genetic loci (SNPs) associated with these traits. We also identified candidate genes located near the associated SNPs that are involved in biological processes such as cell differentiation, proliferation, growth, and the regulation of signalling pathways for growth regulators like auxin and abscisic acid, as observed in other plants. Gene expression analysis reveals that many of these genes are highly expressed in the early stage of fruit development when the fruit attains its maximum size and weight. These findings will enhance our understanding of genetic determinants of fruit size particularly at the commercially important Tamar stage.",Shameem Younuskunju; Yasmin A. Mohamoud; Lisa Sara Mathew; Klaus F. X. Mayer; Karsten Suhre; Joel A. Malek,,2025-01-21T02:21:28Z,http://arxiv.org/abs/2501.11831v1
2502.00934v2,Optimizing Global Genomic Surveillance for Early Detection of Emerging   SARS-CoV-2 Variants,"Background: Global viral threats underscore the need for effective genomic surveillance, but high costs and uneven resource distribution hamper its implementation. Targeting surveillance to international travelers in major travel hubs may offer a more efficient strategy for the early detection of SARS-CoV-2 variants.   Methods: We developed and calibrated a multiple-strain metapopulation model of global SARS-CoV-2 transmission using extensive epidemiological, phylogenetic, and high-resolution air travel data. We then compared baseline surveillance with various resource-allocation approaches that prioritize travelers, focusing on Omicron BA.1/BA.2 retrospectively and on hypothetical future variants under different emergence, transmission and vaccine effectiveness scenarios.   Findings: Focusing existing surveillance resources on travelers at key global hubs significantly shortened detection delays without increasing total surveillance efforts. In retrospective analyses of Omicron BA.1/BA.2, traveler-targeted approaches consistently outperformed baseline strategies, even when overall resources were reduced. Simulations indicate that focusing surveillance on key travel hubs outperform baseline practices in detecting future variants, across different possible origins, even with reduced resources. This approach also remains effective in future pandemic scenarios with varying reproductive numbers and vaccine effectiveness.   Interpretation: These findings provide a quantitative, cost-effective framework for strengthening global genomic surveillance. By reallocating resources toward international travelers in select travel hubs, early detection of emerging variants can be enhanced, informing rapid public health interventions and bolstering preparedness for future pandemics.",Haogao Gu; Jifan Li; Wanying Sun; Mengting Li; Kathy Leung; Joseph T. Wu; Hsiang-Yu Yuan; Maggie H. Wang; Bingyi Yang; Matthew R. McKay; Ning Ning; Leo L. M. Poon,,2025-02-02T21:55:05Z,http://arxiv.org/abs/2502.00934v2
2503.01459v1,Primer C-VAE: An interpretable deep learning primer design method to   detect emerging virus variants,"Motivation: PCR is more economical and quicker than Next Generation Sequencing for detecting target organisms, with primer design being a critical step. In epidemiology with rapidly mutating viruses, designing effective primers is challenging. Traditional methods require substantial manual intervention and struggle to ensure effective primer design across different strains. For organisms with large, similar genomes like Escherichia coli and Shigella flexneri, differentiating between species is also difficult but crucial.   Results: We developed Primer C-VAE, a model based on a Variational Auto-Encoder framework with Convolutional Neural Networks to identify variants and generate specific primers. Using SARS-CoV-2, our model classified variants (alpha, beta, gamma, delta, omicron) with 98% accuracy and generated variant-specific primers. These primers appeared with >95% frequency in target variants and <5% in others, showing good performance in in-silico PCR tests. For Alpha, Delta, and Omicron, our primer pairs produced fragments <200 bp, suitable for qPCR detection. The model also generated effective primers for organisms with longer gene sequences like E. coli and S. flexneri.   Conclusion: Primer C-VAE is an interpretable deep learning approach for developing specific primer pairs for target organisms. This flexible, semi-automated and reliable tool works regardless of sequence completeness and length, allowing for qPCR applications and can be applied to organisms with large and highly similar genomes.",Hanyu Wang; Emmanuel K. Tsinda; Anthony J. Dunn; Francis Chikweto; Alain B. Zemkoho,,2025-03-03T12:17:19Z,http://arxiv.org/abs/2503.01459v1
2503.13078v1,Bayesian Cox model with graph-structured variable selection priors for   multi-omics biomarker identification,"An important goal in cancer research is the survival prognosis of a patient based on a minimal panel of genomic and molecular markers such as genes or proteins. Purely data-driven models without any biological knowledge can produce non-interpretable results. We propose a penalized semiparametric Bayesian Cox model with graph-structured selection priors for sparse identification of multi-omics features by making use of a biologically meaningful graph via a Markov random field (MRF) prior to capturing known relationships between multi-omics features. Since the fixed graph in the MRF prior is for the prior probability distribution, it is not a hard constraint to determine variable selection, so the proposed model can verify known information and has the potential to identify new and novel biomarkers for drawing new biological knowledge. Our simulation results show that the proposed Bayesian Cox model with graph-based prior knowledge results in more trustable and stable variable selection and non-inferior survival prediction, compared to methods modeling the covariates independently without any prior knowledge. The results also indicate that the performance of the proposed model is robust to a partially correct graph in the MRF prior, meaning that in a real setting where not all the true network information between covariates is known, the graph can still be useful. The proposed model is applied to the primary invasive breast cancer patients data in The Cancer Genome Atlas project.",Tobias Østmo Hermansen; Manuela Zucknick; Zhi Zhao,,2025-03-17T11:33:21Z,http://arxiv.org/abs/2503.13078v1
2503.14571v5,Efficient Data Selection for Training Genomic Perturbation Models,"Genomic studies, including CRISPR-based Perturb-seq analyses, face a vast hypothesis space, while gene perturbations remain costly and time-consuming. Gene perturbation models based on graph neural networks are trained to predict the outcomes of gene perturbations to facilitate such experiments. Due to the cost of genomic experiments, active learning is often employed to train these models, alternating between wet-lab experiments and model updates. However, the operational constraints of the wet-lab and the iterative nature of active learning significantly increase the total training time. Furthermore, the inherent sensitivity to model initialization can lead to markedly different sets of gene perturbations across runs, which undermines the reproducibility, interpretability, and reusability of the method. To this end, we propose a graph-based data filtering method that, unlike active learning, selects the gene perturbations in one shot and in a model-free manner. The method optimizes a criterion that maximizes the supervision signal from the graph neural network to enhance generalization. The criterion is defined over the input graph and is optimized with submodular maximization. We compare it empirically to active learning, and the results demonstrate that despite yielding months of acceleration, it also improves the stability of the selected perturbation experiments while achieving comparable test error.",George Panagopoulos; Johannes F. Lutzeyer; Sofiane Ennadir; Jun Pang,,2025-03-18T12:52:03Z,http://arxiv.org/abs/2503.14571v5
2504.00306v1,LOCO-EPI: Leave-one-chromosome-out (LOCO) as a benchmarking paradigm for   deep learning based prediction of enhancer-promoter interactions,"In mammalian and vertebrate genomes, the promoter regions of the gene and their distal enhancers may be located millions of base-pairs from each other, while a promoter may not interact with the closest enhancer. Since base-pair proximity is not a good indicator of these interactions, there is considerable work toward developing methods for predicting Enhancer-Promoter Interactions (EPI). Several machine learning methods have reported increasingly higher accuracies for predicting EPI. Typically, these approaches randomly split the dataset of Enhancer-Promoter (EP) pairs into training and testing subsets followed by model training. However, the aforementioned random splitting causes information leakage by assigning EP pairs from the same genomic region to both testing and training sets, leading to performance overestimation. In this paper we propose to use a more thorough training and testing paradigm i.e., Leave-one-chromosome-out (LOCO) cross-validation for EPI-prediction. We demonstrate that a deep learning algorithm, which gives higher accuracies when trained and tested on random-splitting setting, drops drastically in performance under LOCO setting, confirming overestimation of performance. We further propose a novel hybrid deep neural network for EPI-prediction that fuses k-mer features of the nucleotide sequence. We show that the hybrid architecture performs significantly better in the LOCO setting, demonstrating it can learn more generalizable aspects of EP interactions. With this paper we are also releasing the LOCO splitting-based EPI dataset. Research data is available in this public repository: https://github.com/malikmtahir/EPI",Muhammad Tahir; Shehroz S. Khan; James Davie; Soichiro Yamanaka; Ahmed Ashraf,,2025-04-01T00:20:15Z,http://arxiv.org/abs/2504.00306v1
2504.03876v1,Multiscale Modeling Primer: Focus on Chromatin and Epigenetics,"Essential life processes take place across multiple space and time scales in living organisms but understanding their mechanistic interactions remains an ongoing challenge. Advanced multiscale modeling techniques are providing new opportunities and insights into these complex processes. In cells, meters of chromatin are folded into a nucleus with a diameter on the order of microns. The three-dimensional chromatin structure coupled with biochemical processes that turn genes on or off, specify a given cell type through a complicated set of interactions collectively referred to as epigenetics. Important epigenetic processes include the differential accessibility of genomic loci to transcription factors and chemical modifications to DNA and DNA-binding molecules such as histones. The dynamics of these epigenetic processes span timescales from milliseconds to years. How do chemical modifications consisting of a handful of atoms cooperate to modulate genome folding at the scale of the nucleus and impact organism outcomes? In this review, we highlight the inherently multiscale nature of chromatin organization, with a focus on computational modeling to bridge the gaps in our understanding of biochemical processes across scales. We review relevant chromatin biology, including major types of epigenetic modifications as well as the higher order chromatin structures to present a multiscale view of chromatin. We also review relevant computational methods to simulate chromatin structure, function, and dynamics, as well as experimental techniques that inform and validate said models. Finally, we argue that multiscale modeling provides a path forward towards understanding emergent behavior in this inherently multiscale system.",Achal Mahajan; Erik J. Navarro; William Poole; Carlos F Lopez,,2025-04-04T19:01:17Z,http://arxiv.org/abs/2504.03876v1
2504.05790v1,ViralQC: A Tool for Assessing Completeness and Contamination of   Predicted Viral Contigs,"Motivation: Viruses represent the most abundant biological entities on the planet and play vital roles in diverse ecosystems. Cataloging viruses across various environments is essential for understanding their properties and functions. Metagenomic sequencing has emerged as the most comprehensive method for virus discovery, enabling the sequencing of all genetic materials, including viruses, from host or environmental samples. However, distinguishing viral sequences from the vast background of cellular organism-derived reads in metagenomic data remains a significant challenge. While several learning-based tools, such as VirSorter2 and geNomad, have shown promise in identifying viral contigs, they often experience varying degrees of false positive rates due to noise in sequencing and assembly, shared genes between viruses and their hosts, and the formation of proviruses within host genomes. This highlights the urgent need for an accurate and efficient method to evaluate the quality of viral contigs. Results: To address these challenges, we introduce ViralQC, a tool designed to assess the quality of reported viral contigs or bins. ViralQC identifies contamination regions within putative viral sequences using foundation models trained on viral and cellular genomes and estimates viral completeness through protein organization alignment. We evaluate ViralQC on multiple datasets and compare its performance against CheckV, the state-of-the-art in virus quality assessment. Notably, ViralQC correctly identifies 38% more contamination than CheckV, while maintaining a median absolute error of only 3%. In addition, ViralQC delivers more accurate results for medium- to high-quality (>50% completeness) contigs, demonstrating its superior performance in completeness estimation.",Cheng Peng; Jiayu Shang; Jiaojiao Guan; Yanni Sun,,2025-04-08T08:14:44Z,http://arxiv.org/abs/2504.05790v1
2504.06316v4,DeepGDel: Deep Learning-based Gene Deletion Prediction Framework for   Growth-Coupled Production in Genome-Scale Metabolic Models,"In genome-scale constraint-based metabolic models, gene deletion strategies are crucial for achieving growth-coupled production, where cell growth and target metabolite production are simultaneously achieved. While computational methods for calculating gene deletions have been widely explored and contribute to developing gene deletion strategy databases, current approaches are limited in leveraging new data-driven paradigms, such as machine learning, for more efficient strain design. Therefore, it is necessary to propose a fundamental framework for this objective. In this study, we first formulate the problem of gene deletion strategy prediction and then propose a framework for predicting gene deletion strategies for growth-coupled production in genome-scale metabolic models. The proposed framework leverages deep learning algorithms to learn and integrate sequential gene and metabolite data representation, enabling the automatic gene deletion strategy prediction. Computational experiment results demonstrate the feasibility of the proposed framework, showing substantial improvements over baseline methods. Specifically, the proposed framework achieves a 14.69%, 22.52%, and 13.03% increase in overall accuracy across three metabolic models of different scales under study, while maintaining balanced precision and recall in predicting gene deletion statuses. The source code and examples for the framework are publicly available at https://github.com/MetNetComp/DeepGDel.",Ziwei Yang; Takeyuki Tamura,,2025-04-08T08:07:59Z,http://arxiv.org/abs/2504.06316v4
2505.02206v1,DNAZEN: Enhanced Gene Sequence Representations via Mixed Granularities   of Coding Units,"Genome modeling conventionally treats gene sequence as a language, reflecting its structured motifs and long-range dependencies analogous to linguistic units and organization principles such as words and syntax. Recent studies utilize advanced neural networks, ranging from convolutional and recurrent models to Transformer-based models, to capture contextual information of gene sequence, with the primary goal of obtaining effective gene sequence representations and thus enhance the models' understanding of various running gene samples. However, these approaches often directly apply language modeling techniques to gene sequences and do not fully consider the intrinsic information organization in them, where they do not consider how units at different granularities contribute to representation. In this paper, we propose DNAZEN, an enhanced genomic representation framework designed to learn from various granularities in gene sequences, including small polymers and G-grams that are combinations of several contiguous polymers. Specifically, we extract the G-grams from large-scale genomic corpora through an unsupervised approach to construct the G-gram vocabulary, which is used to provide G-grams in the learning process of DNA sequences through dynamically matching from running gene samples. A Transformer-based G-gram encoder is also proposed and the matched G-grams are fed into it to compute their representations and integrated into the encoder for basic unit (E4BU), which is responsible for encoding small units and maintaining the learning and inference process. To further enhance the learning process, we propose whole G-gram masking to train DNAZEN, where the model largely favors the selection of each entire G-gram to mask rather than an ordinary masking mechanism performed on basic units. Experiments on benchmark datasets demonstrate the effectiveness of DNAZEN on various downstream tasks.",Lei Mao; Yuanhe Tian; Yan Song,,2025-05-04T18:02:28Z,http://arxiv.org/abs/2505.02206v1
2505.23579v1,BioReason: Incentivizing Multimodal Biological Reasoning within a   DNA-LLM Model,"Unlocking deep, interpretable biological reasoning from complex genomic data is a major AI challenge hindering scientific discovery. Current DNA foundation models, despite strong sequence representation, struggle with multi-step reasoning and lack inherent transparent, biologically intuitive explanations. We introduce BioReason, a pioneering architecture that, for the first time, deeply integrates a DNA foundation model with a Large Language Model (LLM). This novel connection enables the LLM to directly process and reason with genomic information as a fundamental input, fostering a new form of multimodal biological understanding. BioReason's sophisticated multi-step reasoning is developed through supervised fine-tuning and targeted reinforcement learning, guiding the system to generate logical, biologically coherent deductions. On biological reasoning benchmarks including KEGG-based disease pathway prediction - where accuracy improves from 88% to 97% - and variant effect prediction, BioReason demonstrates an average 15% performance gain over strong single-modality baselines. BioReason reasons over unseen biological entities and articulates decision-making through interpretable, step-by-step biological traces, offering a transformative approach for AI in biology that enables deeper mechanistic insights and accelerates testable hypothesis generation from genomic data. Data, code, and checkpoints are publicly available at https://github.com/bowang-lab/BioReason",Adibvafa Fallahpour; Andrew Magnuson; Purav Gupta; Shihao Ma; Jack Naimer; Arnav Shah; Haonan Duan; Omar Ibrahim; Hani Goodarzi; Chris J. Maddison; Bo Wang,,2025-05-29T15:49:27Z,http://arxiv.org/abs/2505.23579v1
2505.23839v1,GeneBreaker: Jailbreak Attacks against DNA Language Models with   Pathogenicity Guidance,"DNA, encoding genetic instructions for almost all living organisms, fuels groundbreaking advances in genomics and synthetic biology. Recently, DNA Foundation Models have achieved success in designing synthetic functional DNA sequences, even whole genomes, but their susceptibility to jailbreaking remains underexplored, leading to potential concern of generating harmful sequences such as pathogens or toxin-producing genes. In this paper, we introduce GeneBreaker, the first framework to systematically evaluate jailbreak vulnerabilities of DNA foundation models. GeneBreaker employs (1) an LLM agent with customized bioinformatic tools to design high-homology, non-pathogenic jailbreaking prompts, (2) beam search guided by PathoLM and log-probability heuristics to steer generation toward pathogen-like sequences, and (3) a BLAST-based evaluation pipeline against a curated Human Pathogen Database (JailbreakDNABench) to detect successful jailbreaks. Evaluated on our JailbreakDNABench, GeneBreaker successfully jailbreaks the latest Evo series models across 6 viral categories consistently (up to 60\% Attack Success Rate for Evo2-40B). Further case studies on SARS-CoV-2 spike protein and HIV-1 envelope protein demonstrate the sequence and structural fidelity of jailbreak output, while evolutionary modeling of SARS-CoV-2 underscores biosecurity risks. Our findings also reveal that scaling DNA foundation models amplifies dual-use risks, motivating enhanced safety alignment and tracing mechanisms. Our code is at https://github.com/zaixizhang/GeneBreaker.",Zaixi Zhang; Zhenghong Zhou; Ruofan Jin; Le Cong; Mengdi Wang,,2025-05-28T13:58:32Z,http://arxiv.org/abs/2505.23839v1
2506.19681v1,Genome-Anchored Foundation Model Embeddings Improve Molecular Prediction   from Histology Images,"Precision oncology requires accurate molecular insights, yet obtaining these directly from genomics is costly and time-consuming for broad clinical use. Predicting complex molecular features and patient prognosis directly from routine whole-slide images (WSI) remains a major challenge for current deep learning methods. Here we introduce PathLUPI, which uses transcriptomic privileged information during training to extract genome-anchored histological embeddings, enabling effective molecular prediction using only WSIs at inference. Through extensive evaluation across 49 molecular oncology tasks using 11,257 cases among 20 cohorts, PathLUPI demonstrated superior performance compared to conventional methods trained solely on WSIs. Crucially, it achieves AUC $\geq$ 0.80 in 14 of the biomarker prediction and molecular subtyping tasks and C-index $\geq$ 0.70 in survival cohorts of 5 major cancer types. Moreover, PathLUPI embeddings reveal distinct cellular morphological signatures associated with specific genotypes and related biological pathways within WSIs. By effectively encoding molecular context to refine WSI representations, PathLUPI overcomes a key limitation of existing models and offers a novel strategy to bridge molecular insights with routine pathology workflows for wider clinical application.",Cheng Jin; Fengtao Zhou; Yunfang Yu; Jiabo Ma; Yihui Wang; Yingxue Xu; Huajun Zhou; Hao Jiang; Luyang Luo; Luhui Mao; Zifan He; Xiuming Zhang; Jing Zhang; Ronald Chan; Herui Yao; Hao Chen,,2025-06-24T14:48:12Z,http://arxiv.org/abs/2506.19681v1
2506.23428v1,Multiple Hypothesis Testing in Genomics,"This analysis report presents an in-depth exploration of multiple hypothesis testing in the context of Genomics RNA-seq differential expression (DE) analysis, with a primary focus on techniques designed to control the false discovery rate (FDR). While RNA-seq has become a cornerstone in transcriptomic research, accurately detecting expression changes remains challenging due to the high-dimensional nature of the data. This report delves into the Benjamini-Hochberg (BH) procedure, Benjamini-Yekutieli (BY) approach, and Storey's method, emphasizing their importance in addressing multiple testing issues and improving the reliability of results in large-scale genomic studies. We provide an overview of how these methods can be applied to control FDR while maintaining statistical power, and demonstrate their effectiveness through simulated data analysis.   The discussion highlights the significance of using adaptive methods like Storey's q-value, particularly in high-dimensional datasets where traditional approaches may struggle. Results are presented through typical plots (e.g., Volcano, MA, PCA) and confusion matrices to visualize the impact of these techniques on gene discovery. The limitations section also touches on confounding factors like gene correlations and batch effects, which are often encountered in real-world data.   Ultimately, the analysis achieves a robust framework for handling multiple hypothesis comparisons, offering insights into how these methods can be used to interpret complex gene expression data while minimizing errors. The report encourages further validation and exploration of these techniques in future research.",Shyam Gupta,,2025-06-29T23:30:09Z,http://arxiv.org/abs/2506.23428v1
2507.02221v2,GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic   Data Commons,"The Genomic Data Commons (GDC) provides access to high quality, harmonized cancer genomics data through a unified curation and analysis platform centered around patient cohorts. While GDC users can interactively create complex cohorts through the graphical Cohort Builder, users (especially new ones) may struggle to find specific cohort descriptors across hundreds of possible fields and properties. However, users may be better able to describe their desired cohort in free-text natural language. We introduce GDC Cohort Copilot, an open-source copilot tool for curating cohorts from the GDC. GDC Cohort Copilot automatically generates the GDC cohort filter corresponding to a user-input natural language description of their desired cohort, before exporting the cohort back to the GDC for further analysis. An interactive user interface allows users to further refine the generated cohort. We develop and evaluate multiple large language models (LLMs) for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC Cohort LLM achieves better results than GPT-4o prompting in generating GDC cohorts. We implement and share GDC Cohort Copilot as a containerized Gradio app on HuggingFace Spaces, available at https://huggingface.co/spaces/uc-ctds/GDC-Cohort-Copilot. GDC Cohort LLM weights are available at https://huggingface.co/uc-ctds. All source code is available at https://github.com/uc-cdis/gdc-cohort-copilot.",Steven Song; Anirudh Subramanyam; Zhenyu Zhang; Aarti Venkat; Robert L. Grossman,,2025-07-03T00:55:58Z,http://arxiv.org/abs/2507.02221v2
2507.07839v1,MeD-3D: A Multimodal Deep Learning Framework for Precise Recurrence   Prediction in Clear Cell Renal Cell Carcinoma (ccRCC),"Accurate prediction of recurrence in clear cell renal cell carcinoma (ccRCC) remains a major clinical challenge due to the disease complex molecular, pathological, and clinical heterogeneity. Traditional prognostic models, which rely on single data modalities such as radiology, histopathology, or genomics, often fail to capture the full spectrum of disease complexity, resulting in suboptimal predictive accuracy. This study aims to overcome these limitations by proposing a deep learning (DL) framework that integrates multimodal data, including CT, MRI, histopathology whole slide images (WSI), clinical data, and genomic profiles, to improve the prediction of ccRCC recurrence and enhance clinical decision-making. The proposed framework utilizes a comprehensive dataset curated from multiple publicly available sources, including TCGA, TCIA, and CPTAC. To process the diverse modalities, domain-specific models are employed: CLAM, a ResNet50-based model, is used for histopathology WSIs, while MeD-3D, a pre-trained 3D-ResNet18 model, processes CT and MRI images. For structured clinical and genomic data, a multi-layer perceptron (MLP) is used. These models are designed to extract deep feature embeddings from each modality, which are then fused through an early and late integration architecture. This fusion strategy enables the model to combine complementary information from multiple sources. Additionally, the framework is designed to handle incomplete data, a common challenge in clinical settings, by enabling inference even when certain modalities are missing.",Hasaan Maqsood; Saif Ur Rehman Khan,,2025-07-10T15:11:09Z,http://arxiv.org/abs/2507.07839v1
2508.20275v1,A Systematic Review on the Generative AI Applications in Human Medical   Genomics,"Although traditional statistical techniques and machine learning methods have contributed significantly to genetics and, in particular, inherited disease diagnosis, they often struggle with complex, high-dimensional data, a challenge now addressed by state-of-the-art deep learning models. Large language models (LLMs), based on transformer architectures, have excelled in tasks requiring contextual comprehension of unstructured medical data. This systematic review examines the role of LLMs in the genetic research and diagnostics of both rare and common diseases. Automated keyword-based search in PubMed, bioRxiv, medRxiv, and arXiv was conducted, targeting studies on LLM applications in diagnostics and education within genetics and removing irrelevant or outdated models. A total of 172 studies were analyzed, highlighting applications in genomic variant identification, annotation, and interpretation, as well as medical imaging advancements through vision transformers. Key findings indicate that while transformer-based models significantly advance disease and risk stratification, variant interpretation, medical imaging analysis, and report generation, major challenges persist in integrating multimodal data (genomic sequences, imaging, and clinical records) into unified and clinically robust pipelines, facing limitations in generalizability and practical implementation in clinical settings. This review provides a comprehensive classification and assessment of the current capabilities and limitations of LLMs in transforming hereditary disease diagnostics and supporting genetic education, serving as a guide to navigate this rapidly evolving field.",Anton Changalidis; Yury Barbitoff; Yulia Nasykhova; Andrey Glotov,,2025-08-27T21:17:12Z,http://arxiv.org/abs/2508.20275v1
2509.03551v1,"Predicting Antimicrobial Resistance (AMR) in Campylobacter, a Foodborne   Pathogen, and Cost Burden Analysis Using Machine Learning","Antimicrobial resistance (AMR) poses a significant public health and economic challenge, increasing treatment costs and reducing antibiotic effectiveness. This study employs machine learning to analyze genomic and epidemiological data from the public databases for molecular typing and microbial genome diversity (PubMLST), incorporating data from UK government-supported AMR surveillance by the Food Standards Agency and Food Standards Scotland. We identify AMR patterns in Campylobacter jejuni and Campylobacter coli isolates collected in the UK from 2001 to 2017. The research integrates whole-genome sequencing (WGS) data, epidemiological metadata, and economic projections to identify key resistance determinants and forecast future resistance trends and healthcare costs. We investigate gyrA mutations for fluoroquinolone resistance and the tet(O) gene for tetracycline resistance, training a Random Forest model validated with bootstrap resampling (1,000 samples, 95% confidence intervals), achieving 74% accuracy in predicting AMR phenotypes. Time-series forecasting models (SARIMA, SIR, and Prophet) predict a rise in campylobacteriosis cases, potentially exceeding 130 cases per 100,000 people by 2050, with an economic burden projected to surpass 1.9 billion GBP annually if left unchecked. An enhanced Random Forest system, analyzing 6,683 isolates, refines predictions by incorporating temporal patterns, uncertainty estimation, and resistance trend modeling, indicating sustained high beta-lactam resistance, increasing fluoroquinolone resistance, and fluctuating tetracycline resistance.",Shubham Mishra; The Anh Han; Bruno Silvester Lopes; Shatha Ghareeb; Zia Ush Shamszaman,,2025-09-03T00:56:12Z,http://arxiv.org/abs/2509.03551v1
2503.21837v1,Impact of Oxygen on DNA Damage Distribution in 3D Genome and Its   Correlation to Oxygen Enhancement Ratio under High LET Irradiation,"The variation of the oxygen enhancement ratio (OER) across different values of Linear Energy Transfer (LET) currently lacks a comprehensive mechanistic interpretation and a mechanistic model. Our earlier research revealed a significant correlation between the distribution of double-strand breaks (DSBs) within the 3D genome and radiation-induced cell death, which offers valuable insights into the oxygen effect. In this study, we formulate a model where the reaction of oxygen is represented as the probability of inducing DNA strand breaks. Then it is integrated into a track-structure Monte Carlo simulation to investigate the impact of oxygen on the spatial distribution of DSBs within the 3D genome. Results show that the incidence ratios of clustered DSBs in a single topologically associating domain (TAD) (case 2) and DSBs in frequently-interacting TADs (case 3) under aerobic and hypoxic conditions closely align with the trend of the OER of cell survival across various LET values. By utilizing the parameters derived from our previous study, we calculate the OER values related to cell survival. Our OER curves exhibit good correspondence with experimental data. This study provides a potentially mechanistic explanation for the changes in OER across different LET levels. High-LET irradiation leads to dense ionization events, resulting in an overabundance of lesions that readily induce case 2 and case 3. The probabilities of cell death associated with case 2 and case 3 are substantially higher than other damage patterns. This may contribute to the main mechanism governing the variation of OER for high LET. Our study further underscores the importance of the DSB distribution within the 3D genome in the context of radiation-induced cell death. This study also provides valuable reference points for establishing a mechanistic model of OER.",Ankang Hu; Wanyi Zhou; Xiyu Luo; Rui Qiu; Junli Li,,2025-03-27T05:46:06Z,http://arxiv.org/abs/2503.21837v1
2507.09967v1,SimOmics: A Simulation Toolkit for Multivariate and Multi-Omics Data,"SimOmics is an R package designed to generate realistic, multivariate, and multi-omics synthetic datasets. It is intended for use in benchmarking, method development, and reproducibility in bioinformatics, particularly in the context of omics integration tasks such as those encountered in transcriptomics, proteomics, and metabolomics. SimOmics supports latent factor simulation, sparsity structures, block-wise covariance modeling, and biologically inspired noise models and feature dimensions.",Kaitao Lai,,2025-07-14T06:33:05Z,http://arxiv.org/abs/2507.09967v1
2508.02914v1,2-Categorical Foundations for Multiparameter Persistence,"This paper introduces a novel approach to multi-parameter persistence using 2-categorical structures. We develop a framework that captures hierarchical interactions between filter parameters, overcoming fundamental limitations of traditional persistence modules. Our 2-categorical model yields new invariants that effectively characterize multidimensional topological features while maintaining computational tractability. We prove stability theorems for these invariants and demonstrate their effectiveness through applications in genomics and complex network analysis.",Mauricio Angel,,2025-08-04T21:37:55Z,http://arxiv.org/abs/2508.02914v1
2501.02836v1,Sulfobetaine-Phosphonate Block Copolymer Coated Iron Oxide Nanoparticles   for Genomic Locus Targeting and Magnetic Micromanipulation in the Nucleus of   Living Cells,"Exerting forces on biomolecules inside living cells would allow us to probe their dynamic interactions in their native environment. Magnetic iron oxide nanoparticles represent a unique tool capable of pulling on biomolecules with the application of an external magnetic field gradient; however, their use has been restricted to biomolecules accessible from the extracellular medium. Targeting intracellular biomolecules represents an additional challenge due to potential nonspecific interactions with cytoplasmic or nuclear components. We present the synthesis of sulfobetaine-phosphonate block copolymer ligands, which provide magnetic nanoparticles which are stealthy and targetable in living cells. We demonstrate for the first time their efficient targeting in the nucleus and their use for magnetic micromanipulation of a specific genomic locus in living cells. We believe that these stable and furtive magnetic nanoprobes represent a promising tool to manipulate specific biomolecules in living cells and probe the mechanical properties of living matter at the molecular scale.",Fanny Delille; Elie Balloul; Bassam Hajj; Mohamed Hanafi; Colin Morand; Xiang Zhen Xu; Simon Dumas; Antoine Coulon; Nicolas Lequeux; Thomas Pons,LPEM; PCC; PCC; SIMM; PCC; LPEM; IPGG; PCC; LPEM; LPEM,2025-01-06T08:30:17Z,http://arxiv.org/abs/2501.02836v1
2501.06314v1,BioAgents: Democratizing Bioinformatics Analysis with Multi-Agent   Systems,"Creating end-to-end bioinformatics workflows requires diverse domain expertise, which poses challenges for both junior and senior researchers as it demands a deep understanding of both genomics concepts and computational techniques. While large language models (LLMs) provide some assistance, they often fall short in providing the nuanced guidance needed to execute complex bioinformatics tasks, and require expensive computing resources to achieve high performance. We thus propose a multi-agent system built on small language models, fine-tuned on bioinformatics data, and enhanced with retrieval augmented generation (RAG). Our system, BioAgents, enables local operation and personalization using proprietary data. We observe performance comparable to human experts on conceptual genomics tasks, and suggest next steps to enhance code generation capabilities.",Nikita Mehandru; Amanda K. Hall; Olesya Melnichenko; Yulia Dubinina; Daniel Tsirulnikov; David Bamman; Ahmed Alaa; Scott Saponas; Venkat S. Malladi,,2025-01-10T19:30:59Z,http://arxiv.org/abs/2501.06314v1
2501.08205v1,Modeling Feature Maps for Quantum Machine Learning,"Quantum Machine Learning (QML) offers significant potential for complex tasks like genome sequence classification, but quantum noise on Noisy Intermediate-Scale Quantum (NISQ) devices poses practical challenges. This study systematically evaluates how various quantum noise models including dephasing, amplitude damping, depolarizing, thermal noise, bit-flip, and phase-flip affect key QML algorithms (QSVC, Peg-QSVC, QNN, VQC) and feature mapping techniques (ZFeatureMap, ZZFeatureMap, and PauliFeatureMap). Results indicate that QSVC is notably robust under noise, whereas Peg-QSVC and QNN are more sensitive, particularly to depolarizing and amplitude-damping noise. The PauliFeatureMap is especially vulnerable, highlighting difficulties in maintaining accurate classification under noisy conditions. These findings underscore the critical importance of feature map selection and noise mitigation strategies in optimizing QML for genomic classification, with promising implications for personalized medicine.",Navneet Singh; Shiva Raj Pokhrel,,2025-01-14T15:45:27Z,http://arxiv.org/abs/2501.08205v1
2501.10004v3,Static Three-Dimensional Structures Determine Fast Dynamics Between   Distal Loci Pairs in Interphase Chromosomes,"Live-cell imaging experiments have shown that the distal dynamics between enhancers and promoters are unexpectedly rapid and incompatible with standard polymer models. The discordance between the compact static chromatin organization and dynamics is a conundrum that violates the expected structure-function relationship. We developed a theory to predict chromatin dynamics by accurately determining three-dimensional (3D) structures from static Hi-C contact maps or fixed-cell imaging data. Using the calculated 3D coordinates, the theory accurately forecasts experimentally observed two-point chromatin dynamics. It predicts rapid enhancer-promoter interactions and uncovers a scaling relationship between two-point relaxation time and genomic separation, closely matching recent measurements. The theory predicts that cohesin depletion accelerates single-locus diffusion while significantly slowing relaxation dynamics within topologically associating domains (TADs). Our results demonstrate that chromatin dynamics can be reliably inferred from static structural data, reinforcing the notion that 3D chromatin structure governs dynamic behavior. This general framework offers powerful tools for exploring chromatin dynamics across diverse biological contexts.",Guang Shi; Sucheol Shin; D. Thirumalai,,2025-01-17T07:39:26Z,http://arxiv.org/abs/2501.10004v3
2501.17998v1,MirLibSpark: A Scalable NGS Plant MicroRNA Prediction Pipeline for   Multi-Library Functional Annotation,"The emergence of the Next Generation Sequencing increases drastically the volume of transcriptomic data. Although many standalone algorithms and workflows for novel microRNA (miRNA) prediction have been proposed, few are designed for processing large volume of sequence data from large genomes, and even fewer further annotate functional miRNAs by analyzing multiple libraries. We propose an improved pipeline for a high volume data facility by implementing mirLibSpark based on the Apache Spark framework. This pipeline is the fastest actual method, and provides an accuracy improvement compared to the standard. In this paper, we deliver the first distributed functional miRNA predictor as a standalone and fully automated package. It is an efficient and accurate miRNA predictor with functional insight. Furthermore, it compiles with the gold-standard requirement on plant miRNA predictions.",Chao-Jung Wu; Amine M. Remita; Abdoulaye Baniré Diallo,,2025-01-29T21:11:30Z,http://arxiv.org/abs/2501.17998v1
2502.01577v1,plmmr: an R package to fit penalized linear mixed models for genome-wide   association data with complex correlation structure,"Correlation among the observations in high-dimensional regression modeling can be a major source of confounding. We present a new open-source package, plmmr, to implement penalized linear mixed models in R. This R package estimates correlation among observations in high-dimensional data and uses those estimates to improve prediction with the best linear unbiased predictor. The package uses memory-mapping so that genome-scale data can be analyzed on ordinary machines even if the size of data exceeds RAM. We present here the methods, workflow, and file-backing approach upon which plmmr is built, and we demonstrate its computational capabilities with two examples from real GWAS data.",Tabitha K. Peter; Anna C. Reisetter; Yujing Lu; Oscar A. Rysavy; Patrick J. Breheny,,2025-02-03T18:00:26Z,http://arxiv.org/abs/2502.01577v1
2502.06531v1,A Grain Boundary Embrittlement Genome for Substitutional Cubic Alloys,"Grain boundary chemistry plays a critical role for the properties of metals and alloys, yet there is a lack of consistent datasets for alloy design and development. With the advent of artificial intelligence and machine learning in materials science, open materials models and datasets can be used to overcome such challenges. Here, we use a universal interatomic potential to compute a grain boundary segregation and embrittlement genome for the {\Sigma}5[001](210) grain boundary for FCC and BCC binary alloys. The grain boundary database calculated here serves as a design tool for the embrittlement of high-angle grain boundaries for alloys across 15 base metals system of Ag, Al, Au, Cr, Cu, Fe (both BCC and FCC), Mo, Nb, Ni, Pd, Pt, Rh, Ta, V and W with 75 solute elements for each.",Nutth Tuchinda; Gregory B. Olson; Christopher A. Schuh,,2025-02-10T14:54:54Z,http://arxiv.org/abs/2502.06531v1
2502.08014v2,The Augmented Potential Method: Multiscale Modeling Toward a Spectral   Defect Genome,"The modeling of solute chemistry at low-symmetry defects in materials is historically challenging, due to the computation cost required to evaluate thermodynamic properties from first principles. Here, we offer a hybrid multiscale approach called the augmented potential method that connects the chemical flexibility and near-quantum accuracy of a universal machine learning potential at the site of the defect, with the computational speed of a long-range classical potential implemented away from the defect site in a buffer zone. The method allows us to rapidly compute distributions of grain boundary segregation energy for 1,050 binary alloy pairs (including Ag, Al, Au, Cr, Cu, Fe, Mo, Nb, Ni, Pd, Pt, Ta and V, W solvent), creating a database for polycrystalline grain boundary segregation. This database is ~5x larger than previously published spectral compilations, and yet has improved accuracy. The approach can also address problems far beyond the reach of any other method, such as handling bcc Fe-based alloys, or the complex solute-solute interactions in random polycrystals. The approach thus paves a pathway toward a complete defect genome in crystalline materials.",Nutth Tuchinda; Changle Li; Christopher A. Schuh,,2025-02-11T23:26:46Z,http://arxiv.org/abs/2502.08014v2
2502.14547v1,From Mutation to Degradation: Predicting Nonsense-Mediated Decay with   NMDEP,"Nonsense-mediated mRNA decay (NMD) is a critical post-transcriptional surveillance mechanism that degrades transcripts with premature termination codons, safeguarding transcriptome integrity and shaping disease phenotypes. However, accurately predicting NMD efficiency remains challenging, as existing models often rely on simplistic rule-based heuristics or limited feature sets, constraining their accuracy and generalizability. Using paired DNA and RNA data from The Cancer Genome Atlas, we benchmark embedding-only models and demonstrate that they underperform compared to a simple rule-based approach. To address this, we develop NMDEP (NMD Efficiency Predictor), an integrative framework that combines optimized rule-based methods, sequence embeddings, and curated biological features, achieving state-of-the-art predictive performance. Through explainable AI, we identify key NMD determinants, reaffirming established factors such as variant position while uncovering novel contributors like ribosome loading. Applied to over 2.9 million simulated stop-gain variants, NMDEP facilitates large-scale mRNA degradation assessments, advancing variant interpretation and disease research.",Ali Saadat; Jacques Fellay,,2025-02-20T13:25:08Z,http://arxiv.org/abs/2502.14547v1
2502.18405v1,Enhancing DNA Foundation Models to Address Masking Inefficiencies,"Masked language modelling (MLM) as a pretraining objective has been widely adopted in genomic sequence modelling. While pretrained models can successfully serve as encoders for various downstream tasks, the distribution shift between pretraining and inference detrimentally impacts performance, as the pretraining task is to map [MASK] tokens to predictions, yet the [MASK] is absent during downstream applications. This means the encoder does not prioritize its encodings of non-[MASK] tokens, and expends parameters and compute on work only relevant to the MLM task, despite this being irrelevant at deployment time. In this work, we propose a modified encoder-decoder architecture based on the masked autoencoder framework, designed to address this inefficiency within a BERT-based transformer. We empirically show that the resulting mismatch is particularly detrimental in genomic pipelines where models are often used for feature extraction without fine-tuning. We evaluate our approach on the BIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achieve substantial performance gains in both closed-world and open-world classification tasks when compared against causal models and bidirectional architectures pretrained with MLM tasks.",Monireh Safari; Pablo Millan Arias; Scott C. Lowe; Lila Kari; Angel X. Chang; Graham W. Taylor,,2025-02-25T17:56:25Z,http://arxiv.org/abs/2502.18405v1
2503.12377v1,GCBLANE: A graph-enhanced convolutional BiLSTM attention network for   improved transcription factor binding site prediction,"Identifying transcription factor binding sites (TFBS) is crucial for understanding gene regulation, as these sites enable transcription factors (TFs) to bind to DNA and modulate gene expression. Despite advances in high-throughput sequencing, accurately identifying TFBS remains challenging due to the vast genomic data and complex binding patterns. GCBLANE, a graph-enhanced convolutional bidirectional Long Short-Term Memory (LSTM) attention network, is introduced to address this issue. It integrates convolutional, multi-head attention, and recurrent layers with a graph neural network to detect key features for TFBS prediction. On 690 ENCODE ChIP-Seq datasets, GCBLANE achieved an average AUC of 0.943, and on 165 ENCODE datasets, it reached an AUC of 0.9495, outperforming advanced models that utilize multimodal approaches, including DNA shape information. This result underscores GCBLANE's effectiveness compared to other methods. By combining graph-based learning with sequence analysis, GCBLANE significantly advances TFBS prediction.",Jonas Chris Ferrao; Dickson Dias; Sweta Morajkar; Manisha Gokuldas Fal Dessai,,2025-03-16T06:52:03Z,http://arxiv.org/abs/2503.12377v1
2503.23494v2,DNA and Human Language: Epigenetic Memory and Redundancy in Linear   Sequence,"DNA is often described as the 'language of life', but whether it possesses formal linguistic properties remains unresolved. Here, we present the first empirical evidence that DNA sequences exhibit core linguistic features, specifically, functional and information redundancy, through comprehensive analysis of genomic and epigenetic datasets. By mapping DNA sequences into a linguistic feature space, we demonstrate that fixed-length (41 bp) DNA segments encode information analogously to human language, with redundancy contributing to signal stability in aqueous intracellular environments. Moreover, we provide the first evidence of one-dimensional epigenetic memory, showing that linear DNA sequences can maintain epigenetic marks such as 6mA methylation, contrasting with models focusing on epigenetic memory transmission via 3D chromatin organization[1]. Our tailored linguistic mapping strategy also addresses persistent challenges in genomic data processing, significantly improving data cleaning and feature extraction. Together, these findings establish a conceptual paradigm that bridges molecular information encoding and linguistic theory, laying the foundation for next-generation large language models (LLMs) specifically tailored to DNA, marking a shift at the interface of molecular biology, information theory, and artificial intelligence (AI).",Li Yang; Dongbo Wang,,2025-03-30T16:03:02Z,http://arxiv.org/abs/2503.23494v2
2504.00764v1,A DNA-Centric Mechanism for Protein Targeting in 6mA Methylation,"How DNA-binding proteins locate specific genomic targets remains a central challenge in molecular biology. Traditional protein-centric approaches, which rely on wet-lab experiments and visualization techniques, often lack genome-wide resolution and fail to capture physiological dynamics in living cells. Here, we introduce a DNA-centric strategy that leverages in vivo N6-methyladenine (6mA) data to decode the logic of protein-DNA recognition. By integrating linguistically inspired modeling with machine learning, we reveal two distinct search modes: a protein-driven diffusion mechanism and a DNA sequence-driven mechanism, wherein specific motifs function as protein traps. We further reconstruct high-resolution interaction landscapes at the level of individual sequences and trace the evolutionary trajectories of recognition motifs across species. This framework addresses fundamental limitations of protein-centered approaches and positions DNA itself as an intrinsic reporter of protein-binding behavior.",Li Yang; Dongbo Wang,,2025-03-31T00:41:55Z,http://arxiv.org/abs/2504.00764v1
2504.02081v1,Addressing missing context in regulatory variation across primate   evolution,"In primates, loci associated with adaptive trait variation often fall in non-coding regions. Understanding the mechanisms linking these regulatory variants to fitness-relevant phenotypes remains challenging, but can be addressed using functional genomic data. However, such data are rarely generated at scale in non-human primates. When they are, only select tissues, cell types, developmental stages, and cellular environments are typically considered, despite appreciation that adaptive variants often exhibit context-dependent effects. In this review, we 1) discuss why context-dependent regulatory loci might be especially evolutionarily relevant in primates, 2) explore challenges and emerging solutions for mapping such context-dependent variation, and 3) discuss the scientific questions these data could address. We argue that filling this gap will provide critical insights into evolutionary processes, human disease, and regulatory adaptation.",Genevieve Housman; Audrey Arner; Amy Longtin; Christian Gagnon; Arun Durvasula; Amanda Lea,,2025-04-02T19:40:10Z,http://arxiv.org/abs/2504.02081v1
2504.03550v1,Dimensionality reduction for k-means clustering of large-scale influenza   mutation datasets,"Viral mutations pose significant threats to public health by increasing infectivity, strengthening vaccine resistance, and altering disease severity. To track these evolving patterns, agencies like the CDC annually evaluate thousands of virus strains, underscoring the urgent need to understand viral mutagenesis and evolution in depth. In this study, we integrate genomic analysis, clustering, and three leading dimensionality reduction approaches, namely, principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE), and uniform manifold approximation and projection (UMAP)-to investigate the effects of COVID-19 on influenza virus propagation. By applying these methods to extensive pre- and post-pandemic influenza datasets, we reveal how selective pressures during the pandemic have influenced the diversity of influenza genetics. Our findings indicate that combining robust dimension reduction with clustering yields critical insights into the complex dynamics of viral mutation, informing both future research directions and strategies for public health intervention.",Emilee Walden; Jiahui Chen; Guo-Wei Wei,,2025-04-04T15:57:48Z,http://arxiv.org/abs/2504.03550v1
2504.06306v1,Predicting Survivability of Cancer Patients with Metastatic Patterns   Using Explainable AI,"Cancer remains a leading global health challenge and a major cause of mortality. This study leverages machine learning (ML) to predict the survivability of cancer patients with metastatic patterns using the comprehensive MSK-MET dataset, which includes genomic and clinical data from 25,775 patients across 27 cancer types. We evaluated five ML models-XGBoost, Na\""ive Bayes, Decision Tree, Logistic Regression, and Random Fores using hyperparameter tuning and grid search. XGBoost emerged as the best performer with an area under the curve (AUC) of 0.82. To enhance model interpretability, SHapley Additive exPlanations (SHAP) were applied, revealing key predictors such as metastatic site count, tumor mutation burden, fraction of genome altered, and organ-specific metastases. Further survival analysis using Kaplan-Meier curves, Cox Proportional Hazards models, and XGBoost Survival Analysis identified significant predictors of patient outcomes, offering actionable insights for clinicians. These findings could aid in personalized prognosis and treatment planning, ultimately improving patient care.",Polycarp Nalela; Deepthi Rao; Praveen Rao,,2025-04-07T20:48:15Z,http://arxiv.org/abs/2504.06306v1
2504.13049v1,Multi-modal single-cell foundation models via dynamic token adaptation,"Recent advances in applying deep learning in genomics include DNA-language and single-cell foundation models. However, these models take only one data type as input. We introduce dynamic token adaptation and demonstrate how it combines these models to predict gene regulation at the single-cell level in different genetic contexts. Although the method is generalisable, we focus on an illustrative example by training an adapter from DNA-sequence embeddings to a single-cell foundation model's token embedding space. As a qualitative evaluation, we assess the impact of DNA sequence changes on the model's learned gene regulatory networks by mutating the transcriptional start site of the transcription factor GATA4 in silico, observing predicted expression changes in its target genes in fetal cardiomyocytes.",Wenmin Zhao; Ana Solaguren-Beascoa; Grant Neilson; Louwai Muhammed; Liisi Laaniste; Sera Aylin Cakiroglu,,2025-04-17T16:05:59Z,http://arxiv.org/abs/2504.13049v1
2504.19710v2,PhyloProfile v2: Scalable Exploration of Multilayered Phylogenetic   Profiles via Dimensionality Reduction,"Phylogenetic profiles - presence-absence patterns of genes across taxa - are rich information sources for inferring the evolutionary history of genes and gene families. When aggregated across many genes, these profiles can reveal coevolutionary patterns, supporting the prediction of gene functions and interactions. With rapidly growing numbers of sequenced genomes, phylogenetic profiles now routinely encompass thousands of genes and taxa. Existing software fall short in enabling interactive visualization, exploration, and analysis of such large datasets. We present PhyloProfile v2, a comprehensive overhaul of the original PhyloProfile software. This new version introduces major performance improvements along with novel features designed for more efficient data exploration. Notably, PhyloProfile v2 integrates dimensionality reduction techniques to visualize phylogenetic profiles in interactive 2D or 3D space, offering an intuitive overview even for massive datasets. Furthermore, the platform enables seamless transitions from large-scale analyses - spanning millions of orthology relationships - to detailed comparisons of protein feature architectures between specific orthologs. PhyloProfile v2 thus provides a versatile and scalable solution for evolutionary and functional genomics research. PhyloProfile v2 is available as an R package at Bioconductor https://doi.org/doi:10.18129/B9.bioc.PhyloProfile. The open-source code and documentation are provided under MIT license at https://github.com/BIONF/PhyloProfile",Vinh Tran; Ingo Ebersberger,,2025-04-28T12:04:33Z,http://arxiv.org/abs/2504.19710v2
2505.04431v1,An Asynchronous Distributed-Memory Parallel Algorithm for k-mer Counting,"This paper describes a new asynchronous algorithm and implementation for the problem of k-mer counting (KC), which concerns quantifying the frequency of length k substrings in a DNA sequence. This operation is common to many computational biology workloads and can take up to 77% of the total runtime of de novo genome assembly. The performance and scalability of the current state-of-the-art distributed-memory KC algorithm are hampered by multiple rounds of Many-To-Many collectives. Therefore, we develop an asynchronous algorithm (DAKC) that uses fine-grained, asynchronous messages to obviate most of this global communication while utilizing network bandwidth efficiently via custom message aggregation protocols. DAKC can perform strong scaling up to 256 nodes (512 sockets / 6K cores) and can count k-mers up to 9x faster than the state-of-the-art distributed-memory algorithm, and up to 100x faster than the shared-memory alternative. We also provide an analytical model to understand the hardware resource utilization of our asynchronous KC algorithm and provide insights on the performance.",Souvadra Hati; Akihiro Hayashi; Richard Vuduc,,2025-05-07T14:00:03Z,http://arxiv.org/abs/2505.04431v1
2505.07227v1,CVTree for 16S rRNA: Constructing Taxonomy-Compatible All-Species Living   Tree Effectively and Efficiently,"The Composition Vector Tree (CVTree) method, developed under the leadership of Professor Hao Bailin, is an alignment-free algorithm for constructing phylogenetic trees. Although initially designed for studying prokaryotic evolution based on whole-genome, it has demonstrated broad applicability across diverse biological systems and gene sequences. In this study, we employed two methods, InterList and Hao, of CVTree to investigate the phylogeny and taxonomy of prokaryote based on the 16S rRNA sequences from All-Species Living Tree Project. We have established a comprehensive phylogenetic tree that incorporates the majority of species documented in human scientific knowledge and compared it with the taxonomy of prokaryotes. And the performance of CVTree were also compared with multiple sequence alignment-based approaches. Our results revealed that CVTree methods achieve computational speeds 1-3 orders of magnitude faster than conventional alignment methods while maintaining high consistency with established taxonomic relationships, even outperforming some multiple sequence alignment methods. These findings confirm CVTree's effectiveness and efficiency not only for whole-genome evolutionary studies but also for phylogenetic and taxonomic investigations based on genes.",Yi-Fei Lu; Xiao-Yang Zhi; Guang-Hong Zuo,,2025-05-12T04:55:17Z,http://arxiv.org/abs/2505.07227v1
2505.07338v1,A Systems Biology view of Breast cancer via Fractal Geometry and   Fractional Calculus,"Breast cancer (BC) is the most widespread cancer globally, yet current diagnostic and prognostic methods inadequately capture its biological complexity, despite the benefits of early detection. Cancer systems biology (SB) has advanced over the past two decades as a multiscale approach, but often neglects morphological complexity observable at cellular, tissue, and tumor levels. Structural organization significantly influences system behavior, necessitating the identification of scales where morphological features emerge. Fractal geometry (FG) provides quantitative tools to assess the irregular, scale-invariant structures typical of cancer. Alongside FG, fractional calculus (FC) offers a framework to model memory-dependent and non-local dynamics. However, existing research remains constrained to mono-fractal analyses or limited imaging scopes, lacking integration into broader SB models. This review aims to bridge that gap by presenting core FG concepts, both mono- and multi-fractal measures, and their application to BC across histological, cytological, and radiological data. It further explores how FC enhances dynamic morphological modeling. Finally, it considers how number-theoretic tools like p-adic analysis may represent hierarchical genomic and morphological patterns, unifying insights from tumor morphology to genome organization.",Abhijeet Das; Ramray Bhat; Mohit Kumar Jolly,,2025-05-12T08:24:03Z,http://arxiv.org/abs/2505.07338v1
2505.07683v2,Multimodal Survival Modeling in the Age of Foundation Models,"The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference through its harmonized genomics, clinical, and image data. Prior studies have trained bespoke cancer survival prediction models from unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive meaningful feature embeddings, agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the feasibility of training classical, multimodal survival models over zero-shot embeddings extracted by FMs. We show the ease and additive effect of multimodal fusion, outperforming unimodal models. We demonstrate the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we modernize survival modeling by leveraging FMs and information extraction from pathology reports.",Steven Song; Morgan Borjigin-Wang; Irene Madejski; Robert L. Grossman,,2025-05-12T15:47:21Z,http://arxiv.org/abs/2505.07683v2
2505.12711v2,Any-to-Any Learning in Computational Pathology via Triplet Multimodal   Pretraining,"Recent advances in computational pathology and artificial intelligence have significantly enhanced the utilization of gigapixel whole-slide images and and additional modalities (e.g., genomics) for pathological diagnosis. Although deep learning has demonstrated strong potential in pathology, several key challenges persist: (1) fusing heterogeneous data types requires sophisticated strategies beyond simple concatenation due to high computational costs; (2) common scenarios of missing modalities necessitate flexible strategies that allow the model to learn robustly in the absence of certain modalities; (3) the downstream tasks in CPath are diverse, ranging from unimodal to multimodal, cnecessitating a unified model capable of handling all modalities. To address these challenges, we propose ALTER, an any-to-any tri-modal pretraining framework that integrates WSIs, genomics, and pathology reports. The term ""any"" emphasizes ALTER's modality-adaptive design, enabling flexible pretraining with any subset of modalities, and its capacity to learn robust, cross-modal representations beyond WSI-centric approaches. We evaluate ALTER across extensive clinical tasks including survival prediction, cancer subtyping, gene mutation prediction, and report generation, achieving superior or comparable performance to state-of-the-art baselines.",Qichen Sun; Zhengrui Guo; Rui Peng; Hao Chen; Jinzhuo Wang,,2025-05-19T05:07:34Z,http://arxiv.org/abs/2505.12711v2
2505.20344v1,Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK   Biobank using Structural MRI,"Brain aging trajectories differ between males and females, yet the genetic factors underlying these differences remain underexplored. Using structural MRI and genotyping data from 40,940 UK Biobank participants (aged 45-83), we computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and ventricular volumes. We conducted sex-stratified genome-wide association studies (GWAS) and Post-GWAS analyses to identify genetic variants associated with accelerated brain aging. Distinct gene sets emerged by sex: in females, neurotransmitter transport and mitochondrial stress response genes were implicated; in males, immune and inflammation-related genes dominated. Shared genes, including GMNC and OSTN, were consistently linked to brain volumes across sexes, suggesting core roles in neurostructural maintenance. Tissue expression analyses revealed sex-specific enrichment in pathways tied to neurodegeneration. These findings highlight the importance of sex-stratified approaches in aging research and suggest genetic targets for personalized interventions against age-related cognitive decline.",Karen Ardila; Aashka Mohite; Abdoljalil Addeh; Amanda V. Tyndall; Cindy K. Barha; Quan Long; M. Ethan MacDonald,,2025-05-25T03:59:00Z,http://arxiv.org/abs/2505.20344v1
2505.20578v1,Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via   Constrained RL,"Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences.",Xingyu Chen; Shihao Ma; Runsheng Lin; Jiecong Lin; Bo Wang,,2025-05-26T23:27:50Z,http://arxiv.org/abs/2505.20578v1
2506.00692v1,Not Just $N_e$ $N_e$-more: New Applications for SMC from Ecology to   Phylogenies,"Genomes contain the mutational footprint of an organism's evolutionary history, shaped by diverse forces including ecological factors, selective pressures, and life history traits. The sequentially Markovian coalescent (SMC) is a versatile and tractable model for the genetic genealogy of a sample of genomes, which captures this shared history. Methods that utilize the SMC, such as PSMC and MSMC, have been widely used in evolution and ecology to infer demographic histories. However, these methods ignore common biological features, such as gene flow events and structural variation. Recently, there have been several advancements that widen the applicability of SMC-based methods: inclusion of an isolation with migration model, integration with the multi-species coalescent, incorporation of ecological variables (such as selfing and dormancy), inference of dispersal rates, and many computational advances in applying these models to data. We give an overview of the SMC model and its various recent extensions, discuss examples of biological discoveries through SMC-based inference, and comment on the assumptions, benefits and drawbacks of various methods.",David Peede; Trevor Cousins; Arun Durvasula; Anastasia Ignatieva; Toby G. L. Kovacs; Alba Nieto; Emily E. Puckett; Elizabeth T. Chevy,,2025-05-31T19:53:54Z,http://arxiv.org/abs/2506.00692v1
2506.03279v1,Genotype networks drive oscillating endemicity and epidemic trajectories   in viral evolution,"Rapidly evolving viruses use antigenic drift as a key mechanism to evade host immunity and persist in real populations. While traditional models of antigenic drift and epidemic spread rely on low-dimensional antigenic spaces, genomic surveillance data reveal that viral evolution produces complex antigenic genotype networks with hierarchical modular structures. In this study, we present an eco-evolutionary framework in which viral evolution and population immunity dynamics are shaped by the structure of antigenic genotype networks. Using synthetic networks, we demonstrate that network topology alone can drive transitions between stable endemic states and recurrent seasonal epidemics. Furthermore, our results show how the integration of the genotype network of the H3N2 influenza in our model allows for estimating the emergence times of various haplotypes resulting from its evolution. Our findings underscore the critical role of the topology of genotype networks in shaping epidemic behavior and, besides, provide a robust framework for integrating real-world genomic data into predictive epidemic models.",Santiago Lamata-Otín; Octavian C. Rotita-Ion; Alex Arenas; David Soriano-Paños; Jesús Gómez-Gardeñes,,2025-06-03T18:12:09Z,http://arxiv.org/abs/2506.03279v1
2506.04303v1,Knowledge-guided Contextual Gene Set Analysis Using Large Language   Models,"Gene set analysis (GSA) is a foundational approach for interpreting genomic data of diseases by linking genes to biological processes. However, conventional GSA methods overlook clinical context of the analyses, often generating long lists of enriched pathways with redundant, nonspecific, or irrelevant results. Interpreting these requires extensive, ad-hoc manual effort, reducing both reliability and reproducibility. To address this limitation, we introduce cGSA, a novel AI-driven framework that enhances GSA by incorporating context-aware pathway prioritization. cGSA integrates gene cluster detection, enrichment analysis, and large language models to identify pathways that are not only statistically significant but also biologically meaningful. Benchmarking on 102 manually curated gene sets across 19 diseases and ten disease-related biological mechanisms shows that cGSA outperforms baseline methods by over 30%, with expert validation confirming its increased precision and interpretability. Two independent case studies in melanoma and breast cancer further demonstrate its potential to uncover context-specific insights and support targeted hypothesis generation.",Zhizheng Wang; Chi-Ping Day; Chih-Hsuan Wei; Qiao Jin; Robert Leaman; Yifan Yang; Shubo Tian; Aodong Qiu; Yin Fang; Qingqing Zhu; Xinghua Lu; Zhiyong Lu,,2025-06-04T15:56:57Z,http://arxiv.org/abs/2506.04303v1
2506.04511v1,Viral Hitchhikers and Macroevolution: A Novel Hypothesis on Explosive   Speciation,"Mobile genetic elements (e.g., endogenous viruses, LINEs, SINEs) can transfer between genomes, even between species, triggering dramatic genetic changes. Endogenous viral elements (EVEs) arise when infectious viruses integrate into the host germline. EVEs integrate at specific sites; their genes or regulatory regions can be exapted and could induce chromosomal rearrangement. We propose that EVEs participate in adaptive radiations and that their parent viruses, through interspecific transfer, could initiate new species formation. By synchronously affecting multiple individuals, viral outbreaks generate shared genomic changes that both facilitate reproductive isolation and provide the simultaneous modifications needed for groups to emerge as founders of new species. We suggest horizontal viral transfer during the K-Pg accelerated mammalian radiation linking viral epidemics to macroevolutionary diversification. This theoretical work proposes endogenous viruses as catalysts for explosive speciation.",Mario E. Lozano; Marcela G. Pilloff,,2025-06-04T23:23:19Z,http://arxiv.org/abs/2506.04511v1
2506.11182v1,Multimodal Modeling of CRISPR-Cas12 Activity Using Foundation Models and   Chromatin Accessibility Data,"Predicting guide RNA (gRNA) activity is critical for effective CRISPR-Cas12 genome editing but remains challenging due to limited data, variation across protospacer adjacent motifs (PAMs-short sequence requirements for Cas binding), and reliance on large-scale training. We investigate whether pre-trained biological foundation model originally trained on transcriptomic data can improve gRNA activity estimation even without domain-specific pre-training. Using embeddings from existing RNA foundation model as input to lightweight regressor, we show substantial gains over traditional baselines. We also integrate chromatin accessibility data to capture regulatory context, improving performance further. Our results highlight the effectiveness of pre-trained foundation models and chromatin accessibility data for gRNA activity prediction.",Azim Dehghani Amirabad; Yanfei Zhang; Artem Moskalev; Sowmya Rajesh; Tommaso Mansi; Shuwei Li; Mangal Prakash; Rui Liao,,2025-06-12T16:15:14Z,http://arxiv.org/abs/2506.11182v1
2506.13119v1,PhenoKG: Knowledge Graph-Driven Gene Discovery and Patient Insights from   Phenotypes Alone,"Identifying causative genes from patient phenotypes remains a significant challenge in precision medicine, with important implications for the diagnosis and treatment of genetic disorders. We propose a novel graph-based approach for predicting causative genes from patient phenotypes, with or without an available list of candidate genes, by integrating a rare disease knowledge graph (KG). Our model, combining graph neural networks and transformers, achieves substantial improvements over the current state-of-the-art. On the real-world MyGene2 dataset, it attains a mean reciprocal rank (MRR) of 24.64\% and nDCG@100 of 33.64\%, surpassing the best baseline (SHEPHERD) at 19.02\% MRR and 30.54\% nDCG@100. We perform extensive ablation studies to validate the contribution of each model component. Notably, the approach generalizes to cases where only phenotypic data are available, addressing key challenges in clinical decision support when genomic information is incomplete.",Kamilia Zaripova; Ege Özsoy; Nassir Navab; Azade Farshad,,2025-06-16T05:54:12Z,http://arxiv.org/abs/2506.13119v1
2506.21641v1,Quantum Variational Transformer Model for Enhanced Cancer Classification,"Accurate prediction of cancer type and primary tumor site is critical for effective diagnosis, personalized treatment, and improved outcomes. Traditional models struggle with the complexity of genomic and clinical data, but quantum computing offers enhanced computational capabilities. This study develops a hybrid quantum-classical transformer model, incorporating quantum attention mechanisms via variational quantum circuits (VQCs) to improve prediction accuracy. Using 30,000 anonymized cancer samples from the Genome Warehouse (GWH), data preprocessing included cleaning, encoding, and feature selection. Classical self-attention modules were replaced with quantum attention layers, with classical data encoded into quantum states via amplitude encoding. The model, trained using hybrid backpropagation and quantum gradient calculations, outperformed the classical transformer model, achieving 92.8% accuracy and an AUC of 0.96 compared to 87.5% accuracy and an AUC of 0.89. It also demonstrated 35% faster training and 25% fewer parameters, highlighting computational efficiency. These findings showcase the potential of quantum-enhanced transformers to advance biomedical data analysis, enabling more accurate diagnostics and personalized medicine.",Don Roosan; Rubayat Khan; Md Rahatul Ashakin; Tiffany Khou; Saif Nirzhor; Mohammad Rifat Haider,,2025-06-25T20:53:40Z,http://arxiv.org/abs/2506.21641v1
2506.22172v2,Bridging Chaos Game Representations and $k$-mer Frequencies of DNA   Sequences,"This paper establishes formal mathematical foundations linking Chaos Game Representations (CGR) of DNA sequences to their underlying $k$-mer frequencies. We prove that the Frequency CGR (FCGR) of order $k$ is mathematically equivalent to a discretization of CGR at resolution $2^k \times 2^k$, and its vectorization corresponds to the $k$-mer frequencies of the sequence. Additionally, we characterize how symmetry transformations of CGR images correspond to specific nucleotide permutations in the originating sequences. Leveraging these insights, we introduce an algorithm that generates synthetic DNA sequences from prescribed $k$-mer distributions by constructing Eulerian paths on De Bruijn multigraphs. This enables reconstruction of sequences matching target $k$-mer profiles with arbitrarily high precision, facilitating the creation of synthetic CGR images for applications such as data augmentation for machine learning-based taxonomic classification of DNA sequences. Numerical experiments validate the effectiveness of our method across both real genomic data and artificially sampled distributions. To our knowledge, this is the first comprehensive framework that unifies CGR geometry, $k$-mer statistics, and sequence reconstruction, offering new tools for genomic analysis and visualization.",Haoze He; Lila Kari; Pablo Millan Arias,,2025-06-27T12:35:57Z,http://arxiv.org/abs/2506.22172v2
2507.02150v1,Modelling transcriptional silencing and its coupling to 3D genome   organisation,"Timely up- or down-regulation of gene expression is crucial for cellular differentiation and function. While gene upregulation via transcriptional activators has been extensively investigated, gene silencing remains understudied, especially by modelling. This study employs 3D simulations to study the biophysics of a chromatin fibre where active transcription factors compete with repressors for binding to transcription units along the fibre, and investigates how different silencing mechanisms affect 3D chromatin structure and transcription. We examine three gene silencing feedback mechanisms: positive, negative, and neutral. These mechanisms capture different silencing pathways observed or proposed in biological systems. Our findings reveal that, whilst all mechanisms lead to a silencing transition, the signatures of this transition depend on the choice of the feedback. The latter controls the morphologies of the emergent 3D transcription factor clusters, the average gene expression and its variability, or gene noise, and the network of ensuing correlations between activities of neighbouring transcription units. These results provide insights into the biophysics of gene silencing, as well as into the interplay between transcriptional regulation and 3D genome organisation.",Massimiliano Semeraro; Giuseppe Negro; Davide Marenduzzo; Giada Forte,,2025-07-02T21:13:05Z,http://arxiv.org/abs/2507.02150v1
2507.02818v1,Genetic Features for Drug Responses in Cancer -- Investigating an   Ensemble-Feature-Selection Approach,"Predicting drug responses using genetic and transcriptomic features is crucial for enhancing personalized medicine. In this study, we implemented an ensemble of machine learning algorithms to analyze the correlation between genetic and transcriptomic features of cancer cell lines and IC50 values, a reliable metric for drug efficacy. Our analysis involved a reduction of the feature set from an original pool of 38,977 features, demonstrating a strong linear relationship between genetic features and drug responses across various algorithms, including SVR, Linear Regression, and Ridge Regression. Notably, copy number variations (CNVs) emerged as more predictive than mutations, suggesting a significant reevaluation of biomarkers for drug response prediction. Through rigorous statistical methods, we identified a highly reduced set of 421 critical features. This set offers a novel perspective that contrasts with traditional cancer driver genes, underscoring the potential for these biomarkers in designing targeted therapies. Furthermore, our findings advocate for IC50 values as a predictable measurement of drug responses and underscore the need for more data that can represent the dimensionality of genomic data in drug response prediction. Future work will aim to expand the dataset and refine feature selection to enhance the generalizability of the predictive model in clinical settings.",Johannes Schlüter; Alexander Schönhuth,,2025-07-03T17:33:12Z,http://arxiv.org/abs/2507.02818v1
2507.03718v2,Finding easy regions for short-read variant calling from pangenome data,"Background: While benchmarks on short-read variant calling suggest low error rate below 0.5%, they are only applicable to predefined confident regions. For a human sample without such regions, the error rate could be 10 times higher. Although multiple sets of easy regions have been identified to alleviate the issue, they fail to consider non-reference samples or are biased towards existing short-read data or aligners.   Results: Here, using hundreds of high-quality human assemblies, we derived a set of sample-agnostic easy regions where short-read variant calling reaches high accuracy. These regions cover 88.2% of GRCh38, 92.2% of coding regions and 96.3% of ClinVar pathogenic variants. They achieve a good balance between coverage and easiness and can be generated for other human assemblies or species with multiple well assembled genomes.   Conclusion: This resource provides a convient and powerful way to filter spurious variant calls for clinical or research human samples.",Heng Li,,2025-07-04T17:11:15Z,http://arxiv.org/abs/2507.03718v2
2507.04891v1,MurreNet: Modeling Holistic Multimodal Interactions Between   Histopathology and Genomic Profiles for Survival Prediction,"Cancer survival prediction requires integrating pathological Whole Slide Images (WSIs) and genomic profiles, a challenging task due to the inherent heterogeneity and the complexity of modeling both inter- and intra-modality interactions. Current methods often employ straightforward fusion strategies for multimodal feature integration, failing to comprehensively capture modality-specific and modality-common interactions, resulting in a limited understanding of multimodal correlations and suboptimal predictive performance. To mitigate these limitations, this paper presents a Multimodal Representation Decoupling Network (MurreNet) to advance cancer survival analysis. Specifically, we first propose a Multimodal Representation Decomposition (MRD) module to explicitly decompose paired input data into modality-specific and modality-shared representations, thereby reducing redundancy between modalities. Furthermore, the disentangled representations are further refined then updated through a novel training regularization strategy that imposes constraints on distributional similarity, difference, and representativeness of modality features. Finally, the augmented multimodal features are integrated into a joint representation via proposed Deep Holistic Orthogonal Fusion (DHOF) strategy. Extensive experiments conducted on six TCGA cancer cohorts demonstrate that our MurreNet achieves state-of-the-art (SOTA) performance in survival prediction.",Mingxin Liu; Chengfei Cai; Jun Li; Pengbo Xu; Jinze Li; Jiquan Ma; Jun Xu,,2025-07-07T11:26:29Z,http://arxiv.org/abs/2507.04891v1
2507.08062v1,AMRScan: A hybrid R and Nextflow toolkit for rapid antimicrobial   resistance gene detection from sequencing data,"AMRScan is a hybrid bioinformatics toolkit implemented in both R and [Nextflow](https://www.nextflow.io/) for the rapid and reproducible detection of antimicrobial resistance (AMR) genes from next-generation sequencing (NGS) data. The toolkit enables users to identify AMR gene hits in sequencing reads by aligning them against reference databases such as CARD using BLAST.   The R implementation provides a concise, script-based approach suitable for single-sample analysis, teaching, and rapid prototyping. In contrast, the Nextflow implementation enables reproducible, scalable workflows for multi-sample batch processing in high-performance computing (HPC) and containerized environments. It leverages modular pipeline design with support for automated database setup, quality control, conversion, BLAST alignment, and results parsing.   AMRScan helps bridge the gap between lightweight exploratory analysis and production-ready surveillance pipelines, making it suitable for both research and public health genomics applications.",Kaitao Lai,,2025-07-10T15:58:26Z,http://arxiv.org/abs/2507.08062v1
2507.21706v1,EnTao-GPM: DNA Foundation Model for Predicting the Germline Pathogenic   Mutations,"Distinguishing pathogenic mutations from benign polymorphisms remains a critical challenge in precision medicine. EnTao-GPM, developed by Fudan University and BioMap, addresses this through three innovations: (1) Cross-species targeted pre-training on disease-relevant mammalian genomes (human, pig, mouse), leveraging evolutionary conservation to enhance interpretation of pathogenic motifs, particularly in non-coding regions; (2) Germline mutation specialization via fine-tuning on ClinVar and HGMD, improving accuracy for both SNVs and non-SNVs; (3) Interpretable clinical framework integrating DNA sequence embeddings with LLM-based statistical explanations to provide actionable insights. Validated against ClinVar, EnTao-GPM demonstrates superior accuracy in mutation classification. It revolutionizes genetic testing by enabling faster, more accurate, and accessible interpretation for clinical diagnostics (e.g., variant assessment, risk identification, personalized treatment) and research, advancing personalized medicine.",Zekai Lin; Haoran Sun; Yucheng Guo; Yujie Yang; Yanwen Wang; Bozhen Hu; Chonghang Ye; Qirong Yang; Fan Zhong; Xiaoming Zhang; Lei Liu,,2025-07-29T11:34:41Z,http://arxiv.org/abs/2507.21706v1
2507.23048v1,CARTEpigenoQC: A Quality Control Toolkit for CAR-T Single-Cell   Epigenomic Data,"CARTEpigenoQC is an R-based toolkit designed to streamline quality control (QC) for single-cell epigenomic datasets involving Chimeric Antigen Receptor (CAR)-engineered T cells. With the growing application of scATAC-seq, scCUT&Tag, and scBS-seq to characterize CAR-T cell states, it has become critical to perform customized QC that not only addresses standard metrics like FRiP (Fraction of Reads in Peaks) and TSS enrichment, but also directly detects signal from CAR vector insertion sites. CARTEpigenoQC supports both 10x Genomics and non-10x data formats and produces HTML and PNG summary outputs suited for exploratory analysis and regulatory-grade preclinical reporting. It is intended to assist researchers, core facilities, and translational immunologists in ensuring the validity of single-cell epigenomic profiling of engineered T cells.",Kaitao Lai,,2025-07-30T19:22:23Z,http://arxiv.org/abs/2507.23048v1
2508.03593v1,On the (In)Significance of Feature Selection in High-Dimensional   Datasets,"Extensive research has been done on feature selection (FS) algorithms for high-dimensional datasets aiming to improve model performance, reduce computational cost and identify features of interest. We test the null hypothesis of using randomly selected features to compare against features selected by FS algorithms to validate the performance of the latter. Our results show that FS on high-dimensional datasets (in particular gene expression) in classification tasks is not useful. We find that (1) models trained on small subsets (0.02%-1% of all features) of randomly selected features almost always perform comparably to those trained on all features, and (2) a ""typical""- sized random subset provides comparable or superior performance to that of top-k features selected in various published studies. Thus, our work challenges many feature selection results on high dimensional datasets, particularly in computational genomics. It raises serious concerns about studies that propose drug design or targeted interventions based on computationally selected genes, without further validation in a wet lab.",Bhavesh Neekhra; Debayan Gupta; Partha Pratim Chakravarti,,2025-08-05T15:58:31Z,http://arxiv.org/abs/2508.03593v1
2508.05278v1,A near-exact linear mixed model for genome-wide association studies,"Linear mixed models (LMM) are widely adopted in genome-wide association studies (GWAS) to account for population stratification and cryptic relatedness. However, the parameter estimation of LMMs imposes substantial computational burdens due to large-scale operations on genetic similarity matrices (GSM). We introduced the near-exact linear mixed model (NExt-LMM), a novel LMM framework that overcomes critical computational bottlenecks in GWAS through the following key innovations. Firstly, we exploit the inherent low-rank structure of the GSM iteratively with the Hierarchical Off-Diagonal Low-Rank (HODLR) format, which is much faster than traditional decomposition methods. Secondly, we leverage the HODLR-approximated GSM to dramatically accelerate the further maximum likelihood estimation with the shared heritability ratios. Moreover, we establish rigorous error bounds for the NExt-LMM estimator, proving that Kullback-Leibler divergence between the approximated and exact estimators can be arbitrarily small. Consequently, our proposed dual approach accelerates inference of LMMs while guaranteeing low approximation errors. We use numerical experiments to demonstrate that the NExt-LMM significantly improves inference efficiency compared to existing methods. We develop a Python package that is available at https://github.com/ZhibinPU/NExt-LMM.",Zhibin Pu; Shufei Ge; Shijia Wang,,2025-08-07T11:23:05Z,http://arxiv.org/abs/2508.05278v1
2508.08200v1,Pangenome-guided sequence assembly via binary optimisation,"De novo genome assembly is challenging in highly repetitive regions; however, reference-guided assemblers often suffer from bias. We propose a framework for pangenome-guided sequence assembly, which can resolve short-read data in complex regions without bias towards a single reference genome. Our method frames assembly as a graph traversal optimisation problem, which can be implemented on quantum computers. The pipeline first annotates pangenome graphs with estimated copy numbers for each node, then finds a path on the graph that best explains those copy numbers. On simulated data, our approach significantly reduces the number of contigs compared to de novo assemblers. While it introduces a small increase in inaccuracies, such as false joins, our optimisation-based methods are competitive with current exhaustive search techniques. They are also designed to scale more efficiently as the problem size grows and will run effectively on future quantum computers.",Josh Cudby; James Bonfield; Chenxi Zhou; Richard Durbin; Sergii Strelchuk,,2025-08-11T17:17:21Z,http://arxiv.org/abs/2508.08200v1
2508.09212v1,Deep Generative Models for Discrete Genotype Simulation,"Deep generative models open new avenues for simulating realistic genomic data while preserving privacy and addressing data accessibility constraints. While previous studies have primarily focused on generating gene expression or haplotype data, this study explores generating genotype data in both unconditioned and phenotype-conditioned settings, which is inherently more challenging due to the discrete nature of genotype data. In this work, we developed and evaluated commonly used generative models, including Variational Autoencoders (VAEs), Diffusion Models, and Generative Adversarial Networks (GANs), and proposed adaptation tailored to discrete genotype data. We conducted extensive experiments on large-scale datasets, including all chromosomes from cow and multiple chromosomes from human. Model performance was assessed using a well-established set of metrics drawn from both deep learning and quantitative genetics literature. Our results show that these models can effectively capture genetic patterns and preserve genotype-phenotype association. Our findings provide a comprehensive comparison of these models and offer practical guidelines for future research in genotype simulation. We have made our code publicly available at https://github.com/SihanXXX/DiscreteGenoGen.",Sihan Xie; Thierry Tribout; Didier Boichard; Blaise Hanczar; Julien Chiquet; Eric Barrey,GABI; GABI; GABI; IBISC; MIA Paris-Saclay; GABI,2025-08-11T11:56:03Z,http://arxiv.org/abs/2508.09212v1
2508.13498v1,Improving the FAIRness and Sustainability of the NHGRI Resources   Ecosystem,"In 2024, NHGRI-funded genomic resource projects completed a Self-Assessment Tool (SAT) and interviews to evaluate their application of FAIR (Findable, Accessible, Interoperable, Reusable) principles and sustainability. Key challenges were identified in metadata tools, data curation, variant identifiers, and data processing. Addressing these needs, we engaged the community through webinars and discussions, leading to a two-day workshop in March 2025. The workshop developed targeted recommendations, including improving transparency, standardizing identifiers, enhancing usability, implementing APIs, leveraging AI/ML for curation, and evaluating impact. These outcomes provide a framework for advancing FAIR practices, fostering collaboration, and strengthening the sustainability of NHGRI resources.",Larry Babb; Carol Bult; Vincent J. Carey; Robert J. Carroll; Benjamin C. Hitz; Chris J. Mungall; Heidi L. Rehm; Michael C. Schatz; Alex Wagner; NHGRI Resource Workshop Community,,2025-08-19T04:17:24Z,http://arxiv.org/abs/2508.13498v1
2508.17213v1,Multi-modal Knowledge Decomposition based Online Distillation for   Biomarker Prediction in Breast Cancer Histopathology,"Immunohistochemical (IHC) biomarker prediction benefits from multi-modal data fusion analysis. However, the simultaneous acquisition of multi-modal data, such as genomic and pathological information, is often challenging due to cost or technical limitations. To address this challenge, we propose an online distillation approach based on Multi-modal Knowledge Decomposition (MKD) to enhance IHC biomarker prediction in haematoxylin and eosin (H\&E) stained histopathology images. This method leverages paired genomic-pathology data during training while enabling inference using either pathology slides alone or both modalities. Two teacher and one student models are developed to extract modality-specific and modality-general features by minimizing the MKD loss. To maintain the internal structural relationships between samples, Similarity-preserving Knowledge Distillation (SKD) is applied. Additionally, Collaborative Learning for Online Distillation (CLOD) facilitates mutual learning between teacher and student models, encouraging diverse and complementary learning dynamics. Experiments on the TCGA-BRCA and in-house QHSU datasets demonstrate that our approach achieves superior performance in IHC biomarker prediction using uni-modal data. Our code is available at https://github.com/qiyuanzz/MICCAI2025_MKD.",Qibin Zhang; Xinyu Hao; Qiao Chen; Rui Xu; Fengyu Cong; Cheng Lu; Hongming Xu,,2025-08-24T04:56:17Z,http://arxiv.org/abs/2508.17213v1
2508.18638v1,Biologically Disentangled Multi-Omic Modeling Reveals Mechanistic   Insights into Pan-Cancer Immunotherapy Resistance,"Immune checkpoint inhibitors (ICIs) have transformed cancer treatment, yet patient responses remain highly variable, and the biological mechanisms underlying resistance are poorly understood. While machine learning models hold promise for predicting responses to ICIs, most existing methods lack interpretability and do not effectively leverage the biological structure inherent to multi-omics data. Here, we introduce the Biologically Disentangled Variational Autoencoder (BDVAE), a deep generative model that integrates transcriptomic and genomic data through modality- and pathway-specific encoders. Unlike existing rigid, pathway-informed models, BDVAE employs a modular encoder architecture combined with variational inference to learn biologically meaningful latent features associated with immune, genomic, and metabolic processes. Applied to a pan-cancer cohort of 366 patients across four cancer types treated with ICIs, BDVAE accurately predicts treatment response (AUC-ROC = 0.94 on unseen test data) and uncovers critical resistance mechanisms, including immune suppression, metabolic shifts, and neuronal signaling. Importantly, BDVAE reveals that resistance spans a continuous biological spectrum rather than strictly binary states, reflecting gradations of tumor dysfunction. Several latent features correlate with survival outcomes and known clinical subtypes, demonstrating BDVAE's capability to generate interpretable, clinically relevant insights. These findings underscore the value of biologically structured machine learning in elucidating complex resistance patterns and guiding precision immunotherapy strategies.",Ifrah Tariq; Ernest Fraenkel,,2025-08-26T03:33:56Z,http://arxiv.org/abs/2508.18638v1
2509.00200v1,Simulation-based inference of yeast centromeres,"The chromatin folding and the spatial arrangement of chromosomes in the cell play a crucial role in DNA replication and genes expression. An improper chromatin folding could lead to malfunctions and, over time, diseases. For eukaryotes, centromeres are essential for proper chromosome segregation and folding. Despite extensive research using de novo sequencing of genomes and annotation analysis, centromere locations in yeasts remain difficult to infer and are still unknown in most species. Recently, genome-wide chromosome conformation capture coupled with next-generation sequencing (Hi-C) has become one of the leading methods to investigate chromosome structures. Some recent studies have used Hi-C data to give a point estimate of each centromere, but those approaches highly rely on a good pre-localization. Here, we present a novel approach that infers in a stochastic manner the locations of all centromeres in budding yeast based on both the experimental Hi-C map and simulated contact maps.",Eloïse Touron; Pedro L. C. Rodrigues; Julyan Arbel; Nelle Varoquaux; Michael Arbel,,2025-08-29T19:23:06Z,http://arxiv.org/abs/2509.00200v1
2509.00897v1,Network Community Detection and Novelty Scoring Reveal Underexplored Hub   Genes in Rheumatoid Arthritis,"Understanding the modular structure and central elements of complex biological networks is critical for uncovering system-level mechanisms in disease. Here, we constructed weighted gene co-expression networks from bulk RNA-seq data of rheumatoid arthritis (RA) synovial tissue, using pairwise correlation and a percolation-guided thresholding strategy. Community detection with Louvain and Leiden algorithms revealed robust modules, and node-strength ranking identified the top 50 hub genes globally and within communities. To assess novelty, we integrated genome-wide association studies (GWAS) with literature-based evidence from PubMed, highlighting five high-centrality genes with little to no prior RA-specific association. Functional enrichment confirmed their roles in immune-related processes, including adaptive immune response and lymphocyte regulation. Notably, these hubs showed strong positive correlations with T- and B-cell markers and negative correlations with NK-cell markers, consistent with RA immunopathology. Overall, our framework demonstrates how correlation-based network construction, modularity-driven clustering, and centrality-guided novelty scoring can jointly reveal informative structure in omics-scale data. This generalizable approach offers a scalable path to gene prioritization in RA and other autoimmune conditions.",Neda Amirirad; Hiroki Sayama,,2025-08-31T15:22:19Z,http://arxiv.org/abs/2509.00897v1
2506.05542v1,Agentomics-ML: Autonomous Machine Learning Experimentation Agent for   Genomic and Transcriptomic Data,"The adoption of machine learning (ML) and deep learning methods has revolutionized molecular medicine by driving breakthroughs in genomics, transcriptomics, drug discovery, and biological systems modeling. The increasing quantity, multimodality, and heterogeneity of biological datasets demand automated methods that can produce generalizable predictive models. Recent developments in large language model-based agents have shown promise for automating end-to-end ML experimentation on structured benchmarks. However, when applied to heterogeneous computational biology datasets, these methods struggle with generalization and success rates. Here, we introduce Agentomics-ML, a fully autonomous agent-based system designed to produce a classification model and the necessary files for reproducible training and inference. Our method follows predefined steps of an ML experimentation process, repeatedly interacting with the file system through Bash to complete individual steps. Once an ML model is produced, training and validation metrics provide scalar feedback to a reflection step to identify issues such as overfitting. This step then creates verbal feedback for future iterations, suggesting adjustments to steps such as data representation, model architecture, and hyperparameter choices. We have evaluated Agentomics-ML on several established genomic and transcriptomic benchmark datasets and show that it outperforms existing state-of-the-art agent-based methods in both generalization and success rates. While state-of-the-art models built by domain experts still lead in absolute performance on the majority of the computational biology datasets used in this work, Agentomics-ML narrows the gap for fully autonomous systems and achieves state-of-the-art performance on one of the used benchmark datasets. The code is available at https://github.com/BioGeMT/Agentomics-ML.",Vlastimil Martinek; Andrea Gariboldi; Dimosthenis Tzimotoudis; Aitor Alberdi Escudero; Edward Blake; David Cechak; Luke Cassar; Alessandro Balestrucci; Panagiotis Alexiou,,2025-06-05T19:44:38Z,http://arxiv.org/abs/2506.05542v1
2506.19490v1,phylo2vec: a library for vector-based phylogenetic tree manipulation,"Phylogenetics is a fundamental component of many analysis frameworks in biology as well as linguistics to study the evolutionary relationships of different entities. Recently, the advent of large-scale genomics and the SARS-CoV-2 pandemic has underscored the necessity for phylogenetic software to handle large datasets of genomes or phylogenetic trees. While significant efforts have focused on scaling optimisation algorithms, visualization, and lineage identification, an emerging body of research has been dedicated to efficient representations of data for genomes and phylogenetic trees such as phylo2vec. Compared to traditional tree representations such as the Newick format, which represents trees using strings of nested parentheses, modern representations of phylogenetic trees utilize integer vectors to define the tree topology traversal. This approach offers several advantages, including easier manipulability, increased memory efficiency, and applicability to downstream tasks such as machine learning. Here, we present the latest release of phylo2vec (or Phylo2Vec), a high-performance software package for encoding, manipulating, and analysing binary phylogenetic trees. At its core, the package is based on the phylo2vec representation of binary trees, which defines a bijection from any tree topology with $n$ leaves into an integer vector of size $n-1$. Compared to the traditional Newick format, phylo2vec is designed to enable fast sampling and comparison of binary trees. This release features a core implementation in Rust, providing significant performance improvements and memory efficiency, while remaining available in Python (superseding the release described in the original paper) and R via dedicated wrappers, making it accessible to a broad audience in the bioinformatics community.",Neil Scheidwasser; Ayush Nag; Matthew J Penn; Anthony MV Jakob; Frederik Mølkjær Andersen; Mark P Khurana; Landung Setiawan; Madeline Gordon; David A Duchêne; Samir Bhatt,,2025-06-24T10:24:27Z,http://arxiv.org/abs/2506.19490v1
2501.02275v1,Genomic and pathological analyses of an asymmetric true hermaphroditism   case in a female labrador retriever,"The two main gonadal development disorders in dogs are true hermaphroditism and XX male syndrome. True hermaphroditism can be divided into two subcategories: XX sex reversal and XY sex reversal. XX Sry-negative sex reversal is more common, and it is characterized by the presence of both ovarian and testicular tissues in an animal. To date, there are 16 cases of true hermaphroditism reported in the literature, 15 of which are XX true hermaphroditism. Hermaphroditism has not been formally documented in labrador retrievers, and no case of asymmetric hermaphroditism has been reported in the literature.",Yihang Zhou,,2025-01-04T12:53:40Z,http://arxiv.org/abs/2501.02275v1
2503.13797v1,Adinkras & Genomics in Sixteen Color Systems (I),"Motivated by the search for embedded on-shell supermultiplets in higher dimensional off-shell theories, we investigate several 16-color supermultiplets and their topology. An Adinkra's topology is known to be equivalent to $(\mathbb{Z}_2)$-quotients of an N-cube. This is revisited with the focus of closing the off-shell problem for the 4D $\mathcal{N} = 4$ Maxwell supermultiplet.","Jeth Arunseangroj; Jude Bedessem; S. James Gates, Jr.; Gabriel Yerger",,2025-03-18T00:59:06Z,http://arxiv.org/abs/2503.13797v1
2503.18472v1,A Graph-based Approach to Variant Extraction,"Accurate variant descriptions are of paramount importance in the field of genetics. The domain is confronted with increasingly complex variants, making it more challenging to generate proper variant descriptions. We present a graph based on all minimal alignments that is a complete representation of a variant and we provide three complementary extraction methods to derive variant descriptions from this graph. Our experiments show that our method in comparison with dbSNP results in identical HGVS descriptions for simple variants and more meaningful descriptions for complex variants.",Mark A. Santcroos; Walter A. Kosters; Mihai Lefter; Jeroen F. J. Laros; Jonathan K. Vis,,2025-03-24T09:20:00Z,http://arxiv.org/abs/2503.18472v1
2505.06127v1,FastDup: a scalable duplicate marking tool using speculation-and-test   mechanism,"Duplicate marking is a critical preprocessing step in gene sequence analysis to flag redundant reads arising from polymerase chain reaction(PCR) amplification and sequencing artifacts. Although Picard MarkDuplicates is widely recognized as the gold-standard tool, its single-threaded implementation and reliance on global sorting result in significant computational and resource overhead, limiting its efficiency on large-scale datasets. Here, we introduce FastDup: a high-performance, scalable solution that follows the speculation-and-test mechanism. FastDup achieves up to 20x throughput speedup and guarantees 100\% identical output compared to Picard MarkDuplicates. FastDup is a C++ program available from GitHub (https://github.com/zzhofict/FastDup.git) under the MIT license.",Zhonghai Zhang; Yewen Li; Ke Meng; Chunming Zhang; Guangming Tan,,2025-05-09T15:33:36Z,http://arxiv.org/abs/2505.06127v1
2505.07413v1,Learning Penalty for Optimal Partitioning via Automatic Feature   Extraction,"Changepoint detection identifies significant shifts in data sequences, making it important in areas like finance, genetics, and healthcare. The Optimal Partitioning algorithms efficiently detect these changes, using a penalty parameter to limit the changepoints number. Determining the appropriate value for this penalty can be challenging. Traditionally, this process involved manually extracting statistical features, such as sequence length or variance to make the prediction. This study proposes a novel approach that uses recurrent neural networks to learn this penalty directly from raw sequences by automatically extracting features. Experiments conducted on 20 benchmark genomic datasets show that this novel method surpasses traditional methods in partitioning accuracy in most cases.",Tung L Nguyen; Toby Hocking,,2025-05-12T10:07:55Z,http://arxiv.org/abs/2505.07413v1
2505.08844v1,CellTypeAgent: Trustworthy cell type annotation with Large Language   Models,"Cell type annotation is a critical yet laborious step in single-cell RNA sequencing analysis. We present a trustworthy large language model (LLM)-agent, CellTypeAgent, which integrates LLMs with verification from relevant databases. CellTypeAgent achieves higher accuracy than existing methods while mitigating hallucinations. We evaluated CellTypeAgent across nine real datasets involving 303 cell types from 36 tissues. This combined approach holds promise for more efficient and reliable cell type annotation.",Jiawen Chen; Jianghao Zhang; Huaxiu Yao; Yun Li,,2025-05-13T14:34:11Z,http://arxiv.org/abs/2505.08844v1
2505.09658v1,Great Short History of Microbiology Development as a Science,"The study of microorganisms, or microbiology, has demonstrated significant development since its inception and is currently a key field of biological sciences that has a huge impact on modern society and scientific research. Over the centuries, this discipline has undergone significant changes, shaping our understanding of infectious diseases and food safety. Starting from the simplest observations of microscopic organisms such as bacteria, viruses, fungi and protozoa, and ending with modern molecular and genomic research methods. This article describes a brief historical path of microbiology development. The heuristic, morphological, physiological, immunological, and molecular genetic stages are the main periods into which the development of this science is traditionally divided, despite the lack of full-fledged and precise boundaries between them.",Daniil S. Gerassimov,,2025-05-14T06:11:55Z,http://arxiv.org/abs/2505.09658v1
2505.11385v1,Compendium Manager: a tool for coordination of workflow management   instances for bulk data processing in Python,"Compendium Manager is a command-line tool written in Python to automate the provisioning, launch, and evaluation of bioinformatics pipelines. Although workflow management tools such as Snakemake and Nextflow enable users to automate the processing of samples within a single sequencing project, integrating many datasets in bulk requires launching and monitoring hundreds or thousands of pipelines. We present the Compendium Manager, a lightweight command-line tool to enable launching and monitoring analysis pipelines at scale. The tool can gauge progress through a list of projects, load results into a shared database, and record detailed processing metrics for later evaluation and reproducibility.",Richard J. Abdill; Ran Blekhman,,2025-05-16T15:49:40Z,http://arxiv.org/abs/2505.11385v1
2507.08058v1,HybridQC: Machine Learning-Augmented Quality Control for Single-Cell   RNA-seq Data,"HybridQC is an R package that streamlines quality control (QC) of single-cell RNA sequencing (scRNA-seq) data by combining traditional threshold-based filtering with machine learning-based outlier detection. It provides an efficient and adaptive framework to identify low-quality cells in noisy or shallow-depth datasets using techniques such as Isolation Forest, while remaining compatible with widely adopted formats such as Seurat objects.   The package is lightweight, easy to install, and suitable for small-to-medium scRNA-seq datasets in research settings. HybridQC is especially useful for projects involving non-model organisms, rare samples, or pilot studies, where automated and flexible QC is critical for reproducibility and downstream analysis.",Kaitao Lai,,2025-07-10T14:48:55Z,http://arxiv.org/abs/2507.08058v1
2501.06844v1,REML Implementations of Kernel-based Multi-environment Genomic   Prediction Models,"High-throughput pheno-, geno-, and envirotyping allows routine characterization of plant varieties and the trials they are evaluated in. These datasets can be integrated into statistical models for genomic prediction in several ways. One approach is to create linear or non-linear kernels which are subsequently used in reproducing kernel hilbert spaces (RKHS) regression. Software packages implementing a Bayesian approach are typically used for these RKHS models. However, they often lack some of the flexibility offered by dedicated linear mixed model software such as ASReml-R. Furthermore, a Bayesian approach is often computationally more demanding than a frequentist model. Here we show how frequentist RKHS models can be implemented in ASReml-R and extend these models to allow for heterogeneous (i.e., trial-specific) genetic variances. We also show how an alternative to the typically Bayesian kernel averaging approach can be implemented by treating the bandwidth associated with the non-linear kernel as a parameter to be estimated using restricted maximum likelihood. We show that these REML implementations with homo- or heterogeneous variances perform similarly or better than the Bayesian models. We also show that the REML implementation comes with a significant increase in computational efficiency, being up to 12 times faster than the Bayesian models while using less memory. Finally, we discuss the significant flexibility provided by this approach and the options regarding further customization of variance models.",Killian A. C. Melsen; Salvador Gezan; Fred van Eeuwijk; Carel F. W. Peeters,,2025-01-12T15:30:11Z,http://arxiv.org/abs/2501.06844v1
2501.15890v3,"Complexity in Complexity: Understanding Visual Complexity Through   Structure, Color, and Surprise","Understanding how humans perceive visual complexity is a key area of study in visual cognition. Previous approaches to modeling visual complexity assessments have often resulted in intricate, difficult-to-interpret algorithms that employ numerous features or sophisticated deep learning architectures. While these complex models achieve high performance on specific datasets, they often sacrifice interpretability, making it challenging to understand the factors driving human perception of complexity. Recently (Shen, et al. 2024) proposed an interpretable segmentation-based model that accurately predicted complexity across various datasets, supporting the idea that complexity can be explained simply. In this work, we investigate the failure of their model to capture structural, color and surprisal contributions to complexity. To this end, we propose Multi-Scale Sobel Gradient (MSG) which measures spatial intensity variations, Multi-Scale Unique Color (MUC) which quantifies colorfulness across multiple scales, and surprise scores generated using a Large Language Model. We test our features on existing benchmarks and a novel dataset (Surprising Visual Genome) containing surprising images from Visual Genome. Our experiments demonstrate that modeling complexity accurately is not as simple as previously thought, requiring additional perceptual and semantic factors to address dataset biases. Our model improves predictive performance while maintaining interpretability, offering deeper insights into how visual complexity is perceived and assessed. Our code, analysis and data are available at https://github.com/Complexity-Project/Complexity-in-Complexity.",Karahan Sarıtaş; Peter Dayan; Tingke Shen; Surabhi S Nath,,2025-01-27T09:32:56Z,http://arxiv.org/abs/2501.15890v3
2501.19030v1,A network-driven framework for enhancing gene-disease association   studies in coronary artery disease,"Over the last decade, genome-wide association studies (GWAS) have successfully identified numerous genetic variants associated with complex diseases. These associations have the potential to reveal the molecular mechanisms underlying complex diseases and lead to the identification of novel drug targets. Despite these advancements, the biological pathways and mechanisms linking genetic variants to complex diseases are still not fully understood. Most trait-associated variants reside in non-coding regions and are presumed to influence phenotypes through regulatory effects on gene expression. Yet, it is often unclear which genes they regulate and in which cell types this regulation occurs. Transcriptome-wide association studies (TWAS) aim to bridge this gap by detecting trait-associated tissue gene expression regulated by GWAS variants. However, traditional TWAS approaches frequently overlook the critical contributions of trans-regulatory effects and fail to integrate comprehensive regulatory networks. Here, we present a novel framework that leverages tissue-specific gene regulatory networks (GRNs) to integrate cis- and trans-genetic regulatory effects into the TWAS framework for complex diseases. We validate our approach using coronary artery disease (CAD), utilizing data from the STARNET project, which provides multi-tissue gene expression and genetic data from around 600 living patients with cardiovascular disease. Preliminary results demonstrate the potential of our GRN-driven framework to uncover more genes and pathways that may underlie CAD. This framework extends traditional TWAS methodologies by utilizing tissue-specific regulatory insights and advancing the understanding of complex disease genetic architecture.",Gutama Ibrahim Mohammad; Johan LM Björkegren; Tom Michoel,,2025-01-31T10:54:39Z,http://arxiv.org/abs/2501.19030v1
2502.00647v1,On an RNA-membrane protogenome,"Selected ribonucleotide sequences bind well to zwitterionic phospholipid bilayer membranes, though randomized RNAs do not. There are no evident repeated sequences in selected membrane binding RNAs. This implies small and varied motifs responsible for membrane affinity. Such subsequences have been partially defined. Bound RNAs require divalents like Mg2+ and/or Ca2+, preferring more ordered phospholipids: gel, ripple or rafted membranes, in that order. RNAs also bind and stabilize bilayers that are bent or sharply deformed. In contrast, RNA binding without divalents extends to negatively charged membranes formed from simpler anionic phospholipids, and to plausibly prebiotic fatty acid bilayers. RNA-membranes also retain RNA function, such as base pairing, passive transport of tryptophan, specific affinity for peptide side chains, like arginine, and catalysis by ribozymic ligase. Multiple membrane-bound RNAs with biochemical functions, linked by specific base-pairing are readily constructed. Given these experimental facts, genetic effects seem plausible. RNA functions often reside in few nucleotides, and are easily joined in a small RNA. Base-paired groups of these can evolve to be purposeful, joining related RNA functions. Such RNA groups permit complex genome functions, but require only replication of short RNAs. RNA-membranes facilitate accurate RNA segregation at cell division, and quickly evolve by appending new base-paired functions. Thus, ancient RNA-membranes could act as a protogenome, supporting orderly encoded RNA expression, inheritance and evolution before DNA and the DNA genome.",Michael Yarus,,2025-01-17T17:35:23Z,http://arxiv.org/abs/2502.00647v1
2502.01311v1,TFBS-Finder: Deep Learning-based Model with DNABERT and Convolutional   Networks to Predict Transcription Factor Binding Sites,"Transcription factors are proteins that regulate the expression of genes by binding to specific genomic regions known as Transcription Factor Binding Sites (TFBSs), typically located in the promoter regions of those genes. Accurate prediction of these binding sites is essential for understanding the complex gene regulatory networks underlying various cellular functions. In this regard, many deep learning models have been developed for such prediction, but there is still scope of improvement. In this work, we have developed a deep learning model which uses pre-trained DNABERT, a Convolutional Neural Network (CNN) module, a Modified Convolutional Block Attention Module (MCBAM), a Multi-Scale Convolutions with Attention (MSCA) module and an output module. The pre-trained DNABERT is used for sequence embedding, thereby capturing the long-term dependencies in the DNA sequences while the CNN, MCBAM and MSCA modules are useful in extracting higher-order local features. TFBS-Finder is trained and tested on 165 ENCODE ChIP-seq datasets. We have also performed ablation studies as well as cross-cell line validations and comparisons with other models. The experimental results show the superiority of the proposed method in predicting TFBSs compared to the existing methodologies. The codes and the relevant datasets are publicly available at https://github.com/NimishaGhosh/TFBS-Finder/.",Nimisha Ghosh; Pratik Dutta; Daniele Santoni,,2025-02-03T12:41:11Z,http://arxiv.org/abs/2502.01311v1
2502.01468v1,Bayesian Probit Multi-Study Non-negative Matrix Factorization for   Mutational Signatures,"Mutational signatures are patterns of somatic mutations in tumor genomes that provide insights into underlying mutagenic processes and cancer origin. Developing reliable methods for their estimation is of growing importance in cancer biology. Somatic mutation data are often collected for different cancer types, highlighting the need for multi-study approaches that enable joint analysis in a principled and integrative manner. Despite significant advancements, statistical models tailored for analyzing the genomes of multiple cancer types remain underexplored. In this work, we introduce a Bayesian Multi-Study Non-negative Matrix Factorization (NMF) approach that uses mixture modeling to incorporate sparsity in the exposure weights of each subject to mutational signatures, allowing for individual tumor profiles to be represented by a subset rather than all signatures, and making this subset depend on covariates. This allows for a) more precise ability to identify meaningful contributions of mutational signatures at the individual level; b) estimation of the prevalence of activity of signatures within a cancer type, defined by the proportion of tumor profiles where a certain signature is present; and c) de-novo identification of interpretable patient subtypes based on the mutational signatures present within their mutational profile. We apply our approach to the mutational profiles of tumors from seven different cancer types, demonstrating its ability to accurately estimate mutational signatures while uncovering both individual and tissue-specific differences. An R package implementing our method is available at https://github.com/blhansen/BAPmultiNMF.",Blake Hansen; Isabella N. Grabski; Giovanni Parmigiani; Roberta De Vito,,2025-02-03T16:00:40Z,http://arxiv.org/abs/2502.01468v1
2502.06107v1,An Evaluation on the Role of Non-Coding RNA in HIV Transcription and   Latency: A Review,"The existence of latent cellular reservoirs is recognized as the major barrier to an HIV cure. Reactivating and eliminating ""shock and kill"" or permanently silencing ""block and lock"" the latent HIV reservoir, as well as gene editing, remain promising approaches, but so far have proven to be only partially successful. Moreover, using latency reversing agents or ""block and lock"" drugs pose additional considerations, including the ability to cause cellular toxicity, a potential lack of specificity for HIV, or low potency when each agent is used alone. RNA molecules, such as microRNAs (miRNAs) and long non-coding RNAs (lncRNAs) are becoming increasingly recognized as important regulators of gene expression. RNA-based approaches for combatting HIV latency represent a promising strategy since both miRNAs and lncRNAs are more cell-type and tissue specific than protein coding genes. Thus, a higher specificity of targeting the latent HIV reservoir with less overall cellular toxicity can likely be achieved. In this review, we summarize current knowledge about HIV gene expression regulation by miRNAs and lncRNAs encoded in the human genome, as well as regulatory molecules encoded in the HIV genome. We discuss both the transcriptional and post-transcriptional regulation of HIV gene expression to align with the current definition of latency, and describe RNA molecules that either promote HIV latency or have anti-latency properties. Finally, we provide perspectives on using each class of RNAs as potential targets for combatting HIV latency, and describe the complexity of the interactions between different RNA molecules, their protein targets, and HIV.",Xiangshuai Liu,,2025-02-10T02:37:01Z,http://arxiv.org/abs/2502.06107v1
2502.07639v1,Response rate estimation in single-stage basket trials: A comparison of   estimators that allow for borrowing across cohorts,"Therapeutic advancements in oncology have shifted towards targeted therapy based on genomic aberrations. This necessitates innovative statistical approaches in clinical trials, particularly in master protocol studies. Basket trials, a type of master protocol, evaluate a single treatment across cohorts sharing a genomic aberration but differing in tumor histology. While offering operational advantages, basket trial analysis presents statistical inference challenges. These trials help determine for which tumor histology the treatment is promising enough to advance to confirmatory evaluation and often use Bayesian designs to support decisions. Beyond decision-making, estimating cohort-specific response rates is crucial for designing subsequent trials. This study compares seven Bayesian estimation methods for basket trials with binary outcomes against the (frequentist) sample proportion estimate through simulations. The goal is to estimate cohort-specific response rates, focusing on bias, mean squared error, and information borrowing. Various scenarios are examined, including homogeneous, heterogeneous, and clustered response rates across cohorts. Results show trade-offs in bias and precision, highlighting the importance of method selection. Berry's method performs best with limited heterogeneity. No clear winner emerges in general cases, with performance affected by shrinkage, bias, and the choice of priors and tuning parameters. Challenges include computational complexity, parameter tuning, and the lack of clear guidance on selection. Researchers should consider these factors when designing and analyzing basket trials.",Antonios Daletzakis; Rutger van den Bor; Vincent van der Noort; Kit CB Roes,,2025-02-11T15:26:43Z,http://arxiv.org/abs/2502.07639v1
2503.04347v1,Large Language Models for Zero-shot Inference of Causal Structures in   Biology,"Genes, proteins and other biological entities influence one another via causal molecular networks. Causal relationships in such networks are mediated by complex and diverse mechanisms, through latent variables, and are often specific to cellular context. It remains challenging to characterise such networks in practice. Here, we present a novel framework to evaluate large language models (LLMs) for zero-shot inference of causal relationships in biology. In particular, we systematically evaluate causal claims obtained from an LLM using real-world interventional data. This is done over one hundred variables and thousands of causal hypotheses. Furthermore, we consider several prompting and retrieval-augmentation strategies, including large, and potentially conflicting, collections of scientific articles. Our results show that with tailored augmentation and prompting, even relatively small LLMs can capture meaningful aspects of causal structure in biological systems. This supports the notion that LLMs could act as orchestration tools in biological discovery, by helping to distil current knowledge in ways amenable to downstream analysis. Our approach to assessing LLMs with respect to experimental data is relevant for a broad range of problems at the intersection of causal learning, LLMs and scientific discovery.",Izzy Newsham; Luka Kovačević; Richard Moulange; Nan Rosemary Ke; Sach Mukherjee,,2025-03-06T11:43:30Z,http://arxiv.org/abs/2503.04347v1
2503.13862v1,HySurvPred: Multimodal Hyperbolic Embedding with Angle-Aware   Hierarchical Contrastive Learning and Uncertainty Constraints for Survival   Prediction,"Multimodal learning that integrates histopathology images and genomic data holds great promise for cancer survival prediction. However, existing methods face key limitations: 1) They rely on multimodal mapping and metrics in Euclidean space, which cannot fully capture the hierarchical structures in histopathology (among patches from different resolutions) and genomics data (from genes to pathways). 2) They discretize survival time into independent risk intervals, which ignores its continuous and ordinal nature and fails to achieve effective optimization. 3) They treat censorship as a binary indicator, excluding censored samples from model optimization and not making full use of them. To address these challenges, we propose HySurvPred, a novel framework for survival prediction that integrates three key modules: Multimodal Hyperbolic Mapping (MHM), Angle-aware Ranking-based Contrastive Loss (ARCL) and Censor-Conditioned Uncertainty Constraint (CUC). Instead of relying on Euclidean space, we design the MHM module to explore the inherent hierarchical structures within each modality in hyperbolic space. To better integrate multimodal features in hyperbolic space, we introduce the ARCL module, which uses ranking-based contrastive learning to preserve the ordinal nature of survival time, along with the CUC module to fully explore the censored data. Extensive experiments demonstrate that our method outperforms state-of-the-art methods on five benchmark datasets. The source code is to be released.",Jiaqi Yang; Wenting Chen; Xiaohan Xing; Sean He; Xiaoling Luo; Xinheng Lyu; Linlin Shen; Guoping Qiu,,2025-03-18T03:26:22Z,http://arxiv.org/abs/2503.13862v1
2503.14437v2,Functional classification of metabolic networks,"Chemical reaction networks underpin biological and physical phenomena across scales, from microbial interactions to planetary atmosphere dynamics. Bacterial communities exhibit complex competitive interactions for resources, human organs and tissues demonstrate specialized biochemical functions, and planetary atmospheres are capable of displaying diverse organic and inorganic chemical processes. Despite their complexities, comparing these networks methodically remains a challenge due to the vast underlying degrees of freedom. In biological systems, comparative genomics has been pivotal in tracing evolutionary trajectories and classifying organisms via DNA sequences. However, purely genomic classifications often fail to capture functional roles within ecological systems. Metabolic changes driven by nutrient availability highlight the need for classification schemes that integrate metabolic information. Here we introduce and apply a computational framework for a classification scheme of organisms that compares matrix representations of chemical reaction networks using the Grassmann distance, corresponding to measuring distances between the fundamental subspaces of stoichiometric matrices. Applying this framework to human gut microbiome data confirms that metabolic distances are distinct from phylogenetic distances, underscoring the limitations of genetic information in metabolic classification. Importantly, our analysis of metabolic distances reveals functional groups of organisms enriched or depleted in specific metabolic processes and shows robustness to metabolically silent genetic perturbations. The generalizability of metabolic Grassmann distances is illustrated by application to chemical reaction networks in human tissue and planetary atmospheres, highlighting its potential for advancing functional comparisons across diverse chemical reaction systems.",Jorge Reyes; Jörn Dunkel,,2025-03-18T17:13:58Z,http://arxiv.org/abs/2503.14437v2
2503.16582v1,Machine Learning-Based Genomic Linguistic Analysis (Gene Sequence   Feature Learning): A Case Study on Predicting Heavy Metal Response Genes in   Rice,"This study explores the application of machine learning-based genetic linguistics for identifying heavy metal response genes in rice (Oryza sativa). By integrating convolutional neural networks and random forest algorithms, we developed a hybrid model capable of extracting and learning meaningful features from gene sequences, such as k-mer frequencies and physicochemical properties. The model was trained and tested on datasets of genes, achieving high predictive performance (precision: 0.89, F1-score: 0.82). RNA-seq and qRT-PCR experiments conducted on rice leaves which exposed to Hg0, revealed differential expression of genes associated with heavy metal responses, which validated the model's predictions. Co-expression network analysis identified 103 related genes, and a literature review indicated that these genes are highly likely to be involved in heavy metal-related biological processes. By integrating and comparing the analysis results with those of differentially expressed genes (DEGs), the validity of the new machine learning method was further demonstrated. This study highlights the efficacy of combining machine learning with genetic linguistics for large-scale gene prediction. It demonstrates a cost-effective and efficient approach for uncovering molecular mechanisms underlying heavy metal responses, with potential applications in developing stress-tolerant crop varieties.",Ruiqi Yang; Jianxu Wang; Wei Yuan; Xun Wang; Mei Li,,2025-03-20T13:41:31Z,http://arxiv.org/abs/2503.16582v1
2504.00020v2,Celler:A Genomic Language Model for Long-Tailed Single-Cell Annotation,"Recent breakthroughs in single-cell technology have ushered in unparalleled opportunities to decode the molecular intricacy of intricate biological systems, especially those linked to diseases unique to humans. However, these progressions have also ushered in novel obstacles-specifically, the efficient annotation of extensive, long-tailed single-cell data pertaining to disease conditions. To effectively surmount this challenge, we introduce Celler, a state-of-the-art generative pre-training model crafted specifically for the annotation of single-cell data. Celler incorporates two groundbreaking elements: First, we introduced the Gaussian Inflation (GInf) Loss function. By dynamically adjusting sample weights, GInf Loss significantly enhances the model's ability to learn from rare categories while reducing the risk of overfitting for common categories. Secondly, we introduce an innovative Hard Data Mining (HDM) strategy into the training process, specifically targeting the challenging-to-learn minority data samples, which significantly improved the model's predictive accuracy. Additionally, to further advance research in this field, we have constructed a large-scale single-cell dataset: Celler-75, which encompasses 40 million cells distributed across 80 human tissues and 75 specific diseases. This dataset provides critical support for comprehensively exploring the potential of single-cell technology in disease research. Our code is available at https://github.com/AI4science-ym/HiCeller.",Huan Zhao; Yiming Liu; Jina Yao; Ling Xiong; Zexin Zhou; Zixing Zhang,,2025-03-28T02:04:26Z,http://arxiv.org/abs/2504.00020v2
2504.10338v1,Classifying Copy Number Variations Using State Space Modeling of   Targeted Sequencing Data: A Case Study in Thalassemia,"Thalassemia, a blood disorder and one of the most prevalent hereditary genetic disorders worldwide, is often caused by copy number variations (CNVs) in the hemoglobin genes. This disorder has incredible diversity, with a large number of distinct profiles corresponding to alterations of different regions in the genes. Correctly classifying an individual's profile is critical as it impacts treatment, prognosis, and genetic counseling. However, genetic classification is challenging due to the large number of profiles worldwide, and often requires a large number of sequential tests. Targeted next generation sequencing (NGS), which characterizes segments of an individual's genome, has the potential to dramatically reduce the cost of testing and increase accuracy. In this work, we introduce a probabilistic state space model for profiling thalassemia from targeted NGS data, which naturally characterize the spatial ordering of the genes along the chromosome. We then use decision theory to choose the best profile among the different options. Due to our use of Bayesian methodology, we are also able to detect low-quality samples to be excluded from consideration, an important component of clinical screening. We evaluate our model on a dataset of 57 individuals, including both controls and cases with a variety of thalassemia profiles. Our model has a sensitivity of 0.99 and specificity of 0.93 for thalassemia detection, and accuracy of 91.5\% for characterizing subtypes. Furthermore, the specificity and accuracy rise to $0.96$ and 93.9\% when low-quality samples are excluded using our automated quality control method. This approach outperforms alternative methods, particularly in specificity, and is broadly applicable to other disorders.",Austin Talbot; Alex Kotlar; Lavanya Rishishiwar; Yue Ke,,2025-04-14T15:47:41Z,http://arxiv.org/abs/2504.10338v1
2504.17967v1,LLM Agent Swarm for Hypothesis-Driven Drug Discovery,"Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM ""agents"" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.",Kevin Song; Andrew Trotter; Jake Y. Chen,,2025-04-24T22:27:50Z,http://arxiv.org/abs/2504.17967v1
2505.04773v1,Estimating the Heritability of Longitudinal Rate-of-Change: Genetic   Insights into PSA Velocity in Prostate Cancer-Free Individuals,"Serum prostate-specific antigen (PSA) is widely used for prostate cancer screening. While the genetics of PSA levels has been studied to enhance screening accuracy, the genetic basis of PSA velocity, the rate of PSA change over time, remains unclear. The Prostate, Lung, Colorectal, and Ovarian (PLCO) Cancer Screening Trial, a large, randomized study with longitudinal PSA data (15,260 cancer-free males, averaging 5.34 samples per subject) and genome-wide genotype data, provides a unique opportunity to estimate PSA velocity heritability. We developed a mixed model to jointly estimate heritability of PSA levels at age 54 and PSA velocity. To accommodate the large dataset, we implemented two efficient computational approaches: a partitioning and meta-analysis strategy using average information restricted maximum likelihood (AI-REML), and a fast restricted Haseman-Elston (REHE) regression method. Simulations showed that both methods yield unbiased estimates of both heritability metrics, with AI-REML providing smaller variability in the estimation of velocity heritability than REHE. Applying AI-REML to PLCO data, we estimated heritability at 0.32 (s.e. = 0.07) for baseline PSA and 0.45 (s.e. = 0.18) for PSA velocity. These findings reveal a substantial genetic contribution to PSA velocity, supporting future genome-wide studies to identify variants affecting PSA dynamics and improve PSA-based screening.",Pei Zhang; Xiaoyu Wang; Jianxin Shi; Paul S. Albert,,2025-05-07T19:50:39Z,http://arxiv.org/abs/2505.04773v1
2505.08764v1,The Environment-Dependent Regulatory Landscape of the E. coli Genome,"All cells respond to changes in both their internal milieu and the environment around them through the regulation of their genes. Despite decades of effort, there remain huge gaps in our knowledge of both the function of many genes (the so-called y-ome) and how they adapt to changing environments via regulation. Here we describe a joint experimental and theoretical dissection of the regulation of a broad array of over 100 biologically interesting genes in E. coli across 39 diverse environments, permitting us to discover the binding sites and transcription factors that mediate regulatory control. Using a combination of mutagenesis, massively parallel reporter assays, mass spectrometry and tools from information theory and statistical physics, we go from complete ignorance of a promoter's environment-dependent regulatory architecture to predictive models of its behavior. As a proof of principle of the biological insights to be gained from such a study, we chose a combination of genes from the y-ome, toxin-antitoxin pairs, and genes hypothesized to be part of regulatory modules; in all cases, we discovered a host of new insights into their underlying regulatory landscape and resulting biological function.",Tom Röschinger; Heun Jin Lee; Rosalind Wenshan Pan; Grace Solini; Kian Faizi; Baiyi Quan; Tsui Fen Chou; Madhav Mani; Stephen Quake; Rob Phillips,,2025-05-13T17:33:25Z,http://arxiv.org/abs/2505.08764v1
2505.09883v1,DeepPlantCRE: A Transformer-CNN Hybrid Framework for Plant Gene   Expression Modeling and Cross-Species Generalization,"The investigation of plant transcriptional regulation constitutes a fundamental basis for crop breeding, where cis-regulatory elements (CREs), as the key factor determining gene expression, have become the focus of crop genetic improvement research. Deep learning techniques, leveraging their exceptional capacity for high-dimensional feature extraction and nonlinear regulatory relationship modeling, have been extensively employed in this field. However, current methodologies present notable limitations: single CNN-based architectures struggle to capture long-range regulatory interactions, while existing CNN-Transformer hybrid models demonstrate proneness to overfitting and inadequate generalization in cross-species prediction contexts. To address these challenges, this study proposes DeepPlantCRE, a deep-learning framework for plant gene expression prediction and CRE Extraction. The model employs a Transformer-CNN hybrid architecture that achieves enhanced Accuracy, AUC-ROC, and F1-score metrics over existing baselines (DeepCRE and PhytoExpr), with improved generalization performance and overfitting inhibiting. Cross-species validation experiments conducted on gene expression datasets from \textit{Gossypium}, \textit{Arabidopsis thaliana}, \textit{Solanum lycopersicum}, \textit{Sorghum bicolor}, and \textit{Arabidopsis thaliana} reveal that the model achieves peak prediction accuracy of 92.3\%, particularly excelling in complex genomic data analysis. Furthermore, interpretability investigations using DeepLIFT and Transcription Factor Motif Discovery from the importance scores algorithm (TF-MoDISco) demonstrate that the derived motifs from our model exhibit high concordance with known transcription factor binding sites (TFBSs) such as MYR2, TSO1 in JASPAR plant database, substantiating the potential of biological interpretability and practical agricultural application of DeepPlantCRE.",Yingjun Wu; Jingyun Huang; Liang Ming; Pengcheng Deng; Maojun Wang; Zeyu Zhang,,2025-05-15T00:59:29Z,http://arxiv.org/abs/2505.09883v1
2506.00096v1,PathGene: Benchmarking Driver Gene Mutations and Exon Prediction Using   Multicenter Lung Cancer Histopathology Image Dataset,"Accurately predicting gene mutations, mutation subtypes and their exons in lung cancer is critical for personalized treatment planning and prognostic assessment. Faced with regional disparities in medical resources and the high cost of genomic assays, using artificial intelligence to infer these mutations and exon variants from routine histopathology images could greatly facilitate precision therapy. Although some prior studies have shown that deep learning can accelerate the prediction of key gene mutations from lung cancer pathology slides, their performance remains suboptimal and has so far been limited mainly to early screening tasks. To address these limitations, we have assembled PathGene, which comprises histopathology images paired with next-generation sequencing reports from 1,576 patients at the Second Xiangya Hospital, Central South University, and 448 TCGA-LUAD patients. This multi-center dataset links whole-slide images to driver gene mutation status, mutation subtypes, exon, and tumor mutational burden (TMB) status, with the goal of leveraging pathology images to predict mutations, subtypes, exon locations, and TMB for early genetic screening and to advance precision oncology. Unlike existing datasets, we provide molecular-level information related to histopathology images in PathGene to facilitate the development of biomarker prediction models. We benchmarked 11 multiple-instance learning methods on PathGene for mutation, subtype, exon, and TMB prediction tasks. These experimental methods provide valuable alternatives for early genetic screening of lung cancer patients and assisting clinicians to quickly develop personalized precision targeted treatment plans for patients. Code and data are available at https://github.com/panliangrui/NIPS2025/.",Liangrui Pan; Qingchun Liang; Shen Zhao; Songqing Fan; Shaoliang Peng,,2025-05-30T11:51:11Z,http://arxiv.org/abs/2506.00096v1
2506.00146v1,A DNA Methylation Classification Model Predicts Organ and Disease Site,"Cell-free DNA (cfDNA) analysis is a powerful, minimally invasive tool for monitoring disease progression, treatment response, and early detection. A major challenge, however, is accurately determining the tissue of origin, especially in complex or heterogeneous disease contexts. To address this, we developed a machine learning framework that leverages tissue-specific DNA methylation signatures to classify both tissue and disease origin from cfDNA data. Our model integrates methylation datasets across diverse epigenomic platforms, including Whole Genome Bisulfite Sequencing (WGBS), Illumina Infinium Bead Arrays, and Enzymatic Methyl-seq (EM-seq). To account for platform variability and data sparsity, we applied imputation strategies and harmonized CpG features to enable cross-platform learning. Dimensionality reduction revealed clear tissue-specific clustering of methylation profiles. A random forest classifier trained on these features achieved consistent classification performance (accuracy 0.75-0.8 across test sets and platforms). Notably, our model distinguished clinically relevant tissues such as inflamed synovium and peripheral blood mononuclear cells (PBMCs) in arthritis patients and deconvoluted synthetic cfDNA mixtures mimicking real-world liquid biopsy samples. The predicted tissue proportions closely matched the true values, demonstrating the model's potential for both classification and quantitative inference. These results support the feasibility of using cross-platform methylation data and machine learning for scalable, generalizable cfDNA diagnostics and lay the groundwork for future integration of disease-specific epigenetic features to guide clinical decision-making in precision medicine.",Keng-Jung Lee; Dharanya Sampath; Konstantinos Mavrommatis,,2025-05-30T18:38:26Z,http://arxiv.org/abs/2506.00146v1
2506.01540v1,A nonparametric statistical method for deconvolving densities in the   analysis of proteomic data,"In medical research, often, genomic or proteomic data are collected, with measurements frequently subject to uncertainties or errors, making it crucial to accurately separate the signals of the genes or proteins, respectively, from the noise. Such a signal separation is also of interest in skin aging research in which intrinsic aging driven by genetic factors and extrinsic, i.e.\ environmentally induced, aging are investigated by considering, e.g., the proteome of skin fibroblasts. Since extrinsic influences on skin aging can only be measured alongside intrinsic ones, it is essential to isolate the pure extrinsic signal from the combined intrinisic and extrinsic signal. In such situations, deconvolution methods can be employed to estimate the signal's density function from the data. However, existing nonparametric deconvolution approaches often fail when the variance of the mixed distribution is substantially greater than the variance of the target distribution, which is a common issue in genomic and proteomic data.   We, therefore, propose a new nonparametric deconvolution method called N-Power Fourier Deconvolution (NPFD) that addresses this issue by employing the $N$-th power of the Fourier transform of transformed densities. This procedure utilizes the Fourier transform inversion theorem and exploits properties of Fourier transforms of density functions to mitigate numerical inaccuracies through exponentiation, leading to accurate and smooth density estimation. An extensive simulation study demonstrates that NPFD effectively handles the variance issues and performs comparably or better than existing deconvolution methods in most scenarios. Moreover, applications to real medical data, particularly to proteomic data from fibroblasts affected by intrinsic and extrinsic aging, show how NPFD can be employed to estimate the pure extrinsic density.",Akin Anarat; Jean Krutmann; Holger Schwender,,2025-06-02T11:05:46Z,http://arxiv.org/abs/2506.01540v1
2506.11969v1,A Robust Local Fréchet Regression Using Unbalanced Neural Optimal   Transport with Applications to Dynamic Single-cell Genomics Data,"Single-cell RNA sequencing (scRNA-seq) technologies have enabled the profiling of gene expression for a collection of cells across time during a dynamic biological process. Given that each time point provides only a static snapshot, modeling and understanding the underlying cellular dynamics remains a central yet challenging task in modern genomics. To associate biological time with single cell distributions, we develop a robust local Fr\'echet regression for interpolating the high-dimensional cellular distribution at any given time point using data observed over a finite time points. To allow for robustness in cell distributions, we propose to apply the unbalanced optimal transport-based Wasserstein distance in our local Fr\'echet regression analysis. We develop a computationally efficient algorithm to generate the cell distribution for a given time point using generative neural networks. The resulting single cell generated models and the corresponding transport plans can be use to interpolate the single cells at any unobserved time point and to track the cell trajectory during the cell differentiation process. We demonstrate the methods using three single cell differentiation data sets, including differentiation of human embryonic stem cells into embryoids, mouse hematopoietic and progenitor cell differentiation, and reprogramming of mouse embryonic fibroblasts. We show that the proposed methods lead to better single cell interpolations, reveal different cell differential trajectories, and identify early genes that regulate these cell trajectories.",Binghao Yan; Hongzhe Li,,2025-06-13T17:27:58Z,http://arxiv.org/abs/2506.11969v1
2506.18479v1,"Bayesian integrative factor analysis methods, with application in   nutrition and genomics data","High-dimensional data are crucial in biomedical research. Integrating such data from multiple studies is a critical process that relies on the choice of advanced statistical models, enhancing statistical power, reproducibility, and scientific insight compared to analyzing each study separately. Factor analysis (FA) is a core dimensionality reduction technique that models observed data through a small set of latent factors. Bayesian extensions of FA have recently emerged as powerful tools for multi-study integration, enabling researchers to disentangle shared biological signals from study-specific variability. In this tutorial, we provide a practical and comparative guide to five advanced Bayesian integrative factor models: Perturbed Factor Analysis (PFA), Bayesian Factor Regression with non-local spike-and-slab priors (MOM-SS), Subspace Factor Analysis (SUFA), Bayesian Multi-study Factor Analysis (BMSFA), and Bayesian Combinatorial Multi-study Factor Analysis (Tetris). To contextualize these methods, we also include two benchmark approaches: standard FA applied to pooled data (Stack FA) and FA applied separately to each study (Ind FA). We evaluate all methods through extensive simulations, assessing computational efficiency and accuracy in the estimation of loadings and number of factors. To bridge theory and practice, we present a full analytical workflow, with detailed R code, demonstrating how to apply these models to real-world datasets in nutrition and genomics. This tutorial is designed to guide applied researchers through the landscape of Bayesian integrative factor analysis, offering insights and tools for extracting interpretable, robust patterns from complex multi-source data. All code and resources are available at: https://github.com/Mavis-Liang/Bayesian_integrative_FA_tutorial",Mavis Liang; Blake Hansen; Alejandra Avalos-Pacheco; Roberta De Vito,,2025-06-23T10:25:28Z,http://arxiv.org/abs/2506.18479v1
2506.20697v1,scMamba: A Scalable Foundation Model for Single-Cell Multi-Omics   Integration Beyond Highly Variable Feature Selection,"The advent of single-cell multi-omics technologies has enabled the simultaneous profiling of diverse omics layers within individual cells. Integrating such multimodal data provides unprecedented insights into cellular identity, regulatory processes, and disease mechanisms. However, it remains challenging, as current methods often rely on selecting highly variable genes or peaks during preprocessing, which may inadvertently discard crucial biological information. Here, we present scMamba, a foundation model designed to integrate single-cell multi-omics data without the need for prior feature selection while preserving genomic positional information. scMamba introduces a patch-based cell tokenization strategy that treats genomics regions as words (tokens) and cells as sentences. Building upon the concept of state space duality, scMamba distills rich biological insights from high-dimensional, sparse single-cell multi-omics data. Additionally, our novel contrastive learning approach, enhanced with cosine similarity regularization, enables superior alignment across omics layers compared to traditional methods. Systematic benchmarking across multiple datasets demonstrates that scMamba significantly outperforms state-of-the-art methods in preserving biological variation, aligning omics layers, and enhancing key downstream tasks such as clustering, cell type annotation, and trajectory inference. Our findings position scMamba as a powerful tool for large-scale single-cell multi-omics integration, capable of handling large-scale atlases and advancing biological discovery.",Zhen Yuan; Shaoqing Jiao; Yihang Xiao; Jiajie Peng,,2025-06-25T12:58:01Z,http://arxiv.org/abs/2506.20697v1
2507.09378v1,Context-Aware Regularization with Markovian Integration for   Attention-Based Nucleotide Analysis,"Transformers have revolutionized nucleotide sequence analysis, yet capturing long-range dependencies remains challenging. Recent studies show that autoregressive transformers often exhibit Markovian behavior by relying on fixed-length context windows for next-token prediction. However, standard self-attention mechanisms are computationally inefficient for long sequences due to their quadratic complexity and do not explicitly enforce global transition consistency.   We introduce CARMANIA (Context-Aware Regularization with Markovian Integration for Attention-Based Nucleotide Analysis), a self-supervised pretraining framework that augments next-token (NT) prediction with a transition-matrix (TM) loss. The TM loss aligns predicted token transitions with empirically derived n-gram statistics from each input sequence, encouraging the model to capture higher-order dependencies beyond local context. This integration enables CARMANIA to learn organism-specific sequence structures that reflect both evolutionary constraints and functional organization.   We evaluate CARMANIA across diverse genomic tasks, including regulatory element prediction, functional gene classification, taxonomic inference, antimicrobial resistance detection, and biosynthetic gene cluster classification. CARMANIA outperforms the previous best long-context model by at least 7 percent, matches state-of-the-art on shorter sequences (exceeding prior results on 20 out of 40 tasks while running approximately 2.5 times faster), and shows particularly strong improvements on enhancer and housekeeping gene classification tasks, including up to a 34 percent absolute gain in Matthews correlation coefficient (MCC) for enhancer prediction. The TM loss boosts accuracy in 33 of 40 tasks, especially where local motifs or regulatory patterns drive prediction.",Mohammadsaleh Refahi; Mahdi Abavisani; Bahrad A. Sokhansanj; James R. Brown; Gail Rosen,,2025-07-12T19:03:28Z,http://arxiv.org/abs/2507.09378v1
2507.11313v1,Tree inference with varifold distances,"In this paper, we consider a tree inference problem motivated by the critical problem in single-cell genomics of reconstructing dynamic cellular processes from sequencing data. In particular, given a population of cells sampled from such a process, we are interested in the problem of ordering the cells according to their progression in the process. This is known as trajectory inference. If the process is differentiation, this amounts to reconstructing the corresponding differentiation tree. One way of doing this in practice is to estimate the shortest-path distance between nodes based on cell similarities observed in sequencing data. Recent sequencing techniques make it possible to measure two types of data: gene expression levels, and RNA velocity, a vector that predicts changes in gene expression. The data then consist of a discrete vector field on a (subset of a) Euclidean space of dimension equal to the number of genes under consideration. By integrating this velocity field, we trace the evolution of gene expression levels in each single cell from some initial stage to its current stage. Eventually, we assume that we have a faithful embedding of the differentiation tree in a Euclidean space, but which we only observe through the curves representing the paths from the root to the nodes. Using varifold distances between such curves, we define a similarity measure between nodes which we prove approximates the shortest-path distance in a tree that is isomorphic to the target tree.",Elodie Maignant; Tim Conrad; Christoph von Tycowicz,,2025-07-15T13:45:20Z,http://arxiv.org/abs/2507.11313v1
2507.11848v1,Interactive Hybrid Rice Breeding with Parametric Dual Projection,"Hybrid rice breeding crossbreeds different rice lines and cultivates the resulting hybrids in fields to select those with desirable agronomic traits, such as higher yields. Recently, genomic selection has emerged as an efficient way for hybrid rice breeding. It predicts the traits of hybrids based on their genes, which helps exclude many undesired hybrids, largely reducing the workload of field cultivation. However, due to the limited accuracy of genomic prediction models, breeders still need to combine their experience with the models to identify regulatory genes that control traits and select hybrids, which remains a time-consuming process. To ease this process, in this paper, we proposed a visual analysis method to facilitate interactive hybrid rice breeding. Regulatory gene identification and hybrid selection naturally ensemble a dual-analysis task. Therefore, we developed a parametric dual projection method with theoretical guarantees to facilitate interactive dual analysis. Based on this dual projection method, we further developed a gene visualization and a hybrid visualization to verify the identified regulatory genes and hybrids. The effectiveness of our method is demonstrated through the quantitative evaluation of the parametric dual projection method, identified regulatory genes and desired hybrids in the case study, and positive feedback from breeders.",Changjian Chen; Pengcheng Wang; Fei Lyu; Zhuo Tang; Li Yang; Long Wang; Yong Cai; Feng Yu; Kenli Li,,2025-07-16T02:25:31Z,http://arxiv.org/abs/2507.11848v1
2507.12805v1,PMKLC: Parallel Multi-Knowledge Learning-based Lossless Compression for   Large-Scale Genomics Database,"Learning-based lossless compressors play a crucial role in large-scale genomic database backup, storage, transmission, and management. However, their 1) inadequate compression ratio, 2) low compression \& decompression throughput, and 3) poor compression robustness limit their widespread adoption and application in both industry and academia. To solve those challenges, we propose a novel \underline{P}arallel \underline{M}ulti-\underline{K}nowledge \underline{L}earning-based \underline{C}ompressor (PMKLC) with four crucial designs: 1) We propose an automated multi-knowledge learning-based compression framework as compressors' backbone to enhance compression ratio and robustness; 2) we design a GPU-accelerated ($s$,$k$)-mer encoder to optimize compression throughput and computing resource usage; 3) we introduce data block partitioning and Step-wise Model Passing (SMP) mechanisms for parallel acceleration; 4) We design two compression modes PMKLC-S and PMKLC-M to meet the complex application scenarios, where the former runs on a resource-constrained single GPU and the latter is multi-GPU accelerated. We benchmark PMKLC-S/M and 14 baselines (7 traditional and 7 leaning-based) on 15 real-world datasets with different species and data sizes. Compared to baselines on the testing datasets, PMKLC-S/M achieve the average compression ratio improvement up to 73.609\% and 73.480\%, the average throughput improvement up to 3.036$\times$ and 10.710$\times$, respectively. Besides, PMKLC-S/M also achieve the best robustness and competitive memory cost, indicating its greater stability against datasets with different probability distribution perturbations, and its strong ability to run on memory-constrained devices.",Hui Sun; Yanfeng Ding; Liping Yi; Huidong Ma; Gang Wang; Xiaoguang Liu; Cheng Zhong; Wentong Cai,,2025-07-17T05:46:08Z,http://arxiv.org/abs/2507.12805v1
2507.16978v1,Fast and Scalable Gene Embedding Search: A Comparative Study of FAISS   and ScaNN,"The exponential growth of DNA sequencing data has outpaced traditional heuristic-based methods, which struggle to scale effectively. Efficient computational approaches are urgently needed to support large-scale similarity search, a foundational task in bioinformatics for detecting homology, functional similarity, and novelty among genomic and proteomic sequences. Although tools like BLAST have been widely used and remain effective in many scenarios, they suffer from limitations such as high computational cost and poor performance on divergent sequences.   In this work, we explore embedding-based similarity search methods that learn latent representations capturing deeper structural and functional patterns beyond raw sequence alignment. We systematically evaluate two state-of-the-art vector search libraries, FAISS and ScaNN, on biologically meaningful gene embeddings. Unlike prior studies, our analysis focuses on bioinformatics-specific embeddings and benchmarks their utility for detecting novel sequences, including those from uncharacterized taxa or genes lacking known homologs. Our results highlight both computational advantages (in memory and runtime efficiency) and improved retrieval quality, offering a promising alternative to traditional alignment-heavy tools.",Mohammad Saleh Refahi; Gavin Hearne; Harrison Muller; Kieran Lynch; Bahrad A. Sokhansanj; James R. Brown; Gail Rosen,,2025-07-22T19:28:54Z,http://arxiv.org/abs/2507.16978v1
2507.20644v1,Deep Generative Models of Evolution: SNP-level Population Adaptation by   Genomic Linkage Incorporation,"The investigation of allele frequency trajectories in populations evolving under controlled environmental pressures has become a popular approach to study evolutionary processes on the molecular level. Statistical models based on well-defined evolutionary concepts can be used to validate different hypotheses about empirical observations. Despite their popularity, classic statistical models like the Wright-Fisher model suffer from simplified assumptions such as the independence of selected loci along a chromosome and uncertainty about the parameters. Deep generative neural networks offer a powerful alternative known for the integration of multivariate dependencies and noise reduction. Due to their high data demands and challenging interpretability they have, so far, not been widely considered in the area of population genomics. To address the challenges in the area of Evolve and Resequencing experiments (E&R) based on pooled sequencing (Pool-Seq) data, we introduce a deep generative neural network that aims to model a concept of evolution based on empirical observations over time. The proposed model estimates the distribution of allele frequency trajectories by embedding the observations from single nucleotide polymorphisms (SNPs) with information from neighboring loci. Evaluation on simulated E&R experiments demonstrates the model's ability to capture the distribution of allele frequency trajectories and illustrates the representational power of deep generative models on the example of linkage disequilibrium (LD) estimation. Inspecting the internally learned representations enables estimating pairwise LD, which is typically inaccessible in Pool-Seq data. Our model provides competitive LD estimation in Pool-Seq data high degree of LD when compared to existing methods.",Julia Siekiera; Christian Schlötterer; Stefan Kramer,,2025-07-28T09:03:09Z,http://arxiv.org/abs/2507.20644v1
2507.21035v2,GenoMAS: A Multi-Agent Framework for Scientific Discovery via   Code-Driven Gene Expression Analysis,"Gene expression analysis holds the key to many biomedical discoveries, yet extracting insights from raw transcriptomic data remains formidable due to the complexity of multiple large, semi-structured files and the need for extensive domain expertise. Current automation approaches are often limited by either inflexible workflows that break down in edge cases or by fully autonomous agents that lack the necessary precision for rigorous scientific inquiry. GenoMAS charts a different course by presenting a team of LLM-based scientists that integrates the reliability of structured workflows with the adaptability of autonomous agents. GenoMAS orchestrates six specialized LLM agents through typed message-passing protocols, each contributing complementary strengths to a shared analytic canvas. At the heart of GenoMAS lies a guided-planning framework: programming agents unfold high-level task guidelines into Action Units and, at each juncture, elect to advance, revise, bypass, or backtrack, thereby maintaining logical coherence while bending gracefully to the idiosyncrasies of genomic data.   On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene identification, surpassing the best prior art by 10.61% and 16.85% respectively. Beyond metrics, GenoMAS surfaces biologically plausible gene-phenotype associations corroborated by the literature, all while adjusting for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.",Haoyang Liu; Yijiang Li; Haohan Wang,,2025-07-28T17:55:08Z,http://arxiv.org/abs/2507.21035v2
2508.02743v1,A Novel cVAE-Augmented Deep Learning Framework for Pan-Cancer RNA-Seq   Classification,"Pan-cancer classification using transcriptomic (RNA-Seq) data can inform tumor subtyping and therapy selection, but is challenging due to extremely high dimensionality and limited sample sizes. In this study, we propose a novel deep learning framework that uses a class-conditional variational autoencoder (cVAE) to augment training data for pan-cancer gene expression classification. Using 801 tumor RNA-Seq samples spanning 5 cancer types from The Cancer Genome Atlas (TCGA), we first perform feature selection to reduce 20,531 gene expression features to the 500 most variably expressed genes. A cVAE is then trained on this data to learn a latent representation of gene expression conditioned on cancer type, enabling the generation of synthetic gene expression samples for each tumor class. We augment the training set with these cVAE-generated samples (doubling the dataset size) to mitigate overfitting and class imbalance. A two-layer multilayer perceptron (MLP) classifier is subsequently trained on the augmented dataset to predict tumor type. The augmented framework achieves high classification accuracy (~98%) on a held-out test set, substantially outperforming a classifier trained on the original data alone. We present detailed experimental results, including VAE training curves, classifier performance metrics (ROC curves and confusion matrix), and architecture diagrams to illustrate the approach. The results demonstrate that cVAE-based synthetic augmentation can significantly improve pan-cancer prediction performance, especially for underrepresented cancer classes.",Vinil Polepalli,,2025-08-02T16:57:31Z,http://arxiv.org/abs/2508.02743v1
2508.08312v1,CFM-GP: Unified Conditional Flow Matching to Learn Gene Perturbation   Across Cell Types,"Understanding gene perturbation effects across diverse cellular contexts is a central challenge in functional genomics, with important implications for therapeutic discovery and precision medicine. Single-cell technologies enable high-resolution measurement of transcriptional responses, but collecting such data is costly and time-consuming, especially when repeated for each cell type. Existing computational methods often require separate models per cell type, limiting scalability and generalization. We present CFM-GP, a method for cell type-agnostic gene perturbation prediction. CFM-GP learns a continuous, time-dependent transformation between unperturbed and perturbed gene expression distributions, conditioned on cell type, allowing a single model to predict across all cell types. Unlike prior approaches that use discrete modeling, CFM-GP employs a flow matching objective to capture perturbation dynamics in a scalable manner. We evaluate on five datasets: SARS-CoV-2 infection, IFN-beta stimulated PBMCs, glioblastoma treated with Panobinostat, lupus under IFN-beta stimulation, and Statefate progenitor fate mapping. CFM-GP consistently outperforms state-of-the-art baselines in R-squared and Spearman correlation, and pathway enrichment analysis confirms recovery of key biological pathways. These results demonstrate the robustness and biological fidelity of CFM-GP as a scalable solution for cross-cell type gene perturbation prediction.",Abrar Rahman Abir; Sajib Acharjee Dip; Liqing Zhang,,2025-08-09T00:00:17Z,http://arxiv.org/abs/2508.08312v1
2508.11069v1,Functional Analysis of Variance for Association Studies,"While progress has been made in identifying common genetic variants associated with human diseases, for most of common complex diseases, the identified genetic variants only account for a small proportion of heritability. Challenges remain in finding additional unknown genetic variants predisposing to complex diseases. With the advance in next-generation sequencing technologies, sequencing studies have become commonplace in genetic research. The ongoing exome-sequencing and whole-genome-sequencing studies generate a massive amount of sequencing variants and allow researchers to comprehensively investigate their role in human diseases. The discovery of new disease-associated variants can be enhanced by utilizing powerful and computationally efficient statistical methods. In this paper, we propose a functional analysis of variance (FANOVA) method for testing an association of sequence variants in a genomic region with a qualitative trait. The FANOVA has a number of advantages: (1) it tests for a joint effect of gene variants, including both common and rare; (2) it fully utilizes linkage disequilibrium and genetic position information; and (3) allows for either protective or risk-increasing causal variants. Through simulations, we show that FANOVA outperform two popularly used methods - SKAT and a previously proposed method based on functional linear models (FLM), - especially if a sample size of a study is small and/or sequence variants have low to moderate effects. We conduct an empirical study by applying three methods (FANOVA, SKAT and FLM) to sequencing data from Dallas Heart Study. While SKAT and FLM respectively detected ANGPTL 4 and ANGPTL 3 associated with obesity, FANOVA was able to identify both genes associated with obesity.",Olga A. Vsevolozhskaya; Dmitri V. Zaykin; Mark C. Greenwood; Changshuai Wei; Qing Lu,,2025-08-14T21:02:45Z,http://arxiv.org/abs/2508.11069v1
2508.12464v1,On the Fitness Landscape in the $NK$ Model,"The $NK$ model, introduced by Kauffman, Levin, and Weinberger, is a random field used to describe the fitness landscape of certain species with $N$ genetic loci, each interacting with $K$ others. The model has wide applications in understanding evolutionary and natural selection as it captures ruggedness feature of the fitness landscape. Earlier literature has been focused on the case $K$ being a fixed positive integer and used tools from Ergodic and Markov theory. In this paper, by viewing it as a statistical physics object, we investigate the $NK$ model in the regime $K/N\to\alpha \in(0,1]$ via the spin glass methodologies. Our main result identifies the exact limits for the free energy at any temperature and the maximum fitness. Moreover, we show that the $NK$ model exhibits a multiple-peak structure, namely, the number of near-fittest genomes that are asymptotically orthogonal to each other is exponentially large. Based on establishing the overlap gap properties, we obtain quantitative descriptions for the geometry of the fitness landscape and deduce that, in particular, near-fittest evolutionary paths become impossible as the fitness levels of the genomes approach the global maximum for any $\alpha\in (0,1]$. Nevertheless, we also show that by choosing $\alpha$ sufficiently small, an evolutionary path maintained at a given fitness level can be constructed with high probability.",Wei-Kuo Chen; Si Tang,,2025-08-17T18:27:54Z,http://arxiv.org/abs/2508.12464v1
2508.13552v1,Collapsing ROC approach for risk prediction research on both common and   rare variants,"Risk prediction that capitalizes on emerging genetic findings holds great promise for improving public health and clinical care. However, recent risk prediction research has shown that predictive tests formed on existing common genetic loci, including those from genome-wide association studies, have lacked sufficient accuracy for clinical use. Because most rare variants on the genome have not yet been studied for their role in risk prediction, future disease prediction discoveries should shift toward a more comprehensive risk prediction strategy that takes into account both common and rare variants. We are proposing a collapsing receiver operating characteristic CROC approach for risk prediction research on both common and rare variants. The new approach is an extension of a previously developed forward ROC FROC approach, with additional procedures for handling rare variants. The approach was evaluated through the use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction model built on all SNPs gained more accuracy AUC = 0.605 than one built on common variants alone AUC = 0.585. We further evaluated the performance of two approaches by gradually reducing the number of common variants in the analysis. We found that the CROC method attained more accuracy than the FROC method when the number of common variants in the data decreased. In an extreme scenario, when there are only rare variants in the data, the CROC reached an AUC value of 0.603, whereas the FROC had an AUC value of 0.524.",Changshuai Wei; Qing Lu,,2025-08-19T06:21:50Z,http://arxiv.org/abs/2508.13552v1
2503.13189v1,Causes of evolutionary divergence in prostate cancer,"Cancer progression involves the sequential accumulation of genetic alterations that cumulatively shape the tumour phenotype. In prostate cancer, tumours can follow divergent evolutionary trajectories that lead to distinct subtypes, but the causes of this divergence remain unclear. While causal inference could elucidate the factors involved, conventional methods are unsuitable due to the possibility of unobserved confounders and ambiguity in the direction of causality. Here, we propose a method that circumvents these issues and apply it to genomic data from 829 prostate cancer patients. We identify several genetic alterations that drive divergence as well as others that prevent this transition, locking tumours into one trajectory. Further analysis reveals that these genetic alterations may cause each other, implying a positive-feedback loop that accelerates divergence. Our findings provide insights into how cancer subtypes emerge and offer a foundation for genomic surveillance strategies aimed at monitoring the progression of prostate cancer.",Emre Esenturk; Atef Sahli; Valeriia Haberland; Aleksandra Ziuboniewicz; Christopher Wirth; G. Steven Bova; Robert G Bristow; Mark N Brook; Benedikt Brors; Adam Butler; Géraldine Cancel-Tassin; Kevin CL Cheng; Colin S Cooper; Niall M Corcoran; Olivier Cussenot; Ros A Eeles; Francesco Favero; Clarissa Gerhauser; Abraham Gihawi; Etsehiwot G Girma; Vincent J Gnanapragasam; Andreas J Gruber; Anis Hamid; Vanessa M Hayes; Housheng Hansen He; Christopher M Hovens; Eddie Luidy Imada; G. Maria Jakobsdottir; Chol-hee Jung; Francesca Khani; Zsofia Kote-Jarai; Philippe Lamy; Gregory Leeman; Massimo Loda; Pavlo Lutsik; Luigi Marchionni; Ramyar Molania; Anthony T Papenfuss; Diogo Pellegrina; Bernard Pope; Lucio R Queiroz; Tobias Rausch; Jüri Reimand; Brian Robinson; Thorsten Schlomm; Karina D Sørensen; Sebastian Uhrig; Joachim Weischenfeldt; Yaobo Xu; Takafumi N Yamaguchi; Claudio Zanettini; Andy G Lynch; David C Wedge; Daniel S Brewer; Dan J Woodcock,,2025-03-17T14:00:02Z,http://arxiv.org/abs/2503.13189v1
2503.18810v1,Combining multiplexed functional data to improve variant classification,"With the surge in the number of variants of uncertain significance (VUS) reported in ClinVar in recent years, there is an imperative to resolve VUS at scale. Multiplexed assays of variant effect (MAVEs), which allow the functional consequence of 100s to 1000s of genetic variants to be measured in a single experiment, are emerging as a source of evidence which can be used for clinical gene variant classification. Increasingly, there are multiple published MAVEs for the same gene, sometimes measuring different aspects of variant impact. Where multiple functional consequences may need to be considered to get a more complete understanding of variant effects for a given gene, combining data from multiple MAVEs may lead to the assignment of increased evidence strength which could impact variant classifications. Here, we provide guidance for combining such multiplexed functional data, incorporating a stepwise process from data curation and collection to model generation and validation. We illustrate the potential of this approach by showing the integration of multiplexed functional data from four MAVEs for the gene TP53. By following these steps, researchers can maximize the value of MAVEs, strengthen the functional evidence for clinical variant classification, reclassify more VUS, and potentially uncover novel mechanisms of pathogenicity for clinically relevant genes.",Atlas of Variant Effects Alliance;  :; Jeffrey D. Calhoun; Moez Dawood; Charlie F. Rowlands; Shawn Fayer; Elizabeth J. Radford; Abbye E. McEwen; Clare Turnbull; Amanda B. Spurdle; Lea M. Starita; Sujatha Jagannathan,"Ken and Ruth Davee Department of Neurology, Northwestern Feinberg School of Medicine, Chicago, Illinois; Ken and Ruth Davee Department of Neurology, Northwestern Feinberg School of Medicine, Chicago, Illinois; Ken and Ruth Davee Department of Neurology, Northwestern Feinberg School of Medicine, Chicago, Illinois; Human Genome Sequencing Center, Baylor College of Medicine, Houston, TX, USA; Division of Genetics and Epidemiology, The Institute of Cancer Research, London, UK; Brotman Baty Institute for Precision Medicine, Seattle, WA, USA; Wellcome Sanger Institute, Hinxton, CB10 1SA, UK; Brotman Baty Institute for Precision Medicine, Seattle, WA, USA; Division of Genetics and Epidemiology, The Institute of Cancer Research, London, UK; Population Health Program, QIMR Berghofer Medical Research Institute, Herston, Australia; Division of Genetics and Epidemiology, The Institute of Cancer Research, London, UK; Department of Biochemistry and Molecular Genetics, University of Colorado Anschutz Medical Campus, Aurora, CO, USA",2025-03-24T15:51:25Z,http://arxiv.org/abs/2503.18810v1
2502.09351v1,"Data Sharing in the PRIMED Consortium: Design, implementation, and   recommendations for future policymaking","Sharing diverse genomic and other biomedical datasets is critical to advance scientific discoveries and their equitable translation to improve human health. However, data sharing remains challenging in the context of legacy datasets, evolving policies, multi-institutional consortium science, and international stakeholders. The NIH-funded Polygenic Risk Methods in Diverse Populations (PRIMED) Consortium was established to improve the performance of polygenic risk estimates for a broad range of health and disease outcomes with global impacts. Improving polygenic risk score performance across genetically diverse populations requires access to large, diverse cohorts. We report on the design and implementation of data sharing policies and procedures developed in PRIMED to aggregate and analyze data from multiple, heterogeneous sources while adhering to existing data sharing policies for each integrated dataset. We describe two primary data sharing mechanisms: coordinated dbGaP applications and a Consortium Data Sharing Agreement, as well as provide alternatives when individual-level data cannot be shared within the Consortium (e.g., federated analyses). We also describe technical implementation of Consortium data sharing in the NHGRI Analysis Visualization and Informatics Lab-space (AnVIL) cloud platform, to share derived individual-level data, genomic summary results, and methods workflows with appropriate permissions. As a Consortium making secondary use of pre-existing data sources, we also discuss challenges and propose solutions for release of individual- and summary-level data products to the broader scientific community. We make recommendations for ongoing and future policymaking with the goal of informing future consortia and other research activities.",Johanna L. Smith; Quenna Wong; Whitney Hornsby; Matthew P. Conomos; Benjamin D. Heavner; Iftikhar J. Kullo; Bruce M. Psaty; Stephen S. Rich; Bamidele Tayo; Pradeep Natarajan; Sarah C. Nelson; Polygenic Risk Methods in Diverse Populations PRIMED Consortium Data Sharing Working Group; Polygenic Risk Methods in Diverse Populations PRIMED Consortium,,2025-02-12T17:31:39Z,http://arxiv.org/abs/2502.09351v1
2505.14675v1,Semi-parametric efficient estimation of small genetic effects in   large-scale population cohorts,"Population genetics seeks to quantify DNA variant associations with traits or diseases, as well as interactions among variants and with environmental factors. Computing millions of estimates in large cohorts in which small effect sizes are expected, necessitates minimising model-misspecification bias to control false discoveries. We present TarGene, a unified statistical workflow for the semi-parametric efficient and double robust estimation of genetic effects including k-point interactions among categorical variables in the presence of confounding and weak population dependence. k-point interactions, or Average Interaction Effects (AIEs), are a direct generalisation of the usual average treatment effect (ATE). We estimate AIEs with cross-validated and/or weighted versions of Targeted Minimum Loss-based Estimators (TMLE) and One-Step Estimators (OSE). The effect of dependence among data units on variance estimates is corrected by using sieve plateau variance estimators based on genetic relatedness across the units. We present extensive realistic simulations to demonstrate power, coverage, and control of type I error. Our motivating application is the targeted estimation of genetic effects on trait, including two-point and higher-order gene-gene and gene-environment interactions, in large-scale genomic databases such as UK Biobank and All of Us. All cross-validated and/or weighted TMLE and OSE for the AIE k-point interaction, as well as ATEs, conditional ATEs and functions thereof, are implemented in the general purpose Julia package TMLE.jl. For high-throughput applications in population genomics, we provide the open-source Nextflow pipeline and software TarGene which integrates seamlessly with modern high-performance and cloud computing platforms.",Olivier Labayle; Breeshey Roskams-Hieter; Joshua Slaughter; Kelsey Tetley-Campbell; Mark J. van der Laan; Chris P. Ponting; Sjoerd Viktor Beentjes; Ava Khamseh,,2025-05-20T17:58:10Z,http://arxiv.org/abs/2505.14675v1
2505.15868v1,An Inclusive Foundation Model for Generalizable Cytogenetics in   Precision Oncology,"Chromosome analysis is vital for diagnosing genetic disorders and guiding cancer therapy decisions through the identification of somatic clonal aberrations. However, developing an AI model are hindered by the overwhelming complexity and diversity of chromosomal abnormalities, requiring extensive annotation efforts, while automated methods remain task-specific and lack generalizability due to the scarcity of comprehensive datasets spanning diverse resource conditions. Here, we introduce CHROMA, a foundation model for cytogenomics, designed to overcome these challenges by learning generalizable representations of chromosomal abnormalities. Pre-trained on over 84,000 specimens (~4 million chromosomal images) via self-supervised learning, CHROMA outperforms other methods across all types of abnormalities, even when trained on fewer labelled data and more imbalanced datasets. By facilitating comprehensive mapping of instability and clonal leisons across various aberration types, CHROMA offers a scalable and generalizable solution for reliable and automated clinical analysis, reducing the annotation workload for experts and advancing precision oncology through the early detection of rare genomic abnormalities, enabling broad clinical AI applications and making advanced genomic analysis more accessible.",Changchun Yang; Weiqian Dai; Yilan Zhang; Siyuan Chen; Jingdong Hu; Junkai Su; Yuxuan Chen; Ao Xu; Na Li; Xin Gao; Yongguo Yu,"Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine; Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine; Computer Science Program, Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology; Computer Science Program, Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology; Smiltec; Smiltec; Smiltec; Smiltec; Smiltec; Computer Science Program, Computer, Electrical and Mathematical Sciences and Engineering Division, King Abdullah University of Science and Technology; Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine",2025-05-21T12:03:37Z,http://arxiv.org/abs/2505.15868v1
2505.23879v1,CNN-LSTM Hybrid Model for AI-Driven Prediction of COVID-19 Severity from   Spike Sequences and Clinical Data,"The COVID-19 pandemic, caused by SARS-CoV-2, highlighted the critical need for accurate prediction of disease severity to optimize healthcare resource allocation and patient management. The spike protein, which facilitates viral entry into host cells, exhibits high mutation rates, particularly in the receptor-binding domain, influencing viral pathogenicity. Artificial intelligence approaches, such as deep learning, offer promising solutions for leveraging genomic and clinical data to predict disease outcomes. Objective: This study aimed to develop a hybrid CNN-LSTM deep learning model to predict COVID-19 severity using spike protein sequences and associated clinical metadata from South American patients. Methods: We retrieved 9,570 spike protein sequences from the GISAID database, of which 3,467 met inclusion criteria after standardization. The dataset included 2,313 severe and 1,154 mild cases. A feature engineering pipeline extracted features from sequences, while demographic and clinical variables were one-hot encoded. A hybrid CNN-LSTM architecture was trained, combining CNN layers for local pattern extraction and an LSTM layer for long-term dependency modeling. Results: The model achieved an F1 score of 82.92%, ROC-AUC of 0.9084, precision of 83.56%, and recall of 82.85%, demonstrating robust classification performance. Training stabilized at 85% accuracy with minimal overfitting. The most prevalent lineages (P.1, AY.99.2) and clades (GR, GK) aligned with regional epidemiological trends, suggesting potential associations between viral genetics and clinical outcomes. Conclusion: The CNN-LSTM hybrid model effectively predicted COVID-19 severity using spike protein sequences and clinical data, highlighting the utility of AI in genomic surveillance and precision public health. Despite limitations, this approach provides a framework for early severity prediction in future outbreaks.",Caio Cheohen; Vinnícius M. S. Gomes; Manuela L. da Silva,,2025-05-29T16:20:54Z,http://arxiv.org/abs/2505.23879v1
2506.01456v1,GenDMR: A dynamic multimodal role-swapping network for identifying risk   gene phenotypes,"Recent studies have shown that integrating multimodal data fusion techniques for imaging and genetic features is beneficial for the etiological analysis and predictive diagnosis of Alzheimer's disease (AD). However, there are several critical flaws in current deep learning methods. Firstly, there has been insufficient discussion and exploration regarding the selection and encoding of genetic information. Secondly, due to the significantly superior classification value of AD imaging features compared to genetic features, many studies in multimodal fusion emphasize the strengths of imaging features, actively mitigating the influence of weaker features, thereby diminishing the learning of the unique value of genetic features. To address this issue, this study proposes the dynamic multimodal role-swapping network (GenDMR). In GenDMR, we develop a novel approach to encode the spatial organization of single nucleotide polymorphisms (SNPs), enhancing the representation of their genomic context. Additionally, to adaptively quantify the disease risk of SNPs and brain region, we propose a multi-instance attention module to enhance model interpretability. Furthermore, we introduce a dominant modality selection module and a contrastive self-distillation module, combining them to achieve a dynamic teacher-student role exchange mechanism based on dominant and auxiliary modalities for bidirectional co-updating of different modal data. Finally, GenDMR achieves state-of-the-art performance on the ADNI public dataset and visualizes attention to different SNPs, focusing on confirming 12 potential high-risk genes related to AD, including the most classic APOE and recently highlighted significant risk genes. This demonstrates GenDMR's interpretable analytical capability in exploring AD genetic features, providing new insights and perspectives for the development of multimodal data fusion techniques.",Lina Qin; Cheng Zhu; Chuqi Zhou; Yukun Huang; Jiayi Zhu; Ping Liang; Jinju Wang; Yixing Huang; Cheng Luo; Dezhong Yao; Ying Tan,,2025-06-02T09:12:53Z,http://arxiv.org/abs/2506.01456v1
2507.20440v1,BioNeuralNet: A Graph Neural Network based Multi-Omics Network Data   Analysis Tool,"Multi-omics data offer unprecedented insights into complex biological systems, yet their high dimensionality, sparsity, and intricate interactions pose significant analytical challenges. Network-based approaches have advanced multi-omics research by effectively capturing biologically relevant relationships among molecular entities. While these methods are powerful for representing molecular interactions, there remains a need for tools specifically designed to effectively utilize these network representations across diverse downstream analyses. To fulfill this need, we introduce BioNeuralNet, a flexible and modular Python framework tailored for end-to-end network-based multi-omics data analysis. BioNeuralNet leverages Graph Neural Networks (GNNs) to learn biologically meaningful low-dimensional representations from multi-omics networks, converting these complex molecular networks into versatile embeddings. BioNeuralNet supports all major stages of multi-omics network analysis, including several network construction techniques, generation of low-dimensional representations, and a broad range of downstream analytical tasks. Its extensive utilities, including diverse GNN architectures, and compatibility with established Python packages (e.g., scikit-learn, PyTorch, NetworkX), enhance usability and facilitate quick adoption. BioNeuralNet is an open-source, user-friendly, and extensively documented framework designed to support flexible and reproducible multi-omics network analysis in precision medicine.",Vicente Ramos; Sundous Hussein; Mohamed Abdel-Hafiz; Arunangshu Sarkar; Weixuan Liu; Katerina J. Kechris; Russell P. Bowler; Leslie Lange; Farnoush Banaei-Kashani,"Department of Computer Science and Engineering, University of Colorado Denver, Denver, USA; Department of Computer Science and Engineering, University of Colorado Denver, Denver, USA; Department of Computer Science and Engineering, University of Colorado Denver, Denver, USA; Department of Biostatistics and Informatics, University of Colorado Anschutz Medical Campus, Aurora, USA; Department of Biostatistics and Informatics, University of Colorado Anschutz Medical Campus, Aurora, USA; Department of Biostatistics and Informatics, University of Colorado Anschutz Medical Campus, Aurora, USA; Genomic Medicine Institute, Cleveland Clinic, Cleveland, USA; Division of Biomedical Informatics and Personalized Medicine, University of Colorado Anschutz Medical Campus, Aurora, USA; Department of Computer Science and Engineering, University of Colorado Denver, Denver, USA",2025-07-27T23:21:04Z,http://arxiv.org/abs/2507.20440v1
2508.03433v1,When is String Reconstruction using de Bruijn Graphs Hard?,"The reduction of the fragment assembly problem to (variations of) the classical Eulerian trail problem [Pevzner et al., PNAS 2001] has led to remarkable progress in genome assembly. This reduction employs the notion of de Bruijn graph $G=(V,E)$ of order $k$ over an alphabet $\Sigma$. A single Eulerian trail in $G$ represents a candidate genome reconstruction. Bernardini et al. have also introduced the complementary idea in data privacy [ALENEX 2020] based on $z$-anonymity.   The pressing question is: How hard is it to reconstruct a best string from a de Bruijn graph given a function that models domain knowledge? Such a function maps every length-$k$ string to an interval of positions where it may occur in the reconstructed string. By the above reduction to de Bruijn graphs, the latter function translates into a function $c$ mapping every edge to an interval where it may occur in an Eulerian trail. This gives rise to the following basic problem on graphs: Given an instance $(G,c)$, can we efficiently compute an Eulerian trail respecting $c$? Hannenhalli et al.~[CABIOS 1996] formalized this problem and showed that it is NP-complete.   We focus on parametrization aiming to capture the quality of our domain knowledge in the complexity. Ben-Dor et al. developed an algorithm to solve the problem on de Bruijn graphs in $O(m \cdot w^{1.5} 4^{w})$ time, where $m=|E|$ and $w$ is the maximum interval length over all edges. Bumpus and Meeks [Algorithmica 2023] rediscovered the same algorithm on temporal graphs, highlighting the relevance of this problem in other contexts. We give combinatorial insights that lead to exponential-time improvements over the state-of-the-art. For the important class of de Bruijn graphs, we develop an algorithm parametrized by $w (\log w+1) /(k-1)$. Our improved algorithm shows that it is enough when the range of positions is small relative to $k$.",Ben Bals; Sebastiaan van Krieken; Solon P. Pissis; Leen Stougie; Hilde Verbeek,,2025-08-05T13:22:56Z,http://arxiv.org/abs/2508.03433v1
2501.02401v1,iTARGET: Interpretable Tailored Age Regression for Grouped Epigenetic   Traits,"Accurately predicting chronological age from DNA methylation patterns is crucial for advancing biological age estimation. However, this task is made challenging by Epigenetic Correlation Drift (ECD) and Heterogeneity Among CpGs (HAC), which reflect the dynamic relationship between methylation and age across different life stages. To address these issues, we propose a novel two-phase algorithm. The first phase employs similarity searching to cluster methylation profiles by age group, while the second phase uses Explainable Boosting Machines (EBM) for precise, group-specific prediction. Our method not only improves prediction accuracy but also reveals key age-related CpG sites, detects age-specific changes in aging rates, and identifies pairwise interactions between CpG sites. Experimental results show that our approach outperforms traditional epigenetic clocks and machine learning models, offering a more accurate and interpretable solution for biological age estimation with significant implications for aging research.",Zipeng Wu; Daniel Herring; Fabian Spill; James Andrews,,2025-01-04T23:06:46Z,http://arxiv.org/abs/2501.02401v1
2501.02403v1,A generalized distance covariance framework for genome-wide association   studies,"When testing for the association of a single SNP with a phenotypic response, one usually considers an additive genetic model, assuming that the mean of of the response for the heterozygous state is the average of the means for the two homozygous states. However, this simplification often does not hold. In this paper, we present a novel framework for testing the association of a single SNP and a phenotype. Different from the predominant standard approach, our methodology is guaranteed to detect all dependencies expressed by classical genetic association models. The asymptotic distribution under mild regularity assumptions is derived. Moreover, the finite sample distribution under Gaussianity is provided in which the exact p-value can be efficiently evaluated via the classical Appell hypergeometric series. Both results are extended to a regression-type setting with nuisance covariates, enabling hypotheses testing in a wide range of scenarios. A connection of our approach to score tests is explored, leading to intuitive interpretations as locally most powerful tests. A simulation study demonstrates the computational efficiency and excellent statistical performance of the proposed methodology. A real data example is provided.",Dominic Edelmann; Fernando Castro-Prado; Jelle J. Goeman,,2025-01-04T23:12:24Z,http://arxiv.org/abs/2501.02403v1
2501.02778v1,ICFNet: Integrated Cross-modal Fusion Network for Survival Prediction,"Survival prediction is a crucial task in the medical field and is essential for optimizing treatment options and resource allocation. However, current methods often rely on limited data modalities, resulting in suboptimal performance. In this paper, we propose an Integrated Cross-modal Fusion Network (ICFNet) that integrates histopathology whole slide images, genomic expression profiles, patient demographics, and treatment protocols. Specifically, three types of encoders, a residual orthogonal decomposition module and a unification fusion module are employed to merge multi-modal features to enhance prediction accuracy. Additionally, a balanced negative log-likelihood loss function is designed to ensure fair training across different patients. Extensive experiments demonstrate that our ICFNet outperforms state-of-the-art algorithms on five public TCGA datasets, including BLCA, BRCA, GBMLGG, LUAD, and UCEC, and shows its potential to support clinical decision-making and advance precision medicine. The codes are available at: https://github.com/binging512/ICFNet.",Binyu Zhang; Zhu Meng; Junhao Dong; Fei Su; Zhicheng Zhao,,2025-01-06T05:49:08Z,http://arxiv.org/abs/2501.02778v1
2501.03530v1,The permuted score test for robust differential expression analysis,"Negative binomial (NB) regression is a popular method for identifying differentially expressed genes in genomics data, such as bulk and single-cell RNA sequencing data. However, NB regression makes stringent parametric and asymptotic assumptions, which can fail to hold in practice, leading to excess false positive and false negative results. We propose the permuted score test, a new strategy for robust regression based on permuting score test statistics. The permuted score test provably controls type-I error across a much broader range of settings than standard NB regression while nevertheless approximately matching standard NB regression with respect to power (when the assumptions of standard NB regression obtain) and computational efficiency. We accelerate the permuted score test by leveraging emerging techniques for sequential Monte-Carlo testing and novel algorithms for efficiently computing GLM score tests. We apply the permuted score test to real and simulated RNA sequencing data, finding that it substantially improves upon the error control of existing NB regression implementations, including DESeq2. The permuted score test could enhance the reliability of differential expression analysis across diverse biological contexts.",Timothy Barry; Ziang Niu; Eugene Katsevich; Xihong Lin,,2025-01-07T04:48:59Z,http://arxiv.org/abs/2501.03530v1
2501.05687v1,UniQ: Unified Decoder with Task-specific Queries for Efficient Scene   Graph Generation,"Scene Graph Generation(SGG) is a scene understanding task that aims at identifying object entities and reasoning their relationships within a given image. In contrast to prevailing two-stage methods based on a large object detector (e.g., Faster R-CNN), one-stage methods integrate a fixed-size set of learnable queries to jointly reason relational triplets <subject, predicate, object>. This paradigm demonstrates robust performance with significantly reduced parameters and computational overhead. However, the challenge in one-stage methods stems from the issue of weak entanglement, wherein entities involved in relationships require both coupled features shared within triplets and decoupled visual features. Previous methods either adopt a single decoder for coupled triplet feature modeling or multiple decoders for separate visual feature extraction but fail to consider both. In this paper, we introduce UniQ, a Unified decoder with task-specific Queries architecture, where task-specific queries generate decoupled visual features for subjects, objects, and predicates respectively, and unified decoder enables coupled feature modeling within relational triplets. Experimental results on the Visual Genome dataset demonstrate that UniQ has superior performance to both one-stage and two-stage methods.",Xinyao Liao; Wei Wei; Dangyang Chen; Yuanyuan Fu,,2025-01-10T03:38:16Z,http://arxiv.org/abs/2501.05687v1
2501.08363v1,TopoLa: A Universal Framework to Enhance Cell Representations for   Single-cell and Spatial Omics through Topology-encoded Latent Hyperbolic   Geometry,"Recent advances in cellular research demonstrate that scRNA-seq characterizes cellular heterogeneity, while spatial transcriptomics reveals the spatial distribution of gene expression. Cell representation is the fundamental issue in the two fields. Here, we propose Topology-encoded Latent Hyperbolic Geometry (TopoLa), a computational framework enhancing cell representations by capturing fine-grained intercellular topological relationships. The framework introduces a new metric, TopoLa distance (TLd), which quantifies the geometric distance between cells within latent hyperbolic space, capturing the network's topological structure more effectively. With this framework, the cell representation can be enhanced considerably by performing convolution on its neighboring cells. Performance evaluation across seven biological tasks, including scRNA-seq data clustering and spatial transcriptomics domain identification, shows that TopoLa significantly improves the performance of several state-of-the-art models. These results underscore the generalizability and robustness of TopoLa, establishing it as a valuable tool for advancing both biological discovery and computational methodologies.",Kai Zheng; Shaokai Wang; Yunpei Xu; Qiming Lei; Qichang Zhao; Xiao Liang; Qilong Feng; Yaohang Li; Min Li; Jinhui Xu; Jianxin Wang,,2025-01-14T12:41:01Z,http://arxiv.org/abs/2501.08363v1
2501.08467v1,Simultaneous Estimation of Multiple Treatment Effects from Observational   Studies,"Unmeasured confounding presents a significant challenge in causal inference from observational studies. Classical approaches often rely on collecting proxy variables, such as instrumental variables. However, in applications where the effects of multiple treatments are of simultaneous interest, finding a sufficient number of proxy variables for consistent estimation of treatment effects can be challenging. Various methods in the literature exploit the structure of multiple treatments to address unmeasured confounding. In this paper, we introduce a novel approach to causal inference with multiple treatments, assuming sparsity in the causal effects. Our procedure autonomously selects treatments with non-zero causal effects, thereby providing a sparse causal estimation. Comprehensive evaluations using both simulated and Genome-Wide Association Study (GWAS) datasets demonstrate the effectiveness and robustness of our method compared to alternative approaches.",Xiaochuan Shi; Dehan Kong; Linbo Wang,,2025-01-14T22:25:53Z,http://arxiv.org/abs/2501.08467v1
2501.13583v1,Effect Size-Driven Pathway Meta-Analysis for Gene Expression Data,"The proliferation of omics datasets in public repositories has created unprecedented opportunities for biomedical research but has also posed significant challenges for their integration, particularly due to missing genes and platform-specific discrepancies. Traditional gene expression metaanalysis often focuses on individual genes, leading to data loss and limited biological insights when there are missing genes across different studies. To address these limitations, we propose GSEMA (Gene Set Enrichment Meta-Analysis), a novel methodology that leverages singlesample enrichment scoring to aggregate gene expression data into pathway-level matrices. By applying meta-analysis techniques to enrichment scores, GSEMA preserves the magnitude and directionality of effects, enabling the definition of pathway activity across datasets. Using simulated data and case studies on Systemic Lupus Erythematosus (SLE) and Parkinson's Disease (PD), we demonstrate that GSEMA outperforms other methods in controlling false positive rates while providing meaningful biological interpretations. GSEMA methodology is implemented as an R package available on CRAN repository",Juan Antonio Villatoro-García; Pablo Pedro Jurado-Bascón; Pedro Carmona-Sáez,,2025-01-23T11:45:52Z,http://arxiv.org/abs/2501.13583v1
2501.15941v1,SAPPHIRE: Preconditioned Stochastic Variance Reduction for Faster   Large-Scale Statistical Learning,"Regularized empirical risk minimization (rERM) has become important in data-intensive fields such as genomics and advertising, with stochastic gradient methods typically used to solve the largest problems. However, ill-conditioned objectives and non-smooth regularizers undermine the performance of traditional stochastic gradient methods, leading to slow convergence and significant computational costs. To address these challenges, we propose the $\texttt{SAPPHIRE}$ ($\textbf{S}$ketching-based $\textbf{A}$pproximations for $\textbf{P}$roximal $\textbf{P}$reconditioning and $\textbf{H}$essian $\textbf{I}$nexactness with Variance-$\textbf{RE}$educed Gradients) algorithm, which integrates sketch-based preconditioning to tackle ill-conditioning and uses a scaled proximal mapping to minimize the non-smooth regularizer. This stochastic variance-reduced algorithm achieves condition-number-free linear convergence to the optimum, delivering an efficient and scalable solution for ill-conditioned composite large-scale convex machine learning problems. Extensive experiments on lasso and logistic regression demonstrate that $\texttt{SAPPHIRE}$ often converges $20$ times faster than other common choices such as $\texttt{Catalyst}$, $\texttt{SAGA}$, and $\texttt{SVRG}$. This advantage persists even when the objective is non-convex or the preconditioner is infrequently updated, highlighting its robust and practical effectiveness.",Jingruo Sun; Zachary Frangella; Madeleine Udell,,2025-01-27T10:36:45Z,http://arxiv.org/abs/2501.15941v1
2501.16405v1,DepoRanker: A Web Tool to predict Klebsiella Depolymerases using Machine   Learning,"Background: Phage therapy shows promise for treating antibiotic-resistant Klebsiella infections. Identifying phage depolymerases that target Klebsiella capsular polysaccharides is crucial, as these capsules contribute to biofilm formation and virulence. However, homology-based searches have limitations in novel depolymerase discovery.   Objective: To develop a machine learning model for identifying and ranking potential phage depolymerases targeting Klebsiella.   Methods: We developed DepoRanker, a machine learning algorithm to rank proteins by their likelihood of being depolymerases. The model was experimentally validated on 5 newly characterized proteins and compared to BLAST.   Results: DepoRanker demonstrated superior performance to BLAST in identifying potential depolymerases. Experimental validation confirmed its predictive ability on novel proteins.   Conclusions: DepoRanker provides an accurate and functional tool to expedite depolymerase discovery for phage therapy against Klebsiella. It is available as a webserver and open-source software.   Availability: Webserver: https://deporanker.dcs.warwick.ac.uk/ Source code: https://github.com/wgrgwrght/deporanker",George Wright; Slawomir Michniewski; Eleanor Jameson; Fayyaz ul Amir Afsar Minhas,,2025-01-27T11:48:15Z,http://arxiv.org/abs/2501.16405v1
2501.18170v2,Continually Evolved Multimodal Foundation Models for Cancer Prognosis,"Cancer prognosis is a critical task that involves predicting patient outcomes and survival rates. To enhance prediction accuracy, previous studies have integrated diverse data modalities, such as clinical notes, medical images, and genomic data, leveraging their complementary information. However, existing approaches face two major limitations. First, they struggle to incorporate newly arrived data with varying distributions into training, such as patient records from different hospitals, thus rendering sub-optimal generalizability and limited utility in real-world applications. Second, most multimodal integration methods rely on simplistic concatenation or task-specific pipelines, which fail to capture the complex interdependencies across modalities. To address these, we propose a continually evolving multi-modal foundation model. Extensive experiments on the TCGA dataset demonstrate the effectiveness of our approach, highlighting its potential to advance cancer prognosis by enabling robust and adaptive multimodal integration.",Jie Peng; Shuang Zhou; Longwei Yang; Yiran Song; Mohan Zhang; Kaixiong Zhou; Feng Xie; Mingquan Lin; Rui Zhang; Tianlong Chen,,2025-01-30T06:49:57Z,http://arxiv.org/abs/2501.18170v2
2502.00568v3,Generating crossmodal gene expression from cancer histopathology   improves multimodal AI predictions,"Emerging research has highlighted that artificial intelligence based multimodal fusion of digital pathology and transcriptomic features can improve cancer diagnosis (grading/subtyping) and prognosis (survival risk) prediction. However, such direct fusion for joint decision is impractical in real clinical settings, where histopathology is still the gold standard for diagnosis and transcriptomic tests are rarely requested, at least in the public healthcare system. With our novel diffusion based crossmodal generative AI model PathGen, we show that genomic expressions synthesized from digital histopathology jointly predicts cancer grading and patient survival risk with high accuracy (state-of-the-art performance), certainty (through conformal coverage guarantee) and interpretability (through distributed attention maps). PathGen code is available for open use by the research community through GitHub at https://github.com/Samiran-Dey/PathGen.",Samiran Dey; Christopher R. S. Banerji; Partha Basuchowdhuri; Sanjoy K. Saha; Deepak Parashar; Tapabrata Chakraborti,,2025-02-01T21:28:30Z,http://arxiv.org/abs/2502.00568v3
2502.01178v1,Genetic contribution of advantaged ancestors in the biparental Moran   model -- finite selection,"We study a population of N individuals evolving according to a biparental Moran model with two types, one being advantaged compared to the other. The advantage is conferred by a Mendelian mutation, that reduces the death probability of individuals carrying it. We assume that a proportion a of individuals initially carry this mutation, which therefore eventually gets fixed with high probability. After a long time, we sample a gene uniformly from the population, at a new locus, independent of the locus under selection, and calculate the probability that this gene originated from one of the initially advantaged individuals, when population size is large. Our theorem provides quantitative insights, such as the observation that under strong selection, if only 1% of the individuals are initially advantaged, approximately 19% of the population's genome will originate from them after a long time.",Camille Coron; Yves Le Jan,MIA Paris-Saclay; LMO,2025-02-03T09:11:58Z,http://arxiv.org/abs/2502.01178v1
2502.01309v1,Heterogeneous Image GNN: Graph-Conditioned Diffusion for Image Synthesis,"We introduce a novel method for conditioning diffusion-based image synthesis models with heterogeneous graph data. Existing approaches typically incorporate conditioning variables directly into model architectures, either through cross-attention layers that attend to text latents or image concatenation that spatially restrict generation. However, these methods struggle to handle complex scenarios involving diverse, relational conditioning variables, which are more naturally represented as unstructured graphs. This paper presents Heterogeneous Image Graphs (HIG), a novel representation that models conditioning variables and target images as two interconnected graphs, enabling efficient handling of variable-length conditioning inputs and their relationships. We also propose a magnitude-preserving GNN that integrates the HIG into the existing EDM2 diffusion model using a ControlNet approach. Our approach improves upon the SOTA on a variety of conditioning inputs for the COCO-stuff and Visual Genome datasets, and showcases the ability to condition on graph attributes and relationships represented by edges in the HIG.",Rupert Menneer; Christos Margadji; Sebastian W. Pattinson,,2025-02-03T12:36:14Z,http://arxiv.org/abs/2502.01309v1
2502.03478v1,From In Silico to In Vitro: A Comprehensive Guide to Validating   Bioinformatics Findings,"The integration of bioinformatics predictions and experimental validation plays a pivotal role in advancing biological research, from understanding molecular mechanisms to developing therapeutic strategies. Bioinformatics tools and methods offer powerful means for predicting gene functions, protein interactions, and regulatory networks, but these predictions must be validated through experimental approaches to ensure their biological relevance. This review explores the various methods and technologies used for experimental validation, including gene expression analysis, protein-protein interaction verification, and pathway validation. We also discuss the challenges involved in translating computational predictions to experimental settings and highlight the importance of collaboration between bioinformatics and experimental research. Finally, emerging technologies, such as CRISPR gene editing, next-generation sequencing, and artificial intelligence, are shaping the future of bioinformatics validation and driving more accurate and efficient biological discoveries.",Tianyang Wang; Silin Chen; Yunze Wang; Yichao Zhang; Xinyuan Song; Ziqian Bi; Ming Liu; Qian Niu; Junyu Liu; Pohsun Feng; Xintian Sun; Benji Peng; Charles Zhang; Keyu Chen; Ming Li; Cheng Fei; Lawrence KQ Yan,,2025-01-24T19:15:59Z,http://arxiv.org/abs/2502.03478v1
2502.03811v1,Privacy Risks in Health Big Data: A Systematic Literature Review,"The digitization of health records has greatly improved the efficiency of the healthcare system and promoted the formulation of related research and policies. However, the widespread application of advanced technologies such as electronic health records, genomic data, and wearable devices in the field of health big data has also intensified the collection of personal sensitive data, bringing serious privacy and security issues. Based on a systematic literature review (SLR), this paper comprehensively outlines the key research in the field of health big data security. By analyzing existing research, this paper explores how cutting-edge technologies such as homomorphic encryption, blockchain, federated learning, and artificial immune systems can enhance data security while protecting personal privacy. This paper also points out the current challenges and proposes a future research framework in this key area.",Zhang Si Yuan; Manmeet Mahinderjit Singh,,2025-02-06T06:44:36Z,http://arxiv.org/abs/2502.03811v1
2502.07751v1,CausalGeD: Blending Causality and Diffusion for Spatial Gene Expression   Generation,"The integration of single-cell RNA sequencing (scRNA-seq) and spatial transcriptomics (ST) data is crucial for understanding gene expression in spatial context. Existing methods for such integration have limited performance, with structural similarity often below 60\%, We attribute this limitation to the failure to consider causal relationships between genes. We present CausalGeD, which combines diffusion and autoregressive processes to leverage these relationships. By generalizing the Causal Attention Transformer from image generation to gene expression data, our model captures regulatory mechanisms without predefined relationships. Across 10 tissue datasets, CausalGeD outperformed state-of-the-art baselines by 5- 32\% in key metrics, including Pearson's correlation and structural similarity, advancing both technical and biological insights.",Rabeya Tus Sadia; Md Atik Ahamed; Qiang Cheng,,2025-02-11T18:26:22Z,http://arxiv.org/abs/2502.07751v1
2502.09354v1,Trajectory Inference for Single Cell Omics,"Trajectory inference is used to order single-cell omics data along a path that reflects a continuous transition between cells. This approach is useful for studying processes like cell differentiation, where a stem cell matures into a specialized cell type, or investigating state changes in pathological conditions. In the current article, we provide a general introduction to trajectory inference, explaining the concepts and assumptions underlying the different methods. We then briefly discuss the strengths and weaknesses of different trajectory inference methods. We also describe best practices for using trajectory inference, such as how to validate the results and how to interpret them in the context of biological knowledge. Finally, the article will discuss some of the applications of trajectory inference in single-cell omics research. These applications include studying cell differentiation, development, and disease. We provide examples of how trajectory inference has been used to gain new insights into these processes.",Alexandre Hutton; Jesse G. Meyer,,2025-02-13T14:19:33Z,http://arxiv.org/abs/2502.09354v1
2502.09574v1,Spatial Transcriptomics Iterative Hierarchical Clustering (stIHC): A   Novel Method for Identifying Spatial Gene Co-Expression Modules,"Recent advancements in spatial transcriptomics technologies allow researchers to simultaneously measure RNA expression levels for hundreds to thousands of genes while preserving spatial information within tissues, providing critical insights into spatial gene expression patterns, tissue organization, and gene functionality. However, existing methods for clustering spatially variable genes (SVGs) into co-expression modules often fail to detect rare or unique spatial expression patterns. To address this, we present spatial transcriptomics iterative hierarchical clustering (stIHC), a novel method for clustering SVGs into co-expression modules, representing groups of genes with shared spatial expression patterns. Through three simulations and applications to spatial transcriptomics datasets from technologies such as 10x Visium, 10x Xenium, and Spatial Transcriptomics, stIHC outperforms clustering approaches used by popular SVG detection methods, including SPARK, SPARK-X, MERINGUE, and SpatialDE. Gene Ontology enrichment analysis confirms that genes within each module share consistent biological functions, supporting the functional relevance of spatial co-expression. Robust across technologies with varying gene numbers and spatial resolution, stIHC provides a powerful tool for decoding the spatial organization of gene expression and the functional structure of complex tissues.",Catherine Higgins; Jingyi Jessica Li; Michelle Carey,,2025-02-13T18:31:24Z,http://arxiv.org/abs/2502.09574v1
2502.11333v1,Inverse Flow and Consistency Models,"Inverse generation problems, such as denoising without ground truth observations, is a critical challenge in many scientific inquiries and real-world applications. While recent advances in generative models like diffusion models, conditional flow matching, and consistency models achieved impressive results by casting generation as denoising problems, they cannot be directly used for inverse generation without access to clean data. Here we introduce Inverse Flow (IF), a novel framework that enables using these generative models for inverse generation problems including denoising without ground truth. Inverse Flow can be flexibly applied to nearly any continuous noise distribution and allows complex dependencies. We propose two algorithms for learning Inverse Flows, Inverse Flow Matching (IFM) and Inverse Consistency Model (ICM). Notably, to derive the computationally efficient, simulation-free inverse consistency model objective, we generalized consistency training to any forward diffusion processes or conditional flows, which have applications beyond denoising. We demonstrate the effectiveness of IF on synthetic and real datasets, outperforming prior approaches while enabling noise distributions that previous methods cannot support. Finally, we showcase applications of our techniques to fluorescence microscopy and single-cell genomics data, highlighting IF's utility in scientific problems. Overall, this work expands the applications of powerful generative models to inversion generation problems.",Yuchen Zhang; Jian Zhou,,2025-02-17T01:11:42Z,http://arxiv.org/abs/2502.11333v1
2502.11982v2,Single-Cell Proteomics Using Mass Spectrometry,"Single-cell proteomics (SCP) is transforming our understanding of biological complexity by shifting from bulk proteomics, where signals are averaged over thousands of cells, to the proteome analysis of individual cells. This granular perspective reveals distinct cell states, population heterogeneity, and the underpinnings of disease pathogenesis that bulk approaches may obscure. However, SCP demands exceptional sensitivity, precise cell handling, and robust data processing to overcome the inherent challenges of analyzing picogram-level protein samples without amplification. Recent innovations in sample preparation, separations, data acquisition strategies, and specialized mass spectrometry instrumentation have substantially improved proteome coverage and throughput. Approaches that integrate complementary omics, streamline multi-step sample processing, and automate workflows through microfluidics and specialized platforms promise to further push SCP boundaries. Advances in computational methods, especially for data normalization and imputation, address the pervasive issue of missing values, enabling more reliable downstream biological interpretations. Despite these strides, higher throughput, reproducibility, and consensus best practices remain pressing needs in the field. This mini review summarizes the latest progress in SCP technology and software solutions, highlighting how closer integration of analytical, computational, and experimental strategies will facilitate deeper and broader coverage of single-cell proteomes.",Amanda Momenzadeh; Jesse G. Meyer,,2025-02-17T16:22:55Z,http://arxiv.org/abs/2502.11982v2
2502.13080v2,BOLIMES: Boruta and LIME optiMized fEature Selection for Gene Expression   Classification,"Gene expression classification is a pivotal yet challenging task in bioinformatics, primarily due to the high dimensionality of genomic data and the risk of overfitting. To bridge this gap, we propose BOLIMES, a novel feature selection algorithm designed to enhance gene expression classification by systematically refining the feature subset. Unlike conventional methods that rely solely on statistical ranking or classifier-specific selection, we integrate the robustness of Boruta with the interpretability of LIME, ensuring that only the most relevant and influential genes are retained. BOLIMES first employs Boruta to filter out non-informative genes by comparing each feature against its randomized counterpart, thus preserving valuable information. It then uses LIME to rank the remaining genes based on their local importance to the classifier. Finally, an iterative classification evaluation determines the optimal feature subset by selecting the number of genes that maximizes predictive accuracy. By combining exhaustive feature selection with interpretability-driven refinement, our solution effectively balances dimensionality reduction with high classification performance, offering a powerful solution for high-dimensional gene expression analysis.",Bich-Chung Phan; Thanh Ma; Huu-Hoa Nguyen; Thanh-Nghi Do,,2025-02-18T17:33:41Z,http://arxiv.org/abs/2502.13080v2
2502.13453v1,BISON: Bi-clustering of spatial omics data with feature selection,"The advent of next-generation sequencing-based spatially resolved transcriptomics (SRT) techniques has reshaped genomic studies by enabling high-throughput gene expression profiling while preserving spatial and morphological context. Understanding gene functions and interactions in different spatial domains is crucial, as it can enhance our comprehension of biological mechanisms, such as cancer-immune interactions and cell differentiation in various regions. It is necessary to cluster tissue regions into distinct spatial domains and identify discriminating genes that elucidate the clustering result, referred to as spatial domain-specific discriminating genes (DGs). Existing methods for identifying these genes typically rely on a two-stage approach, which can lead to the phenomenon known as \textit{double-dipping}. To address the challenge, we propose a unified Bayesian latent block model that simultaneously detects a list of DGs contributing to spatial domain identification while clustering these DGs and spatial locations. The efficacy of our proposed method is validated through a series of simulation experiments, and its capability to identify DGs is demonstrated through applications to benchmark SRT datasets.",Bencong Zhu; Alberto Cassese; Marina Vannucci; Michele Guindani; Qiwei Li,,2025-02-19T06:03:03Z,http://arxiv.org/abs/2502.13453v1
2502.13785v2,Helix-mRNA: A Hybrid Foundation Model For Full Sequence mRNA   Therapeutics,"mRNA-based vaccines have become a major focus in the pharmaceutical industry. The coding sequence as well as the Untranslated Regions (UTRs) of an mRNA can strongly influence translation efficiency, stability, degradation, and other factors that collectively determine a vaccine's effectiveness. However, optimizing mRNA sequences for those properties remains a complex challenge. Existing deep learning models often focus solely on coding region optimization, overlooking the UTRs. We present Helix-mRNA, a structured state-space-based and attention hybrid model to address these challenges. In addition to a first pre-training, a second pre-training stage allows us to specialise the model with high-quality data. We employ single nucleotide tokenization of mRNA sequences with codon separation, ensuring prior biological and structural information from the original mRNA sequence is not lost. Our model, Helix-mRNA, outperforms existing methods in analysing both UTRs and coding region properties. It can process sequences 6x longer than current approaches while using only 10% of the parameters of existing foundation models. Its predictive capabilities extend to all mRNA regions. We open-source the model (https://github.com/helicalAI/helical) and model weights (https://huggingface.co/helical-ai/helix-mRNA).",Matthew Wood; Mathieu Klop; Maxime Allard,,2025-02-19T14:51:41Z,http://arxiv.org/abs/2502.13785v2
2502.13991v1,Learning to Discover Regulatory Elements for Gene Expression Prediction,"We consider the problem of predicting gene expressions from DNA sequences. A key challenge of this task is to find the regulatory elements that control gene expressions. Here, we introduce Seq2Exp, a Sequence to Expression network explicitly designed to discover and extract regulatory elements that drive target gene expression, enhancing the accuracy of the gene expression prediction. Our approach captures the causal relationship between epigenomic signals, DNA sequences and their associated regulatory elements. Specifically, we propose to decompose the epigenomic signals and the DNA sequence conditioned on the causal active regulatory elements, and apply an information bottleneck with the Beta distribution to combine their effects while filtering out non-causal components. Our experiments demonstrate that Seq2Exp outperforms existing baselines in gene expression prediction tasks and discovers influential regions compared to commonly used statistical methods for peak detection such as MACS3. The source code is released as part of the AIRS library (https://github.com/divelab/AIRS/).",Xingyu Su; Haiyang Yu; Degui Zhi; Shuiwang Ji,,2025-02-19T03:25:49Z,http://arxiv.org/abs/2502.13991v1
2502.15110v1,Variational phylogenetic inference with products over bipartitions,"Bayesian phylogenetics requires accurate and efficient approximation of posterior distributions over trees. In this work, we develop a variational Bayesian approach for ultrametric phylogenetic trees. We present a novel variational family based on coalescent times of a single-linkage clustering and derive a closed-form density of the resulting distribution over trees. Unlike existing methods for ultrametric trees, our method performs inference over all of tree space, it does not require any Markov chain Monte Carlo subroutines, and our variational family is differentiable. Through experiments on benchmark genomic datasets and an application to SARS-CoV-2, we demonstrate that our method achieves competitive accuracy while requiring significantly fewer gradient evaluations than existing state-of-the-art techniques.",Evan Sidrow; Alexandre Bouchard-Côté; Lloyd T. Elliott,,2025-02-21T00:06:57Z,http://arxiv.org/abs/2502.15110v1
2502.15868v2,Multilevel classification framework for breast cancer cell selection and   its integration with advanced disease models,"Breast cancer cell lines are indispensable tools for unraveling disease mechanisms, enabling drug discovery, and developing personalized treatments, yet their heterogeneity and inconsistent classification pose significant challenges in model selection and data reproducibility. This review aims at providing a comprehensive and user-friendly framework for broadly mapping the features of breast cancer types and commercially available human breast cancer cell lines, defining absolute criteria, i.e. objective features such as origin (e.g., MDA-MB, MCF), histological subtype (ductal, lobular), hormone receptor status (ER/PR/HER2), and genetic mutations (BRCA1, TP53), and relative criteria, which contextualize functional behaviors like metastatic potential, drug sensitivity, and genomic instability. It then examines how the proposed framework could be applied to cell line screening in advanced and emerging disease models. By supporting better informed choices, this work aims to improve experimental design and strengthen the connection between in vitro breast cancer studies and their clinical translation.",Catarina Franco Jones; Diogo Dias; Ana C. Moreira; Gil Gonçalves; Stefano Cinti; Mustafa B. A. Djamgoz; Frederico Castelo Ferreira; Paola Sanjuan-Alberte; Rosalia Moreddu,,2025-02-21T13:31:16Z,http://arxiv.org/abs/2502.15868v2
2502.16118v2,Local False Sign Rate and the Role of Prior Covariance Rank in   Multivariate Empirical Bayes Multiple Testing,"This paper investigates the relationship between the rank of the prior covariance matrix and the local false sign rate (lfsr) in multivariate empirical Bayes multiple testing, specifically within the context of normal mean models. We demonstrate that using low-rank covariance matrices for the prior results in inflated false sign rates, a consequence of rank deficiency. To address this, we propose an adjustment that mitigates this inflation by employing full-rank covariance matrices. Through simulations, we validate the effectiveness of this adjustment in controlling false sign rates, thereby improving the robustness of empirical Bayes methods in high-dimensional settings. Our results show that the rank of the prior covariance matrix directly influences the accuracy of sign estimation and the performance of the lfsr, with significant implications for large-scale hypothesis testing in statistics and genomics.",Dongyue Xie,,2025-02-22T07:00:29Z,http://arxiv.org/abs/2502.16118v2
2502.16732v2,DeepSeek reshaping healthcare in China's tertiary hospitals,"The rapid integration of artificial intelligence (AI) into healthcare is transforming clinical decision-making and hospital operations. DeepSeek has emerged as a leading AI system, widely deployed across China's tertiary hospitals since January 2025. Initially implemented in Shanghai's major medical institutions, it has since expanded nationwide, enhancing diagnostic accuracy, streamlining workflows, and improving patient management. AI-powered pathology, imaging analysis, and clinical decision support systems have demonstrated significant potential in optimizing medical processes and reducing the cognitive burden on healthcare professionals. However, the widespread adoption of AI in healthcare raises critical regulatory and ethical challenges, particularly regarding accountability in AI-assisted diagnosis and the risk of automation bias. The absence of a well-defined liability framework underscores the need for policies that ensure AI functions as an assistive tool rather than an autonomous decision-maker. With continued technological advancements, AI is expected to integrate multimodal data sources, such as genomics and radiomics, paving the way for precision medicine and personalized treatment strategies. The future of AI in healthcare depends on the development of transparent regulatory structures, industry collaboration, and adaptive governance frameworks that balance innovation with responsibility, ensuring equitable and effective AI-driven medical services.",Jishizhan Chen; Qingzeng Zhang,,2025-02-23T22:09:17Z,http://arxiv.org/abs/2502.16732v2
2502.19429v1,scMamba: A Pre-Trained Model for Single-Nucleus RNA Sequencing Analysis   in Neurodegenerative Disorders,"Single-nucleus RNA sequencing (snRNA-seq) has significantly advanced our understanding of the disease etiology of neurodegenerative disorders. However, the low quality of specimens derived from postmortem brain tissues, combined with the high variability caused by disease heterogeneity, makes it challenging to integrate snRNA-seq data from multiple sources for precise analyses. To address these challenges, we present scMamba, a pre-trained model designed to improve the quality and utility of snRNA-seq analysis, with a particular focus on neurodegenerative diseases. Inspired by the recent Mamba model, scMamba introduces a novel architecture that incorporates a linear adapter layer, gene embeddings, and bidirectional Mamba blocks, enabling efficient processing of snRNA-seq data while preserving information from the raw input. Notably, scMamba learns generalizable features of cells and genes through pre-training on snRNA-seq data, without relying on dimension reduction or selection of highly variable genes. We demonstrate that scMamba outperforms benchmark methods in various downstream tasks, including cell type annotation, doublet detection, imputation, and the identification of differentially expressed genes.",Gyutaek Oh; Baekgyu Choi; Seyoung Jin; Inkyung Jung; Jong Chul Ye,,2025-02-12T11:48:22Z,http://arxiv.org/abs/2502.19429v1
2502.20338v3,KeBaB: $k$-mer based breaking for finding long MEMs,Long maximal exact matches (MEMs) are used in many genomics applications such as read classification and sequence alignment. Li's ropebwt3 finds long MEMs quickly because it can often ignore much of its input. In this paper we show that a fast and space efficient $k$-mer filtration step using a Bloom filter speeds up MEM-finders such as ropebwt3 even further by letting them ignore even more. We also show experimentally that our approach can accelerate metagenomic classification without significantly hurting accuracy.,Nathaniel K. Brown; Lore Depuydt; Mohsen Zakeri; Anas Alhadi; Nour Allam; Dove Begleiter; Nithin Bharathi Kabilan Karpagavalli; Suchith Sridhar Khajjayam; Hamza Wahed; Travis Gagie; Ben Langmead,,2025-02-27T18:08:10Z,http://arxiv.org/abs/2502.20338v3
2503.01994v1,Fungal Genetic Variants in Oceanic Environments,"Comparing specific types of organisms as they are found across environmental conditions has helped inform how genes and gene products of these organisms relate to phenotypes and adaptation. In this study, we examine metatranscriptomic data as found for oceanic fungi across different oceanic sampling sites. A specific set of three genes was chosen for evaluation based on conserved orthology, known association with core physiological processes in fungi, and level of abundance within oceanic metatranscriptomic data. We report upon a potential association of genetic variance with environmental conditions of iron, salt and phosphate in oceanic waters based on heatmap visualization and PERMANOVA analysis.",Sade A. Davenport; Scott H. Harrison,"Intelligence Community Postdoctoral Research Fellowship Program and North Carolina Agricultural and Technical State University; Department of Biology, North Carolina Agricultural and Technical State University",2025-03-03T19:11:45Z,http://arxiv.org/abs/2503.01994v1
2503.02726v1,Measurement noise scaling laws for cellular representation learning,"Deep learning scaling laws predict how performance improves with increased model and dataset size. Here we identify measurement noise in data as another performance scaling axis, governed by a distinct logarithmic law. We focus on representation learning models of biological single cell genomic data, where a dominant source of measurement noise is due to molecular undersampling. We introduce an information-theoretic metric for cellular representation model quality, and find that it scales with sampling depth. A single quantitative relationship holds across several model types and across several datasets. We show that the analytical form of this relationship can be derived from a simple Gaussian noise model, which in turn provides an intuitive interpretation for the scaling law. Finally, we show that the same relationship emerges in image classification models with respect to two types of imaging noise, suggesting that measurement noise scaling may be a general phenomenon. Scaling with noise can serve as a guide in generating and curating data for deep learning models, particularly in fields where measurement quality can vary dramatically between datasets.",Gokul Gowri; Peng Yin; Allon M. Klein,,2025-03-04T15:44:59Z,http://arxiv.org/abs/2503.02726v1
2503.05961v2,Model-based bi-clustering using multivariate Poisson-lognormal with   general block-diagonal covariance matrix and its applications,"While several Gaussian mixture models-based biclustering approaches currently exist in the literature for continuous data, approaches to handle discrete data have not been well researched. A multivariate Poisson-lognormal (MPLN) model-based bi-clustering approach that utilizes a block-diagonal covariance structure is introduced to allow for a more flexible structure of the covariance matrix. Two variations of the algorithm are developed where the number of column clusters: 1) are assumed equal across groups or 2) can vary across groups. Variational Gaussian approximation is utilized for parameter estimation, and information criteria are used for model selection. The proposed models are investigated in the context of clustering multivariate count data. Using simulated data the models display strong accuracy and computational efficiency and is applied to breast cancer RNA-sequence data from The Cancer Genome Atlas.",Caitlin Kral; Evan Chance; Ryan Browne; Sanjeena Subedi,,2025-03-07T22:01:07Z,http://arxiv.org/abs/2503.05961v2
2503.06182v1,FORESCENE: FOREcasting human activity via latent SCENE graphs diffusion,"Forecasting human-environment interactions in daily activities is challenging due to the high variability of human behavior. While predicting directly from videos is possible, it is limited by confounding factors like irrelevant objects or background noise that do not contribute to the interaction. A promising alternative is using Scene Graphs (SGs) to track only the relevant elements. However, current methods for forecasting future SGs face significant challenges and often rely on unrealistic assumptions, such as fixed objects over time, limiting their applicability to long-term activities where interacted objects may appear or disappear. In this paper, we introduce FORESCENE, a novel framework for Scene Graph Anticipation (SGA) that predicts both object and relationship evolution over time. FORESCENE encodes observed video segments into a latent representation using a tailored Graph Auto-Encoder and forecasts future SGs using a Latent Diffusion Model (LDM). Our approach enables continuous prediction of interaction dynamics without making assumptions on the graph's content or structure. We evaluate FORESCENE on the Action Genome dataset, where it outperforms existing SGA methods while solving a significantly more complex task.",Antonio Alliegro; Francesca Pistilli; Tatiana Tommasi; Giuseppe Averta,,2025-03-08T11:56:00Z,http://arxiv.org/abs/2503.06182v1
2503.06845v1,Bizard: A Community-Driven Platform for Accelerating and Enhancing   Biomedical Data Visualization,"Bizard is a novel visualization code repository designed to simplify data analysis in biomedical research. It integrates diverse visualization codes, facilitating the selection and customization of optimal visualization methods for specific research needs. The platform offers a user-friendly interface with advanced browsing and filtering mechanisms, comprehensive tutorials, and interactive forums to enhance knowledge exchange and innovation. Bizard's collaborative model encourages continuous refinement and expansion of its functionalities, making it an indispensable tool for advancing biomedical data visualization and analytical methodologies. By leveraging Bizard's resources, researchers can enhance data visualization skills, drive methodological advancements, and improve data interpretation standards, ultimately fostering the development of precision medicine and personalized therapeutic interventions.Bizard can be accessed from http://genaimed.org/Bizard/.",Kexin Li; Hong Yang; Ying Shi; Yujie Peng; Yinying Chai; Kexin Huang; Chunyang Wang; Anqi Lin; Jianfeng Li; Jianming Zeng; Peng Luo; Shixiang Wang,,2025-03-10T02:15:26Z,http://arxiv.org/abs/2503.06845v1
2503.07981v1,Regulatory DNA sequence Design with Reinforcement Learning,"Cis-regulatory elements (CREs), such as promoters and enhancers, are relatively short DNA sequences that directly regulate gene expression. The fitness of CREs, measured by their ability to modulate gene expression, highly depends on the nucleotide sequences, especially specific motifs known as transcription factor binding sites (TFBSs). Designing high-fitness CREs is crucial for therapeutic and bioengineering applications. Current CRE design methods are limited by two major drawbacks: (1) they typically rely on iterative optimization strategies that modify existing sequences and are prone to local optima, and (2) they lack the guidance of biological prior knowledge in sequence optimization. In this paper, we address these limitations by proposing a generative approach that leverages reinforcement learning (RL) to fine-tune a pre-trained autoregressive (AR) model. Our method incorporates data-driven biological priors by deriving computational inference-based rewards that simulate the addition of activator TFBSs and removal of repressor TFBSs, which are then integrated into the RL process. We evaluate our method on promoter design tasks in two yeast media conditions and enhancer design tasks for three human cell types, demonstrating its ability to generate high-fitness CREs while maintaining sequence diversity. The code is available at https://github.com/yangzhao1230/TACO.",Zhao Yang; Bing Su; Chuan Cao; Ji-Rong Wen,,2025-03-11T02:33:33Z,http://arxiv.org/abs/2503.07981v1
2503.10655v1,Language modelling techniques for analysing the impact of human genetic   variation,"Interpreting the effects of variants within the human genome and proteome is essential for analysing disease risk, predicting medication response, and developing personalised health interventions. Due to the intrinsic similarities between the structure of natural languages and genetic sequences, natural language processing techniques have demonstrated great applicability in computational variant effect prediction. In particular, the advent of the Transformer has led to significant advancements in the field. However, Transformer-based models are not without their limitations, and a number of extensions and alternatives have been developed to improve results and enhance computational efficiency. This review explores the use of language models for computational variant effect prediction over the past decade, analysing the main architectures, and identifying key trends and future directions.",Megha Hegde; Jean-Christophe Nebel; Farzana Rahman,,2025-03-07T21:34:17Z,http://arxiv.org/abs/2503.10655v1
2503.13957v1,DIFFVSGG: Diffusion-Driven Online Video Scene Graph Generation,"Top-leading solutions for Video Scene Graph Generation (VSGG) typically adopt an offline pipeline. Though demonstrating promising performance, they remain unable to handle real-time video streams and consume large GPU memory. Moreover, these approaches fall short in temporal reasoning, merely aggregating frame-level predictions over a temporal context. In response, we introduce DIFFVSGG, an online VSGG solution that frames this task as an iterative scene graph update problem. Drawing inspiration from Latent Diffusion Models (LDMs) which generate images via denoising a latent feature embedding, we unify the decoding of object classification, bounding box regression, and graph generation three tasks using one shared feature embedding. Then, given an embedding containing unified features of object pairs, we conduct a step-wise Denoising on it within LDMs, so as to deliver a clean embedding which clearly indicates the relationships between objects. This embedding then serves as the input to task-specific heads for object classification, scene graph generation, etc. DIFFVSGG further facilitates continuous temporal reasoning, where predictions for subsequent frames leverage results of past frames as the conditional inputs of LDMs, to guide the reverse diffusion process for current frames. Extensive experiments on three setups of Action Genome demonstrate the superiority of DIFFVSGG.",Mu Chen; Liulei Li; Wenguan Wang; Yi Yang,,2025-03-18T06:49:51Z,http://arxiv.org/abs/2503.13957v1
2503.16687v2,biniLasso: Automated cut-point detection via sparse cumulative   binarization,"We present biniLasso and its sparse variant (sparse biniLasso), novel methods for prognostic analysis of high-dimensional survival data that enable detection of multiple cut-points per feature. Our approach leverages the Cox proportional hazards model with two key innovations: (1) a cumulative binarization scheme with $L_1$-penalized coefficients operating on context-dependent cut-point candidates, and (2) for sparse biniLasso, additional uniLasso regularization to enforce sparsity while preserving univariate coefficient patterns. These innovations yield substantially improved interpretability, computational efficiency (4-11x faster than existing approaches), and prediction performance. Through extensive simulations, we demonstrate superior performance in cut-point detection, particularly in high-dimensional settings. Application to three genomic cancer datasets from TCGA confirms the methods' practical utility, with both variants showing enhanced risk prediction accuracy compared to conventional techniques. The biniLasso framework thus provides a powerful, flexible tool for survival analysis in biomedical research.",Abdollah Safari; Hamed Halisaz; Peter Loewen,,2025-03-20T20:07:00Z,http://arxiv.org/abs/2503.16687v2
2503.17418v1,Application of Single-cell Deep Learning in Elucidating the Mapping   Relationship Between Visceral and Body Surface Inflammatory Patterns,"As a system of integrated homeostasis, life is susceptible to disruptions by visceral inflammation, which can disturb internal environment equilibrium. The role of body-spread subcutaneous fascia (scFascia) in this process is poorly understood. In the rat model of Salmonella-induced dysentery, scRNA-seq of scFascia and deep-learning analysis revealed Warburg-like metabolic reprogramming in macrophages (MPs) with reduced citrate cycle activity. Cd34+/Pdgfra+ telocytes (CPTCs) regulated MPs differentiation and proliferation via Wnt/Fgf signal, suggesting a pathological crosstalk pattern in the scFascia, herein termed the fascia-visceral inflammatory crosstalk pattern (FVICP). PySCENIC analysis indicated increased activity transcription factors Fosl1, Nfkb2, and Atf4, modulated by CPTCs signaling to MPs, downregulating aerobic respiration and upregulating cell cycle, DNA replication, and transcription. This study highlights scFascia's role in immunomodulation and metabolic reprogramming during visceral inflammation, underscoring its function in systemic homeostasis.",Haixiang Huang; Bingbing Shen; Zhenwei Zhang; Jianming Yue; Lu Mei; Qiusheng Chen,,2025-03-21T03:53:25Z,http://arxiv.org/abs/2503.17418v1
2503.20179v1,ProtoBERT-LoRA: Parameter-Efficient Prototypical Finetuning for   Immunotherapy Study Identification,"Identifying immune checkpoint inhibitor (ICI) studies in genomic repositories like Gene Expression Omnibus (GEO) is vital for cancer research yet remains challenging due to semantic ambiguity, extreme class imbalance, and limited labeled data in low-resource settings. We present ProtoBERT-LoRA, a hybrid framework that combines PubMedBERT with prototypical networks and Low-Rank Adaptation (LoRA) for efficient fine-tuning. The model enforces class-separable embeddings via episodic prototype training while preserving biomedical domain knowledge. Our dataset was divided as: Training (20 positive, 20 negative), Prototype Set (10 positive, 10 negative), Validation (20 positive, 200 negative), and Test (71 positive, 765 negative). Evaluated on test dataset, ProtoBERT-LoRA achieved F1-score of 0.624 (precision: 0.481, recall: 0.887), outperforming the rule-based system, machine learning baselines and finetuned PubMedBERT. Application to 44,287 unlabeled studies reduced manual review efforts by 82%. Ablation studies confirmed that combining prototypes with LoRA improved performance by 29% over stand-alone LoRA.",Shijia Zhang; Xiyu Ding; Kai Ding; Jacob Zhang; Kevin Galinsky; Mengrui Wang; Ryan P. Mayers; Zheyu Wang; Hadi Kharrazi,,2025-03-26T03:09:11Z,http://arxiv.org/abs/2503.20179v1
2503.21546v1,consexpressionR: an R package for consensus differential gene expression   analysis,"Motivation: Bulk RNA-Seq is a widely used method for studying gene expression across a variety of contexts. The significance of RNA-Seq studies has grown with the advent of high-throughput sequencing technologies. Computational methods have been developed for each stage of the identification of differentially expressed genes. Nevertheless, there are few studies exploring the association between different types of methods. In this study, we evaluated the impact of the association of methodologies in the results of differential expression analysis. By adopting two data sets with qPCR data (to gold-standard reference), seven methods were implemented and assessed in R packages (EBSeq, edgeR, DESeq2, limma, SAMseq, NOISeq, and Knowseq), which was performed and assessed separately and in association. The results were evaluated considering the adopted qPCR data. Results: Here, we introduce consexpressionR, an R package that automates differential expression analysis using consensus of at least seven methodologies, producing more assertive results with a significant reduction in false positives. Availability: consexpressionR is an R package available via source code and support are available at GitHub (https://github.com/costasilvati/consexpressionR).",Juliana Costa-Silva; David Menotti; Fabricio M. Lopes,,2025-03-27T14:35:17Z,http://arxiv.org/abs/2503.21546v1
2503.24021v1,IntelliCircos: A Data-driven and AI-powered Authoring Tool for Circos   Plots,"Genomics data is essential in biological and medical domains, and bioinformatics analysts often manually create circos plots to analyze the data and extract valuable insights. However, creating circos plots is complex, as it requires careful design for multiple track attributes and positional relationships between them. Typically, analysts often seek inspiration from existing circos plots, and they have to iteratively adjust and refine the plot to achieve a satisfactory final design, making the process both tedious and time-intensive. To address these challenges, we propose IntelliCircos, an AI-powered interactive authoring tool that streamlines the process from initial visual design to the final implementation of circos plots. Specifically, we build a new dataset containing 4396 circos plots with corresponding annotations and configurations, which are extracted and labeled from published papers. With the dataset, we further identify track combination patterns, and utilize Large Language Model (LLM) to provide domain-specific design recommendations and configuration references to navigate the design of circos plots. We conduct a user study with 8 bioinformatics analysts to evaluate IntelliCircos, and the results demonstrate its usability and effectiveness in authoring circos plots.",Mingyang Gu; Jiamin Zhu; Qipeng Wang; Fengjie Wang; Xiaolin Wen; Yong Wang; Min Zhu,,2025-03-31T12:48:39Z,http://arxiv.org/abs/2503.24021v1
2504.01520v1,Time-to-event prediction for grouped variables using Exclusive Lasso,"The integration of high-dimensional genomic data and clinical data into time-to-event prediction models has gained significant attention due to the growing availability of these datasets. Traditionally, a Cox regression model is employed, concatenating various covariate types linearly. Given that much of the data may be redundant or irrelevant, feature selection through penalization is often desirable. A notable characteristic of these datasets is their organization into blocks of distinct data types, such as methylation and clinical predictors, which requires selecting a subset of covariates from each group due to high intra-group correlations. For this reason, we propose utilizing Exclusive Lasso regularization in place of standard Lasso penalization. We apply our methodology to a real-life cancer dataset, demonstrating enhanced survival prediction performance compared to the conventional Cox regression model.",Dayasri Ravi; Andreas Groll,,2025-04-02T09:07:05Z,http://arxiv.org/abs/2504.01520v1
2504.03976v2,OLAF: An Open Life Science Analysis Framework for Conversational   Bioinformatics Powered by Large Language Models,"OLAF (Open Life Science Analysis Framework) is an open-source platform that enables researchers to perform bioinformatics analyses using natural language. By combining large language models (LLMs) with a modular agent-pipe-router architecture, OLAF generates and executes bioinformatics code on real scientific data, including formats like .h5ad. The system includes an Angular front end and a Python/Firebase backend, allowing users to run analyses such as single-cell RNA-seq workflows, gene annotation, and data visualization through a simple web interface. Unlike general-purpose AI tools, OLAF integrates code execution, data handling, and scientific libraries in a reproducible, user-friendly environment. It is designed to lower the barrier to computational biology for non-programmers and support transparent, AI-powered life science research.",Dylan Riffle; Nima Shirooni; Cody He; Manush Murali; Sovit Nayak; Rishikumar Gopalan; Diego Gonzalez Lopez,,2025-04-04T22:41:16Z,http://arxiv.org/abs/2504.03976v2
2504.06282v1,ProHap Explorer: Visualizing Haplotypes in Proteogenomic Datasets,"In mass spectrometry-based proteomics, experts usually project data onto a single set of reference sequences, overlooking the influence of common haplotypes (combinations of genetic variants inherited together from a parent). We recently introduced ProHap, a tool for generating customized protein haplotype databases. Here, we present ProHap Explorer, a visualization interface designed to investigate the influence of common haplotypes on the human proteome. It enables users to explore haplotypes, their effects on protein sequences, and the identification of non-canonical peptides in public mass spectrometry datasets. The design builds on well-established representations in biological sequence analysis, ensuring familiarity for domain experts while integrating novel interactive elements tailored to proteogenomic data exploration. User interviews with proteomics experts confirmed the tool's utility, highlighting its ability to reveal whether haplotypes affect proteins of interest. By facilitating the intuitive exploration of proteogenomic variation, ProHap Explorer supports research in personalized medicine and the development of targeted therapies.",Jakub Vašíček; Dafni Skiadopoulou; Ksenia G. Kuznetsova; Lukas Käll; Marc Vaudel; Stefan Bruckner,,2025-03-25T14:48:20Z,http://arxiv.org/abs/2504.06282v1
2504.06408v1,Parallel GPU-Enabled Algorithms for SpGEMM on Arbitrary Semirings with   Hybrid Communication,"Sparse General Matrix Multiply (SpGEMM) is key for various High-Performance Computing (HPC) applications such as genomics and graph analytics. Using the semiring abstraction, many algorithms can be formulated as SpGEMM, allowing redefinition of addition, multiplication, and numeric types. Today large input matrices require distributed memory parallelism to avoid disk I/O, and modern HPC machines with GPUs can greatly accelerate linear algebra computation. In this paper, we implement a GPU-based distributed-memory SpGEMM routine on top of the CombBLAS library. Our implementation achieves a speedup of over 2x compared to the CPU-only CombBLAS implementation and up to 3x compared to PETSc for large input matrices. Furthermore, we note that communication between processes can be optimized by either direct host-to-host or device-to-device communication, depending on the message size. To exploit this, we introduce a hybrid communication scheme that dynamically switches data paths depending on the message size, thus improving runtimes in communication-bound scenarios.",Thomas McFarland; Julian Bellavita; Giulia Guidi,,2025-04-08T20:04:32Z,http://arxiv.org/abs/2504.06408v1
2504.07384v1,Convergence-divergence models: Generalizations of phylogenetic trees   modeling gene flow over time,"Phylogenetic trees are simple models of evolutionary processes. They describe conditionally independent divergent evolution of taxa from common ancestors. Phylogenetic trees commonly do not have enough flexibility to adequately model all evolutionary processes. For example, introgressive hybridization, where genes can flow from one taxon to another. Phylogenetic networks model evolution not fully described by a phylogenetic tree. However, many phylogenetic network models assume ancestral taxa merge instantaneously to form ``hybrid'' descendant taxa. In contrast, our convergence-divergence models retain a single underlying ``principal'' tree, but permit gene flow over arbitrary time frames. Alternatively, convergence-divergence models can describe other biological processes leading to taxa becoming more similar over a time frame, such as replicated evolution. Here we present novel maximum likelihood-based algorithms to infer most aspects of $N$-taxon convergence-divergence models, many consistently, using a quartet-based approach. The algorithms can be applied to multiple sequence alignments restricted to genes or genomic windows or to gene presence/absence datasets.",Jonathan D. Mitchell; Barbara R. Holland,,2025-04-10T02:08:34Z,http://arxiv.org/abs/2504.07384v1
2504.12251v2,An Evaluation of N-Gram Selection Strategies for Regular Expression   Indexing in Contemporary Text Analysis Tasks. Extended Version,"Efficient evaluation of regular expressions (regex, for short) is crucial for text analysis, and n-gram indexes are fundamental to achieving fast regex evaluation performance. However, these indexes face scalability challenges because of the exponential number of possible n-grams that must be indexed. Many existing selection strategies, developed decades ago, have not been rigorously evaluated on contemporary large-scale workloads and lack comprehensive performance comparisons. Therefore, a unified and comprehensive evaluation framework is necessary to compare these methods under the same experimental settings. This paper presents the first systematic evaluation of three representative n-gram selection strategies across five workloads, including real-time production logs and genomic sequence analysis. We examine their trade-offs in terms of index construction time, storage overhead, false positive rates, and end-to-end query performance. Through empirical results, this study provides a modern perspective on existing n-gram based regular expression evaluation methods, extensive observations, valuable discoveries, and an adaptable testing framework to guide future research in this domain. We make our implementations of these methods and our test framework available as open-source at https://github.com/mush-zhang/RegexIndexComparison.",Ling Zhang; Shaleen Deep; Jignesh M. Patel; Karthikeyan Sankaralingam,,2025-04-16T16:58:26Z,http://arxiv.org/abs/2504.12251v2
2504.16956v1,Bidirectional Mamba for Single-Cell Data: Efficient Context Learning   with Biological Fidelity,"Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.",Cong Qi; Hanzhang Fang; Tianxing Hu; Siqi Jiang; Wei Zhi,,2025-04-22T20:34:47Z,http://arxiv.org/abs/2504.16956v1
2504.18076v1,3plex Web: An Interactive Platform for RNA:DNA Triplex Prediction and   Analysis,"Summary: Long non-coding RNAs (lncRNAs) exert their functions by cooperating with other molecules including proteins and DNA. Triplexes, formed through the interaction between a single-stranded RNA (ssRNA) and a double-stranded DNA (dsDNA), have been consistently described as a mechanism that allows lncRNAs to target specific genomic sequences in vivo. Building on the computational tool 3plex, we developed 3plex Web, an accessible platform that enhances RNA:DNA triplex prediction by integrating interactive visualization, statistical evaluation, and user-friendly downstream analysis workflows. 3plex Web implements new features such as input randomization for statistical assessments, interactive profile plotting for triplex stability, and customizable DNA Binding Domain (DBD) selection. This platform enables rapid analysis through PATO, substantially reducing processing times compared to previous methods, while offering Snakemake workflows to integrate gene expression data and explore lncRNA regulatory mechanisms.   Availability and implementation: 3plex Web is freely available at https://3plex.unito.it as an online web service. The source code for 3plex is available at https://github.com/molinerisLab/3plex, paired with a definition file to set up the application into a Singularity image.   Contact: ivan.molineris@unito.it   Keywords: DNA; RNA; RNA-DNA interaction; triplex; long non-coding RNA; lncRNA; gene regulation; web application",Marco Masera; Chiara Cicconetti; Francesca Ferrero; Salvatore Oliviero; Ivan Molineris,,2025-04-25T05:08:57Z,http://arxiv.org/abs/2504.18076v1
2504.19123v1,Fast and memory-efficient BWT construction of repetitive texts using   Lyndon grammars,"The Burrows-Wheeler Transform (BWT) serves as the basis for many important sequence indexes. On very large datasets (e.g. genomic databases), classical BWT construction algorithms are often infeasible because they usually need to have the entire dataset in main memory. Fortunately, such large datasets are often highly repetitive. It can thus be beneficial to compute the BWT from a compressed representation. We propose an algorithm for computing the BWT via the Lyndon straight-line program, a grammar based on the standard factorization of Lyndon words. Our algorithm can also be used to compute the extended BWT (eBWT) of a multiset of sequences. We empirically evaluate our implementation and find that we can compute the BWT and eBWT of very large datasets faster and/or with less memory than competing methods.",Jannik Olbrich,,2025-04-27T06:38:05Z,http://arxiv.org/abs/2504.19123v1
2504.21116v1,Beyond level-1: Identifiability of a class of galled tree-child networks,"Inference of phylogenetic networks is of increasing interest in the genomic era. However, the extent to which phylogenetic networks are identifiable from various types of data remains poorly understood, despite its crucial role in justifying methods. This work obtains strong identifiability results for large sub-classes of galled tree-child semidirected networks. Some of the conditions our proofs require, such as the identifiability of a network's tree of blobs or the circular order of 4 taxa around a cycle in a level-1 network, are already known to hold for many data types. We show that all these conditions hold for quartet concordance factor data under various gene tree models, yielding the strongest results from 2 or more samples per taxon. Although the network classes we consider have topological restrictions, they include non-planar networks of any level and are substantially more general than level-1 networks -- the only class previously known to enjoy identifiability from many data types. Our work establishes a route for proving future identifiability results for tree-child galled networks from data types other than quartet concordance factors, by checking that explicit conditions are met.",Elizabeth S. Allman; Cecile Ane; Hector Banos; John A. Rhodes,,2025-04-29T18:55:47Z,http://arxiv.org/abs/2504.21116v1
2505.00650v1,OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery   and Survival Stratification,"Unsupervised learning of disease subtypes from multi-omics data presents a significant opportunity for advancing personalized medicine. We introduce OmicsCL, a modular contrastive learning framework that jointly embeds heterogeneous omics modalities-such as gene expression, DNA methylation, and miRNA expression-into a unified latent space. Our method incorporates a survival-aware contrastive loss that encourages the model to learn representations aligned with survival-related patterns, without relying on labeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers clinically meaningful clusters and achieves strong unsupervised concordance with patient survival. The framework demonstrates robustness across hyperparameter configurations and can be tuned to prioritize either subtype coherence or survival stratification. Ablation studies confirm that integrating survival-aware loss significantly enhances the predictive power of learned embeddings. These results highlight the promise of contrastive objectives for biological insight discovery in high-dimensional, heterogeneous omics data.",Atahan Karagoz,,2025-05-01T16:51:48Z,http://arxiv.org/abs/2505.00650v1
2505.00784v2,Reconfigurable legged metamachines that run on autonomous modular legs,"Legged machines are becoming increasingly agile and adaptive but they have so far lacked the morphological diversity of legged animals, which have been rearranged and reshaped to fill millions of niches. Unlike their biological counterparts, legged machines have largely converged over the past decade to canonical quadrupedal and bipedal architectures that cannot be easily reconfigured to meet new tasks or recover from injury. Here we introduce autonomous modular legs: agile yet minimal, single-degree-of-freedom jointed links that can learn complex dynamic behaviors and may be freely attached to form legged metamachines at the meter scale. This enables rapid repair, redesign, and recombination of highly-dynamic modular agents that move quickly and acrobatically (non-quasistatically) through unstructured environments. Because each module is itself a complete agent, legged metamachines are able to sustain deep structural damage that would completely disable other legged robots. We also show how to encode the vast space of possible body configurations into a compact latent design genome that can be efficiently explored, revealing a wide diversity of novel legged forms.",Chen Yu; David Matthews; Jingxian Wang; Jing Gu; Douglas Blackiston; Michael Rubenstein; Sam Kriegman,,2025-05-01T18:28:09Z,http://arxiv.org/abs/2505.00784v2
2505.00934v1,Bayesian Forensic DNA Mixture Deconvolution Using a Novel String   Similarity Measure,"Mixture interpretation is a central challenge in forensic science, where evidence often contains contributions from multiple sources. In the context of DNA analysis, biological samples recovered from crime scenes may include genetic material from several individuals, necessitating robust statistical tools to assess whether a specific person of interest (POI) is among the contributors. Methods based on capillary electrophoresis (CE) are currently in use worldwide, but offer limited resolution in complex mixtures. Advancements in massively parallel sequencing (MPS) technologies provide a richer, more detailed representation of DNA mixtures, but require new analytical strategies to fully leverage this information. In this work, we present a Bayesian framework for evaluating whether a POIs DNA is present in an MPS-based forensic sample. The model accommodates known contributors, such as the victim, and uses a novel string edit distance to quantify similarity between observed alleles and sequencing artifacts. The resulting Bayes factors enable effective discrimination between samples that do and do not contain the POIs DNA, demonstrating strong performance in both hypothesis testing and classification settings.",Taylor Petty; Jan Hannig; Hari Iyer,,2025-05-02T00:33:37Z,http://arxiv.org/abs/2505.00934v1
2505.05749v1,A Mechanism-Guided Inverse Engineering Framework to Unlock Design   Principles of H-Bonded Organic Frameworks for Gas Separation,"The diverse combinations of novel building blocks offer a vast design space for hydrogen-boned frameworks (HOFs), rendering it a great promise for gas separation and purification. However, the underlying separation mechanism facilitated by their unique hydrogen-bond networks has not yet been fully understood. In this work, a comprehensive understanding of the separation mechanisms was achieved through an iterative data-driven inverse engineering approach established upon a hypothetical HOF database possessing nearly 110,000 structures created by a material genomics method. Leveraging a simple yet universal feature extracted from hydrogen bonding information with unambiguous physical meanings, the entire design space was exploited to rapidly identify the optimization route towards novel HOF structures with superior Xe/Kr separation performance (selectivity >103). This work not only provides the first large-scale HOF database, but also demonstrates the enhanced machine learning interpretability of our model-driven iterative inverse design framework, offering new insights into the rational design of nanoporous materials for gas separation.",Yong Qiu; Lei Wang; Letian Chen; Yun Tian; Zhen Zhou; Jianzhong Wu,,2025-05-09T03:11:11Z,http://arxiv.org/abs/2505.05749v1
2505.06945v2,A systematic review of challenges and proposed solutions in modeling   multimodal data,"Multimodal data modeling has emerged as a powerful approach in clinical research, enabling the integration of diverse data types such as imaging, genomics, wearable sensors, and electronic health records. Despite its potential to improve diagnostic accuracy and support personalized care, modeling such heterogeneous data presents significant technical challenges. This systematic review synthesizes findings from 69 studies to identify common obstacles, including missing modalities, limited sample sizes, dimensionality imbalance, interpretability issues, and finding the optimal fusion techniques. We highlight recent methodological advances, such as transfer learning, generative models, attention mechanisms, and neural architecture search that offer promising solutions. By mapping current trends and innovations, this review provides a comprehensive overview of the field and offers practical insights to guide future research and development in multimodal modeling for medical applications.",Maryam Farhadizadeh; Maria Weymann; Michael Blaß; Johann Kraus; Christopher Gundler; Sebastian Walter; Noah Hempen; Harald Binder; Nadine Binder,,2025-05-11T11:23:51Z,http://arxiv.org/abs/2505.06945v2
2505.07896v1,Bridging Large Language Models and Single-Cell Transcriptomics in   Dissecting Selective Motor Neuron Vulnerability,"Understanding cell identity and function through single-cell level sequencing data remains a key challenge in computational biology. We present a novel framework that leverages gene-specific textual annotations from the NCBI Gene database to generate biologically contextualized cell embeddings. For each cell in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by expression level, retrieve their NCBI Gene descriptions, and transform these descriptions into vector embedding representations using large language models (LLMs). The models used include OpenAI text-embedding-ada-002, text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as domain-specific models BioBERT and SciBERT. Embeddings are computed via an expression-weighted average across the top N most highly expressed genes in each cell, providing a compact, semantically rich representation. This multimodal strategy bridges structured biological data with state-of-the-art language modeling, enabling more interpretable downstream applications such as cell-type clustering, cell vulnerability dissection, and trajectory inference.",Douglas Jiang; Zilin Dai; Luxuan Zhang; Qiyi Yu; Haoqi Sun; Feng Tian,,2025-05-12T03:39:33Z,http://arxiv.org/abs/2505.07896v1
2505.08341v1,Benchmarking AI scientists in omics data-driven biological research,"The rise of large language models and multi-agent systems has sparked growing interest in AI scientists capable of autonomous biological research. However, existing benchmarks either focus on reasoning without data or on data analysis with predefined statistical answers, lacking realistic, data-driven evaluation settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench), a benchmark designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge. BaisBench comprises two tasks: cell type annotation on 31 expert-labeled single-cell datasets, and scientific discovery through answering 198 multiple-choice questions derived from the biological insights of 41 recent single-cell studies. Systematic experiments on state-of-the-art AI scientists and LLM agents showed that while promising, current models still substantially underperform human experts on both tasks. We hope BaisBench will fill this gap and serve as a foundation for advancing and evaluating AI models for scientific discovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.",Erpai Luo; Jinmeng Jia; Yifan Xiong; Xiangyu Li; Xiaobo Guo; Baoqi Yu; Lei Wei; Xuegong Zhang,,2025-05-13T08:33:54Z,http://arxiv.org/abs/2505.08341v1
2505.09848v1,Radiogenomic Bipartite Graph Representation Learning for Alzheimer's   Disease Detection,"Imaging and genomic data offer distinct and rich features, and their integration can unveil new insights into the complex landscape of diseases. In this study, we present a novel approach utilizing radiogenomic data including structural MRI images and gene expression data, for Alzheimer's disease detection. Our framework introduces a novel heterogeneous bipartite graph representation learning featuring two distinct node types: genes and images. The network can effectively classify Alzheimer's disease (AD) into three distinct stages:AD, Mild Cognitive Impairment (MCI), and Cognitive Normal (CN) classes, utilizing a small dataset. Additionally, it identified which genes play a significant role in each of these classification groups. We evaluate the performance of our approach using metrics including classification accuracy, recall, precision, and F1 score. The proposed technique holds potential for extending to radiogenomic-based classification to other diseases.",Aditya Raj; Golrokh Mirzaei,,2025-05-14T23:13:35Z,http://arxiv.org/abs/2505.09848v1
2505.11384v1,MOSAIK: Multi-Origin Spatial Transcriptomics Analysis and Integration   Kit,"Spatial transcriptomics (ST) has revolutionised transcriptomics analysis by preserving tissue architecture, allowing researchers to study gene expression in its native spatial context. However, despite its potential, ST still faces significant technical challenges. Two major issues include: (1) the integration of raw data into coherent and reproducible analysis workflows, and (2) the accurate assignment of transcripts to individual cells. To address these challenges, we present MOSAIK, the first fully integrated, end-to-end workflow that supports raw data from both NanoString CosMx Spatial Molecular Imager (CosMx) and 10x Genomics Xenium In Situ (Xenium). MOSAIK (Multi-Origin Spatial Transcriptomics Analysis and Integration Kit) unifies transcriptomics and imaging data into a single Python object based on the spatialdata format. This unified structure ensures compatibility with a broad range of Python tools, enabling robust quality control and downstream analyses. With MOSAIK, users can perform advanced analyses such as re-segmentation (to more accurately assign transcripts to individual cells), cell typing, tissue domain identification, and cell-cell communication within a seamless and reproducible Python environment.",Anthony Baptista; Rosamond Nuamah; Ciro Chiappini; Anita Grigoriadis,,2025-05-16T15:46:55Z,http://arxiv.org/abs/2505.11384v1
2505.13197v2,Inferring stochastic dynamics with growth from cross-sectional data,"Time-resolved single-cell omics data offers high-throughput, genome-wide measurements of cellular states, which are instrumental to reverse-engineer the processes underpinning cell fate. Such technologies are inherently destructive, allowing only cross-sectional measurements of the underlying stochastic dynamical system. Furthermore, cells may divide or die in addition to changing their molecular state. Collectively these present a major challenge to inferring realistic biophysical models. We present a novel approach, \emph{unbalanced} probability flow inference, that addresses this challenge for biological processes modelled as stochastic dynamics with growth. By leveraging a Lagrangian formulation of the Fokker-Planck equation, our method accurately disentangles drift from intrinsic noise and growth. We showcase the applicability of our approach through evaluation on a range of simulated and real single-cell RNA-seq datasets. Comparing to several existing methods, we find our method achieves higher accuracy while enjoying a simple two-step training scheme.",Stephen Zhang; Suryanarayana Maddu; Xiaojie Qiu; Victor Chardès,,2025-05-19T14:51:47Z,http://arxiv.org/abs/2505.13197v2
2505.13216v1,Spatio-temporal patterns of active epigenetic turnover,"DNA methylation is a primary layer of epigenetic modification that plays a pivotal role in the regulation of development, aging, and cancer. The concurrent activity of opposing enzymes that mediate DNA methylation and demethylation gives rise to a biochemical cycle and active turnover of DNA methylation. While the ensuing biochemical oscillations have been implicated in the regulation of cell differentiation, their functional role and spatio-temporal dynamics are, however, unknown. In this work, we demonstrate that chromatin-mediated coupling between these local biochemical cycles can lead to the emergence of phase-locked domains, regions of locally synchronized turnover activity, whose coarsening is arrested by genomic heterogeneity. We introduce a minimal model based on stochastic oscillators with constrained long-range and non-reciprocal interactions, shaped by the local chromatin organization. Through a combination of analytical theory and stochastic simulations, we predict both the degree of synchronization and the typical size of emergent phase-locked domains. We qualitatively test these predictions using single-cell sequencing data. Our results show that DNA methylation turnover exhibits surprisingly rich spatio-temporal patterns which may be used by cells to control cell differentiation.",Fabrizio Olmeda; Misha Gupta; Onurcan Bektas; Steffen Rulands,,2025-05-19T15:00:26Z,http://arxiv.org/abs/2505.13216v1
2505.13583v2,Physiological and Transcriptional Responses of Arabidopsis thaliana to   Simulated Lunar and Martian Regolith Substrates,"The integration of plant-based bioregenerative life support systems is a central objective in NASA's Moon to Mars strategy. Arabidopsis thaliana, a model organism with extensive genomic resources, serves as a key species to investigate plant resilience in extraterrestrial environments. We assessed the physiological and gene expression responses of A. thaliana (Col-0) grown in two off-world regolith simulants: LHS-2 (lunar highlands) and MGS-1 (Martian global). Plants exposed to these substrates exhibited significant reductions in root elongation, biomass, and chlorophyll content, along with elevated anthocyanin levels and transcriptional upregulation of stress-related genes including IRT1, PCS1, SOD1, and JAZ1. Evidence of jasmonic acid pathway activation and auxin signaling suppression suggests metal-induced hormonal misregulation. Our integrated analysis of morphological traits, pigment accumulation, and transcriptomic profiles reveals distinct mineral-specific stress responses, offering critical insights into substrate engineering strategies for future space agriculture.",A'nya Buckner; Sarah Lang; Rafael Loureiro,,2025-05-19T16:40:43Z,http://arxiv.org/abs/2505.13583v2
2505.14923v1,Robustness of Boolean networks to update modes: an application to   hereditary angioedema,"Many familial diseases are caused by genetic accidents, which affect both the genome and its epigenetic environment, expressed as an interaction graph between the genes as that involved in one familial disease we shall study, the hereditary angioedema. The update of the gene states at the vertices of this graph (1 if a gene is activated, 0 if it is inhibited) can be done in multiple ways, well studied over the last two decades: parallel, sequential, block-sequential, block-parallel, random, etc. We will study a particular graph, related to the familial disease proposed as an example, which has subgraphs which activate in an intricate manner (\emph{i.e.}, in an alternating block-parallel mode, with one core constantly updated and two complementary subsets of genes alternating their updating), of which we will study the structural aspects, robust or unstable, in relation to some classical periodic update modes.",Jacques Demongeot; Eric Goles; Houssem ben Khalfallah; Marco Montalva-Medel; Sylvain Sené,,2025-05-20T21:15:50Z,http://arxiv.org/abs/2505.14923v1
2505.16982v1,Beyond Correlation: Towards Causal Large Language Model Agents in   Biomedicine,"Large Language Models (LLMs) show promise in biomedicine but lack true causal understanding, relying instead on correlations. This paper envisions causal LLM agents that integrate multimodal data (text, images, genomics, etc.) and perform intervention-based reasoning to infer cause-and-effect. Addressing this requires overcoming key challenges: designing safe, controllable agentic frameworks; developing rigorous benchmarks for causal evaluation; integrating heterogeneous data sources; and synergistically combining LLMs with structured knowledge (KGs) and formal causal inference tools. Such agents could unlock transformative opportunities, including accelerating drug discovery through automated hypothesis generation and simulation, enabling personalized medicine through patient-specific causal models. This research agenda aims to foster interdisciplinary efforts, bridging causal concepts and foundation models to develop reliable AI partners for biomedical progress.",Adib Bazgir; Amir Habibdoust Lafmajani; Yuwen Zhang,,2025-05-22T17:52:59Z,http://arxiv.org/abs/2505.16982v1
2505.21117v2,ReassembleNet: Learnable Keypoints and Diffusion for 2D Fresco   Reconstruction,"The task of reassembly is a significant challenge across multiple domains, including archaeology, genomics, and molecular docking, requiring the precise placement and orientation of elements to reconstruct an original structure. In this work, we address key limitations in state-of-the-art Deep Learning methods for reassembly, namely i) scalability; ii) multimodality; and iii) real-world applicability: beyond square or simple geometric shapes, realistic and complex erosion, or other real-world problems. We propose ReassembleNet, a method that reduces complexity by representing each input piece as a set of contour keypoints and learning to select the most informative ones by Graph Neural Networks pooling inspired techniques. ReassembleNet effectively lowers computational complexity while enabling the integration of features from multiple modalities, including both geometric and texture data. Further enhanced through pretraining on a semi-synthetic dataset. We then apply diffusion-based pose estimation to recover the original structure. We improve on prior methods by 55% and 86% for RMSE Rotation and Translation, respectively.",Adeela Islam; Stefano Fiorini; Stuart James; Pietro Morerio; Alessio Del Bue,,2025-05-27T12:38:06Z,http://arxiv.org/abs/2505.21117v2
2505.21441v1,Autoencoding Random Forests,"We propose a principled method for autoencoding with random forests. Our strategy builds on foundational results from nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data. We provide exact and approximate solutions to the decoding problem via constrained optimization, split relabeling, and nearest neighbors regression. These methods effectively invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees. The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. We demonstrate various applications of this autoencoder, including powerful new tools for visualization, compression, clustering, and denoising. Experiments illustrate the ease and utility of our method in a wide range of settings, including tabular, image, and genomic data.",Binh Duc Vu; Jan Kapar; Marvin Wright; David S. Watson,,2025-05-27T17:15:02Z,http://arxiv.org/abs/2505.21441v1
2505.24759v2,Unsupervised Evolutionary Cell Type Matching via Entropy-Minimized   Optimal Transport,"Identifying evolutionary correspondences between cell types across species is a fundamental challenge in comparative genomics and evolutionary biology. Existing approaches often rely on either reference-based matching, which imposes asymmetry by designating one species as the reference, or projection-based matching, which may increase computational complexity and obscure biological interpretability at the cell-type level. Here, we present OT-MESH, an unsupervised computational framework leveraging entropy-regularized optimal transport (OT) to systematically determine cross-species cell type homologies. Our method uniquely integrates the Minimize Entropy of Sinkhorn (MESH) technique to refine the OT plan, transforming diffuse transport matrices into sparse, interpretable correspondences. Through systematic evaluation on synthetic datasets, we demonstrate that OT-MESH achieves near-optimal matching accuracy with computational efficiency, while maintaining remarkable robustness to noise. Compared to other OT-based methods like RefCM, OT-MESH provides speedup while achieving comparable accuracy. Applied to retinal bipolar cells (BCs) and retinal ganglion cells (RGCs) from mouse and macaque, OT-MESH accurately recovers known evolutionary relationships and uncovers novel correspondences, one of which was independently validated experimentally. Thus, our framework offers a principled, scalable, and interpretable solution for evolutionary cell type mapping, facilitating deeper insights into cellular specialization and conservation across species.",Mu Qiao,,2025-05-30T16:20:00Z,http://arxiv.org/abs/2505.24759v2
2506.00410v1,JojoSCL: Shrinkage Contrastive Learning for single-cell RNA sequence   Clustering,"Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of cellular processes by enabling gene expression analysis at the individual cell level. Clustering allows for the identification of cell types and the further discovery of intrinsic patterns in single-cell data. However, the high dimensionality and sparsity of scRNA-seq data continue to challenge existing clustering models. In this paper, we introduce JojoSCL, a novel self-supervised contrastive learning framework for scRNA-seq clustering. By incorporating a shrinkage estimator based on hierarchical Bayesian estimation, which adjusts gene expression estimates towards more reliable cluster centroids to reduce intra-cluster dispersion, and optimized using Stein's Unbiased Risk Estimate (SURE), JojoSCL refines both instance-level and cluster-level contrastive learning. Experiments on ten scRNA-seq datasets substantiate that JojoSCL consistently outperforms prevalent clustering methods, with further validation of its practicality through robustness analysis and ablation studies. JojoSCL's code is available at: https://github.com/ziwenwang28/JojoSCL.",Ziwen Wang,,2025-05-31T05:59:56Z,http://arxiv.org/abs/2506.00410v1
2506.01487v2,FDSG: Forecasting Dynamic Scene Graphs,"Dynamic scene graph generation extends scene graph generation from images to videos by modeling entity relationships and their temporal evolution. However, existing methods either generate scene graphs from observed frames without explicitly modeling temporal dynamics, or predict only relationships while assuming static entity labels and locations. These limitations hinder effective extrapolation of both entity and relationship dynamics, restricting video scene understanding. We propose Forecasting Dynamic Scene Graphs (FDSG), a novel framework that predicts future entity labels, bounding boxes, and relationships, for unobserved frames, while also generating scene graphs for observed frames. Our scene graph forecast module leverages query decomposition and neural stochastic differential equations to model entity and relationship dynamics. A temporal aggregation module further refines predictions by integrating forecasted and observed information via cross-attention. To benchmark FDSG, we introduce Scene Graph Forecasting, a new task for full future scene graph prediction. Experiments on Action Genome show that FDSG outperforms state-of-the-art methods on dynamic scene graph generation, scene graph anticipation, and scene graph forecasting. Codes will be released upon publication.",Yi Yang; Yuren Cong; Hao Cheng; Bodo Rosenhahn; Michael Ying Yang,,2025-06-02T09:46:22Z,http://arxiv.org/abs/2506.01487v2
2506.01560v1,SPAC: A Python Package for Spatial Single-Cell Analysis of Multiplex   Imaging,"Multiplexed immunofluorescence microscopy captures detailed measurements of spatially resolved, multiple biomarkers simultaneously, revealing tissue composition and cellular interactions in situ among single cells. The growing scale and dimensional complexity of these datasets demand reproducible, comprehensive and user-friendly computational tools. To address this need, we developed SPAC (SPAtial single-Cell analysis), a Python-based package and a corresponding shiny application within an integrated, modular SPAC ecosystem (Liu et al., 2025) designed specifically for biologists without extensive coding expertise. Following image segmentation and extraction of spatially resolved single-cell data, SPAC streamlines downstream phenotyping and spatial analysis, facilitating characterization of cellular heterogeneity and spatial organization within tissues. Through scalable performance, specialized spatial statistics, highly customizable visualizations, and seamless workflows from dataset to insights, SPAC significantly lowers barriers to sophisticated spatial analyses.",Fang Liu; Rui He; Andrei Bombin; Ahmad B. Abdallah; Omar Eldaghar; Tommy R. Sheeley; Sam E. Ying; George Zaki,,2025-06-02T11:36:32Z,http://arxiv.org/abs/2506.01560v1
2506.01883v1,scDataset: Scalable Data Loading for Deep Learning on Large-Scale   Single-Cell Omics,"Modern single-cell datasets now comprise hundreds of millions of cells, presenting significant challenges for training deep learning models that require shuffled, memory-efficient data loading. While the AnnData format is the community standard for storing single-cell datasets, existing data loading solutions for AnnData are often inadequate: some require loading all data into memory, others convert to dense formats that increase storage demands, and many are hampered by slow random disk access. We present scDataset, a PyTorch IterableDataset that operates directly on one or more AnnData files without the need for format conversion. The core innovation is a combination of block sampling and batched fetching, which together balance randomness and I/O efficiency. On the Tahoe 100M dataset, scDataset achieves up to a 48$\times$ speed-up over AnnLoader, a 27$\times$ speed-up over HuggingFace Datasets, and an 18$\times$ speed-up over BioNeMo in single-core settings. These advances democratize large-scale single-cell model training for the broader research community.",Davide D'Ascenzo; Sebastiano Cultrera di Montesano,,2025-06-02T17:11:49Z,http://arxiv.org/abs/2506.01883v1
2506.03294v2,Prefix-free parsing for merging big BWTs,"When building Burrows-Wheeler Transforms (BWTs) of truly huge datasets, prefix-free parsing (PFP) can use an unreasonable amount of memory. In this paper we show how if a dataset can be broken down into small datasets that are not very similar to each other -- such as collections of many copies of genomes of each of several species, or collections of many copies of each of the human chromosomes -- then we can drastically reduce PFP's memory footprint by building the BWTs of the small datasets and then merging them into the BWT of the whole dataset.",Diego Diaz-Dominguez; Travis Gagie; Veronica Guerrini; Ben Langmead; Zsuzsanna Liptak; Giovanni Manzini; Francesco Masillo; Vikram Shivakumar,,2025-06-03T18:34:01Z,http://arxiv.org/abs/2506.03294v2
2506.04060v1,Complex rheology of condensin in entangled DNA,"Structural-Maintenance-of-Chromosome (SMC) complexes such as condensins are well-known to dictate the folding and entanglement of interphase and mitotic chromosomes. However, their role in modulating the rheology and viscoelasticity of entangled DNA is not fully understood. In this work, we discover that physiological concentrations of yeast condensin increase both the effective viscosity and elasticity of dense solutions of $\lambda$-DNA even in absence of ATP. By combining biochemical assays and single-molecule imaging, we discover that yeast condensin can proficiently bind double-stranded DNA through its hinge domain, in addition to its heads. We further discover that presence of ATP fluidifies the entangled solution possibly by activating loop extrusion. Finally, we show that the observed rheology can be understood by modelling SMCs as transient crosslinkers in bottle-brush-like entangled polymers. Our findings help us to understand how SMCs affect the rheology and dynamics of the genome.",Filippo Conforto; Antonio Valdes; Willem Vanderlinden; Davide Michieletto,,2025-06-04T15:24:55Z,http://arxiv.org/abs/2506.04060v1
2506.05361v1,Scalable Generation of Spatial Transcriptomics from Histology Images via   Whole-Slide Flow Matching,"Spatial transcriptomics (ST) has emerged as a powerful technology for bridging histology imaging with gene expression profiling. However, its application has been limited by low throughput and the need for specialized experimental facilities. Prior works sought to predict ST from whole-slide histology images to accelerate this process, but they suffer from two major limitations. First, they do not explicitly model cell-cell interaction as they factorize the joint distribution of whole-slide ST data and predict the gene expression of each spot independently. Second, their encoders struggle with memory constraints due to the large number of spots (often exceeding 10,000) in typical ST datasets. Herein, we propose STFlow, a flow matching generative model that considers cell-cell interaction by modeling the joint distribution of gene expression of an entire slide. It also employs an efficient slide-level encoder with local spatial attention, enabling whole-slide processing without excessive memory overhead. On the recently curated HEST-1k and STImage-1K4M benchmarks, STFlow substantially outperforms state-of-the-art baselines and achieves over 18% relative improvements over the pathology foundation models.",Tinglin Huang; Tianyu Liu; Mehrtash Babadi; Wengong Jin; Rex Ying,,2025-05-25T01:29:19Z,http://arxiv.org/abs/2506.05361v1
2506.05651v1,"Hallucinate, Ground, Repeat: A Framework for Generalized Visual   Relationship Detection","Understanding relationships between objects is central to visual intelligence, with applications in embodied AI, assistive systems, and scene understanding. Yet, most visual relationship detection (VRD) models rely on a fixed predicate set, limiting their generalization to novel interactions. A key challenge is the inability to visually ground semantically plausible, but unannotated, relationships hypothesized from external knowledge. This work introduces an iterative visual grounding framework that leverages large language models (LLMs) as structured relational priors. Inspired by expectation-maximization (EM), our method alternates between generating candidate scene graphs from detected objects using an LLM (expectation) and training a visual model to align these hypotheses with perceptual evidence (maximization). This process bootstraps relational understanding beyond annotated data and enables generalization to unseen predicates. Additionally, we introduce a new benchmark for open-world VRD on Visual Genome with 21 held-out predicates and evaluate under three settings: seen, unseen, and mixed. Our model outperforms LLM-only, few-shot, and debiased baselines, achieving mean recall (mR@50) of 15.9, 13.1, and 11.7 on predicate classification on these three sets. These results highlight the promise of grounded LLM priors for scalable open-world visual understanding.",Shanmukha Vellamcheti; Sanjoy Kundu; Sathyanarayanan N. Aakur,,2025-06-06T00:43:15Z,http://arxiv.org/abs/2506.05651v1
2506.06233v1,"Bayesian variable selection in a Cox proportional hazards model with the   ""Sum of Single Effects"" prior","Motivated by genetic fine-mapping applications, we introduce a new approach to Bayesian variable selection regression (BVSR) for time-to-event (TTE) outcomes. This new approach is designed to deal with the specific challenges that arise in genetic fine-mapping, including: the presence of very strong correlations among the covariates, often exceeding 0.99; very large data sets containing potentially thousands of covariates and hundreds of thousands of samples. We accomplish this by extending the ""Sum of Single Effects"" (SuSiE) method to the Cox proportional hazards (CoxPH) model. We demonstrate the benefits of the new method, ""CoxPH-SuSiE"", over existing BVSR methods for TTE outcomes in simulated fine-mapping data sets. We also illustrate CoxPH-SuSiE on real data by fine-mapping asthma loci using data from UK Biobank. This fine-mapping identified 14 asthma risk SNPs in 8 asthma risk loci, among which 6 had strong evidence for being causal (posterior inclusion probability greater than 50%). Two of the 6 putatively causal variants are known to be pathogenic, and others lie within a genomic sequence that is known to regulate the expression of GATA3.",Yunqi Yang; Karl Tayeb; Peter Carbonetto; Xiaoyuan Zhong; Carole Ober; Matthew Stephens,,2025-06-06T16:53:16Z,http://arxiv.org/abs/2506.06233v1
2506.07790v1,Heavy Lasso: sparse penalized regression under heavy-tailed noise via   data-augmented soft-thresholding,"High-dimensional linear regression is a fundamental tool in modern statistics, particularly when the number of predictors exceeds the sample size. The classical Lasso, which relies on the squared loss, performs well under Gaussian noise assumptions but often deteriorates in the presence of heavy-tailed errors or outliers commonly encountered in real data applications such as genomics, finance, and signal processing. To address these challenges, we propose a novel robust regression method, termed Heavy Lasso, which incorporates a loss function inspired by the Student's t-distribution within a Lasso penalization framework. This loss retains the desirable quadratic behavior for small residuals while adaptively downweighting large deviations, thus enhancing robustness to heavy-tailed noise and outliers. Heavy Lasso enjoys computationally efficient by leveraging a data augmentation scheme and a soft-thresholding algorithm, which integrate seamlessly with classical Lasso solvers. Theoretically, we establish non-asymptotic bounds under both $\ell_1$ and $\ell_2 $ norms, by employing the framework of localized convexity, showing that the Heavy Lasso estimator achieves rates comparable to those of the Huber loss. Extensive numerical studies demonstrate Heavy Lasso's superior performance over classical Lasso and other robust variants, highlighting its effectiveness in challenging noisy settings. Our method is implemented in the R package heavylasso available on Github.",The Tien Mai,,2025-06-09T14:13:02Z,http://arxiv.org/abs/2506.07790v1
2506.10031v1,scSSL-Bench: Benchmarking Self-Supervised Learning for Single-Cell Data,"Self-supervised learning (SSL) has proven to be a powerful approach for extracting biologically meaningful representations from single-cell data. To advance our understanding of SSL methods applied to single-cell data, we present scSSL-Bench, a comprehensive benchmark that evaluates nineteen SSL methods. Our evaluation spans nine datasets and focuses on three common downstream tasks: batch correction, cell type annotation, and missing modality prediction. Furthermore, we systematically assess various data augmentation strategies. Our analysis reveals task-specific trade-offs: the specialized single-cell frameworks, scVI, CLAIRE, and the finetuned scGPT excel at uni-modal batch correction, while generic SSL methods, such as VICReg and SimCLR, demonstrate superior performance in cell typing and multi-modal data integration. Random masking emerges as the most effective augmentation technique across all tasks, surpassing domain-specific augmentations. Notably, our results indicate the need for a specialized single-cell multi-modal data integration framework. scSSL-Bench provides a standardized evaluation platform and concrete recommendations for applying SSL to single-cell analysis, advancing the convergence of deep learning and single-cell genomics.",Olga Ovcharenko; Florian Barkmann; Philip Toma; Imant Daunhawer; Julia Vogt; Sebastian Schelter; Valentina Boeva,,2025-06-10T12:31:42Z,http://arxiv.org/abs/2506.10031v1
2506.10370v1,Estimating Signal-to-Noise Ratios for Multivariate High-dimensional   Linear Models,"Signal-to-noise ratios (SNR) play a crucial role in various statistical models, with important applications in tasks such as estimating heritability in genomics. The method-of-moments estimator is a widely used approach for estimating SNR, primarily explored in single-response settings. In this study, we extend the method-of-moments SNR estimation framework to encompass both fixed effects and random effects linear models with multivariate responses. In particular, we establish and compare the asymptotic distributions of the proposed estimators. Furthermore, we extend our approach to accommodate cases with residual heteroskedasticity and derive asymptotic inference procedures based on standard error estimation. The effectiveness of our methods is demonstrated through extensive numerical experiments.",Xiaohan Hu; Zhentao Li; Xiaodong Li,,2025-06-12T05:46:40Z,http://arxiv.org/abs/2506.10370v1
2506.11158v1,Brain-wide interpolation and conditioning of gene expression in the   human brain using Implicit Neural Representations,"In this paper, we study the efficacy and utility of recent advances in non-local, non-linear image interpolation and extrapolation algorithms, specifically, ideas based on Implicit Neural Representations (INR), as a tool for analysis of spatial transcriptomics data. We seek to utilize the microarray gene expression data sparsely sampled in the healthy human brain, and produce fully resolved spatial maps of any given gene across the whole brain at a voxel-level resolution. To do so, we first obtained the 100 top AD risk genes, whose baseline spatial transcriptional profiles were obtained from the Allen Human Brain Atlas (AHBA). We adapted Implicit Neural Representation models so that the pipeline can produce robust voxel-resolution quantitative maps of all genes. We present a variety of experiments using interpolations obtained from Abagen as a baseline/reference.",Xizheng Yu; Justin Torok; Sneha Pandya; Sourav Pal; Vikas Singh; Ashish Raj,,2025-06-11T17:03:13Z,http://arxiv.org/abs/2506.11158v1
2506.15671v2,Quantum-inspired algorithm for simulating viral response,"Understanding the properties of biological systems is an exciting avenue for applying advanced approaches to solving corresponding computational tasks. A specific class of problems that arises in the resolution of biological challenges is optimization. In this work, we present the results of a proof-of-concept study that applies a quantum-inspired optimization algorithm to simulate a viral response. We formulate an Ising-type model to describe the patterns of gene activity in host responses. Reducing the problem to the Ising form allows the use of available quantum and quantum-inspired optimization tools. We demonstrate the application of a quantum-inspired optimization algorithm to this problem. Our study paves the way for exploring the full potential of quantum and quantum-inspired optimization tools in biological applications.",Daria O. Konina; Dmitry I. Korbashov; Ilya V. Kovalchuk; Aygul A. Nizamieva; Dmitry A. Chermoshentsev; Aleksey K. Fedorov,,2025-06-18T17:51:21Z,http://arxiv.org/abs/2506.15671v2
2506.15696v1,CoC: Chain-of-Cancer based on Cross-Modal Autoregressive Traction for   Survival Prediction,"Survival prediction aims to evaluate the risk level of cancer patients. Existing methods primarily rely on pathology and genomics data, either individually or in combination. From the perspective of cancer pathogenesis, epigenetic changes, such as methylation data, could also be crucial for this task. Furthermore, no previous endeavors have utilized textual descriptions to guide the prediction. To this end, we are the first to explore the use of four modalities, including three clinical modalities and language, for conducting survival prediction. In detail, we are motivated by the Chain-of-Thought (CoT) to propose the Chain-of-Cancer (CoC) framework, focusing on intra-learning and inter-learning. We encode the clinical data as the raw features, which remain domain-specific knowledge for intra-learning. In terms of inter-learning, we use language to prompt the raw features and introduce an Autoregressive Mutual Traction module for synergistic representation. This tailored framework facilitates joint learning among multiple modalities. Our approach is evaluated across five public cancer datasets, and extensive experiments validate the effectiveness of our methods and proposed designs, leading to producing \sota results. Codes will be released.",Haipeng Zhou; Sicheng Yang; Sihan Yang; Jing Qin; Lei Chen; Lei Zhu,,2025-05-28T07:11:49Z,http://arxiv.org/abs/2506.15696v1
2506.15761v1,Advancing Digital Precision Medicine for Chronic Fatigue Syndrome   through Longitudinal Large-Scale Multi-Modal Biological Omics Modeling with   Machine Learning and Artificial Intelligence,"We studied a generalized question: chronic diseases like ME/CFS and long COVID exhibit high heterogeneity with multifactorial etiology and progression, complicating diagnosis and treatment. To address this, we developed BioMapAI, an explainable Deep Learning framework using the richest longitudinal multi-omics dataset for ME/CFS to date. This dataset includes gut metagenomics, plasma metabolome, immune profiling, blood labs, and clinical symptoms. By connecting multi-omics to a symptom matrix, BioMapAI identified both disease- and symptom-specific biomarkers, reconstructed symptoms, and achieved state-of-the-art precision in disease classification. We also created the first connectivity map of these omics in both healthy and disease states and revealed how microbiome-immune-metabolome crosstalk shifted from healthy to ME/CFS.",Ruoyun Xiong,,2025-06-18T15:31:26Z,http://arxiv.org/abs/2506.15761v1
2506.16457v1,Scientific Applications Leveraging Randomized Linear Algebra,"This report showcases the role of, and future directions for, the field of Randomized Numerical Linear Algebra (RNLA) in a selection of scientific applications. These applications span the domains of imaging, genomics and time-varying systems, and are thematically connected by needing to perform linear algebra routines on large-scale matrices (with up to quantillions of entries). At such scales, the linear algebra routines face typical bottlenecks: memory constraints, data access latencies, and substantial floating-point operation costs. RNLA routines are discussed at a high level to demonstrate how RNLA is able to solve the challenges faced by traditional linear algebra routines, and, consequently, address the computational problem posed in the underlying application. For each application, RNLA's open challenges and possible future directions are also presented, which broadly fall into the categories: creating structure-aware RNLA algorithms; co-designing RNLA algorithms with hardware and mixed-precision considerations; and advancing modular, composable software infrastructure. Ultimately, this report serves two purposes: it invites domain scientists to engage with RNLA; and it offers a guide for future RNLA research grounded in real applications.",Vivak Patel; D. Adrian Maldonado; Maksim Melnichenko; Nathaniel Pritchard; Vishwas Rao; Elizaveta Rebrova; Sriram Sankararaman,,2025-06-19T16:50:56Z,http://arxiv.org/abs/2506.16457v1
2506.16701v1,Language-driven Description Generation and Common Sense Reasoning for   Video Action Recognition,"Recent video action recognition methods have shown excellent performance by adapting large-scale pre-trained language-image models to the video domain. However, language models contain rich common sense priors - the scene contexts that humans use to constitute an understanding of objects, human-object interactions, and activities - that have not been fully exploited. In this paper, we introduce a framework incorporating language-driven common sense priors to identify cluttered video action sequences from monocular views that are often heavily occluded. We propose: (1) A video context summary component that generates candidate objects, activities, and the interactions between objects and activities; (2) A description generation module that describes the current scene given the context and infers subsequent activities, through auxiliary prompts and common sense reasoning; (3) A multi-modal activity recognition head that combines visual and textual cues to recognize video actions. We demonstrate the effectiveness of our approach on the challenging Action Genome and Charades datasets.",Xiaodan Hu; Chuhang Zou; Suchen Wang; Jaechul Kim; Narendra Ahuja,,2025-06-20T02:43:53Z,http://arxiv.org/abs/2506.16701v1
2506.20016v1,New Insights on Unfolding and Fine-tuning Quantum Federated Learning,"Client heterogeneity poses significant challenges to the performance of Quantum Federated Learning (QFL). To overcome these limitations, we propose a new approach leveraging deep unfolding, which enables clients to autonomously optimize hyperparameters, such as learning rates and regularization factors, based on their specific training behavior. This dynamic adaptation mitigates overfitting and ensures robust optimization in highly heterogeneous environments where standard aggregation methods often fail. Our framework achieves approximately 90% accuracy, significantly outperforming traditional methods, which typically yield around 55% accuracy, as demonstrated through real-time training on IBM quantum hardware and Qiskit Aer simulators. By developing self adaptive fine tuning, the proposed method proves particularly effective in critical applications such as gene expression analysis and cancer detection, enhancing diagnostic precision and predictive modeling within quantum systems. Our results are attributed to convergence-aware, learnable optimization steps intrinsic to the deep unfolded framework, which maintains the generalization. Hence, this study addresses the core limitations of conventional QFL, streamlining its applicability to any complex challenges such as healthcare and genomic research.",Shanika Iroshi Nanayakkara; Shiva Raj Pokhrel,,2025-06-24T21:17:48Z,http://arxiv.org/abs/2506.20016v1
2506.20769v1,inMOTIFin: a lightweight end-to-end simulation software for regulatory   sequences,"The accurate development, assessment, interpretation, and benchmarking of bioinformatics frameworks for analyzing transcriptional regulatory grammars rely on controlled simulations to validate the underlying methods. However, existing simulators often lack end-to-end flexibility or ease of integration, which limits their practical use. We present inMOTIFin, a lightweight, modular, and user-friendly Python-based software that addresses these gaps by providing versatile and efficient simulation and modification of DNA regulatory sequences. inMOTIFin enables users to simulate or modify regulatory sequences efficiently for the customizable generation of motifs and insertion of motif instances with precise control over their positions, co-occurrences, and spacing, as well as direct modification of real sequences, facilitating a comprehensive evaluation of motif-based methods and interpretation tools. We demonstrate inMOTIFin applications for the assessment of de novo motif discovery prediction, the analysis of transcription factor cooperativity, and the support of explainability analyses for deep learning models. inMOTIFin ensures robust and reproducible analyses for studying transcriptional regulatory grammars.   inMOTIFin is available at PyPI https://pypi.org/project/inMOTIFin/ and Docker Hub https://hub.docker.com/r/cbgr/inmotifin. Detailed documentation is available at https://inmotifin.readthedocs.io/en/latest/. The code for use case analyses is available at https://bitbucket.org/CBGR/inmotifin_evaluation/src/main/.",Katalin Ferenc; Lorenzo Martini; Ieva Rauluseviciute; Geir Kjetil Sandve; Anthony Mathelier,,2025-06-25T19:03:53Z,http://arxiv.org/abs/2506.20769v1
2506.22641v1,Diversity by Design: Addressing Mode Collapse Improves scRNA-seq   Perturbation Modeling on Well-Calibrated Metrics,"Recent benchmarks reveal that models for single-cell perturbation response are often outperformed by simply predicting the dataset mean. We trace this anomaly to a metric artifact: control-referenced deltas and unweighted error metrics reward mode collapse whenever the control is biased or the biological signal is sparse. Large-scale \textit{in silico} simulations and analysis of two real-world perturbation datasets confirm that shared reference shifts, not genuine biological change, drives high performance in these evaluations. We introduce differentially expressed gene (DEG)-aware metrics, weighted mean-squared error (WMSE) and weighted delta $R^{2}$ ($R^{2}_{w}(\Delta)$) with respect to all perturbations, that measure error in niche signals with high sensitivity. We further introduce negative and positive performance baselines to calibrate these metrics. With these improvements, the mean baseline sinks to null performance while genuine predictors are correctly rewarded. Finally, we show that using WMSE as a loss function reduces mode collapse and improves model performance.",Gabriel M. Mejia; Henry E. Miller; Francis J. A. Leblanc; Bo Wang; Brendan Swain; Lucas Paulo de Lima Camillo,,2025-06-27T21:12:46Z,http://arxiv.org/abs/2506.22641v1
2506.22901v1,Missing-Modality-Aware Graph Neural Network for Cancer Classification,"A key challenge in learning from multimodal biological data is missing modalities, where all data from some modalities are missing for some patients. Current fusion methods address this by excluding patients with missing modalities, imputing missing modalities, or making predictions directly with partial modalities. However, they often struggle with diverse missing-modality patterns and the exponential growth of the number of such patterns as the number of modalities increases. To address these limitations, we propose MAGNET (Missing-modality-Aware Graph neural NETwork) for direct prediction with partial modalities, which introduces a patient-modality multi-head attention mechanism to fuse lower-dimensional modality embeddings based on their importance and missingness. MAGNET's complexity increases linearly with the number of modalities while adapting to missing-pattern variability. To generate predictions, MAGNET further constructs a patient graph with fused multimodal embeddings as node features and the connectivity determined by the modality missingness, followed by a conventional graph neural network. Experiments on three public multiomics datasets for cancer classification, with real-world instead of artificial missingness, show that MAGNET outperforms the state-of-the-art fusion methods. The data and code are available at https://github.com/SinaTabakhi/MAGNET.",Sina Tabakhi; Haiping Lu,,2025-06-28T14:31:00Z,http://arxiv.org/abs/2506.22901v1
2506.24126v1,Controlling the false discovery rate under a non-parametric graphical   dependence model,"We propose sufficient conditions and computationally efficient procedures for false discovery rate control in multiple testing when the $p$-values are related by a known \emph{dependency graph} -- meaning that we assume independence of $p$-values that are not within each other's neighborhoods, but otherwise leave the dependence unspecified. Our methods' rejection sets coincide with that of the Benjamini--Hochberg (BH) procedure whenever there are no edges between BH rejections, and we find in simulations and a genomics data example that their power approaches that of the BH procedure when there are few such edges, as is commonly the case. Because our methods ignore all hypotheses not in the BH rejection set, they are computationally efficient whenever that set is small. Our fastest method, the IndBH procedure, typically finishes within seconds even in simulations with up to one million hypotheses.",Drew T. Nguyen; William Fithian,,2025-06-30T17:59:35Z,http://arxiv.org/abs/2506.24126v1
2507.00072v1,Classifying Hotspots Mutations for Biosimulation with Quantum Neural   Networks and Variational Quantum Eigensolver,"The rapid expansion of biomolecular datasets presents significant challenges for computational biology. Quantum computing emerges as a promising solution to address these complexities. This study introduces a novel quantum framework for analyzing TART-T and TART-C gene data by integrating genomic and structural information. Leveraging a Quantum Neural Network (QNN), we classify hotspot mutations, utilizing quantum superposition to uncover intricate relationships within the data. Additionally, a Variational Quantum Eigensolver (VQE) is employed to estimate molecular ground-state energies through a hybrid classical-quantum approach, overcoming the limitations of traditional computational methods. Implemented using IBM Qiskit, our framework demonstrates high accuracy in both mutation classification and energy estimation on current Noisy Intermediate-Scale Quantum (NISQ) devices. These results underscore the potential of quantum computing to advance the understanding of gene function and protein structure. Furthermore, this research serves as a foundational blueprint for extending quantum computational methods to other genes and biological systems, highlighting their synergy with classical approaches and paving the way for breakthroughs in drug discovery and personalized medicine.",Don Roosan; Rubayat Khan; Saif Nirzhor; Tiffany Khou; Fahmida Hai,,2025-06-29T03:20:37Z,http://arxiv.org/abs/2507.00072v1
2507.00087v1,pUniFind: a unified large pre-trained deep learning model pushing the   limit of mass spectra interpretation,"Deep learning has advanced mass spectrometry data interpretation, yet most models remain feature extractors rather than unified scoring frameworks. We present pUniFind, the first large-scale multimodal pre-trained model in proteomics that integrates end-to-end peptide-spectrum scoring with open, zero-shot de novo sequencing. Trained on over 100 million open search-derived spectra, pUniFind aligns spectral and peptide modalities via cross modality prediction and outperforms traditional engines across diverse datasets, particularly achieving a 42.6 percent increase in the number of identified peptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind identifies 60 percent more PSMs than existing de novo methods despite a 300-fold larger search space. A deep learning based quality control module further recovers 38.5 percent additional peptides including 1,891 mapped to the genome but absent from reference proteomes while preserving full fragment ion coverage. These results establish a unified, scalable deep learning framework for proteomic analysis, offering improved sensitivity, modification coverage, and interpretability.",Jiale Zhao; Pengzhi Mao; Kaifei Wang; Yiming Li; Yaping Peng; Ranfei Chen; Shuqi Lu; Xiaohong Ji; Jiaxiang Ding; Xin Zhang; Yucheng Liao; Weinan E; Weijie Zhang; Han Wen; Hao Chi,,2025-06-30T08:32:39Z,http://arxiv.org/abs/2507.00087v1
2507.00154v1,Five-Gene Expression Formula Accurately Detects Hepatocellular Carcinoma   Tumors,"Hepatocellular carcinoma (HCC) is one of the leading causes of cancer-related deaths worldwide. Several diagnostic methods, such as imaging modalities and Serum Alpha-Fetoprotein (AFP) testing, have been used for HCC detection; however, their effectiveness is limited to later stages of the disease. In contrast, transcriptomic analysis of biposy samples has shown promise for early detection. While machine learning techniques have been applied to transcriptomic data for cancer detection, their clinical adoption remains limited due to challenges such as poor generalizability across different datasets, lack of interpretability, and high computational complexity. To address these limitations, we developed a novel predictive formula for HCC detection using the Kolmogorov-Arnold Network (KAN). This formula is based on the expression levels of five genes: VIPR1, CYP1A2, FCN3, ECM1, and LIFR. Derived from the GSE25097 dataset, the formula offers a simple, interpretable, efficient, and accessible approach for HCC identification. It achieves 99% accuracy on the GSE25097 test set and demonstrates robust performance on six additional independent datasets, achieving accuracies of above 90% in all cases. These findings highlight the critical role of these five genes as biomarkers for HCC detection, offering a foundation for future research and clinical applications to improve HCC diagnostic approaches.",Aram Ansary Ogholbake; Qiang Cheng,,2025-06-27T17:08:24Z,http://arxiv.org/abs/2507.00154v1
2507.01922v2,Efficient stochastic simulation of gene regulatory networks using hybrid   models of transcriptional bursting,"Single-cell data reveal the presence of biological stochasticity between cells of identical genome and environment, in particular highlighting the transcriptional bursting phenomenon. To account for this property, gene expression may be modeled as a continuous-time Markov chain where biochemical species are described in a discrete way, leading to Gillespie's stochastic simulation algorithm (SSA) which turns out to be computationally expensive for realistic mRNA and protein copy numbers. Alternatively, hybrid models based on piecewise-deterministic Markov processes (PDMPs) offer an effective compromise for capturing cell-to-cell variability, but their simulation remains limited to specialized mathematical communities. With a view to making them more accessible, we present here a simple simulation method that is reminiscent of SSA, while allowing for much lower computational cost. We detail the algorithm for a bursty PDMP describing an arbitrary number of interacting genes, and prove that it simulates exact trajectories of the model. As an illustration, we use the algorithm to simulate a two-gene toggle switch: this example highlights the fact that bimodal distributions as observed in real data are not explained by transcriptional bursting per se, but rather by distinct burst frequencies that may emerge from interactions between genes.",Mathilde Gaillard; Ulysse Herbach,,2025-07-02T17:31:38Z,http://arxiv.org/abs/2507.01922v2
2507.02980v1,Modeling Gene Expression Distributional Shifts for Unseen Genetic   Perturbations,"We train a neural network to predict distributional responses in gene expression following genetic perturbations. This is an essential task in early-stage drug discovery, where such responses can offer insights into gene function and inform target identification. Existing methods only predict changes in the mean expression, overlooking stochasticity inherent in single-cell data. In contrast, we offer a more realistic view of cellular responses by modeling expression distributions. Our model predicts gene-level histograms conditioned on perturbations and outperforms baselines in capturing higher-order statistics, such as variance, skewness, and kurtosis, at a fraction of the training cost. To generalize to unseen perturbations, we incorporate prior knowledge via gene embeddings from large language models (LLMs). While modeling a richer output space, the method remains competitive in predicting mean expression changes. This work offers a practical step towards more expressive and biologically informative models of perturbation effects.",Kalyan Ramakrishnan; Jonathan G. Hedley; Sisi Qu; Puneet K. Dokania; Philip H. S. Torr; Cesar A. Prada-Medina; Julien Fauqueur; Kaspar Martens,,2025-07-01T06:04:28Z,http://arxiv.org/abs/2507.02980v1
2507.04125v1,Graph Neural Networks as a Substitute for Transformers in Single-Cell   Transcriptomics,"Graph Neural Networks (GNNs) and Transformers share significant similarities in their encoding strategies for interacting with features from nodes of interest, where Transformers use query-key scores and GNNs use edges. Compared to GNNs, which are unable to encode relative positions, Transformers leverage dynamic attention capabilities to better represent relative relationships, thereby becoming the standard backbones in large-scale sequential pre-training. However, the subtle difference prompts us to consider: if positions are no longer crucial, could we substitute Transformers with Graph Neural Networks in some fields such as Single-Cell Transcriptomics? In this paper, we first explore the similarities and differences between GNNs and Transformers, specifically in terms of relative positions. Additionally, we design a synthetic example to illustrate their equivalence where there are no relative positions between tokens in the sample. Finally, we conduct extensive experiments on a large-scale position-agnostic dataset-single-cell transcriptomics-finding that GNNs achieve competitive performance compared to Transformers while consuming fewer computation resources. These findings provide novel insights for researchers in the field of single-cell transcriptomics, challenging the prevailing notion that the Transformer is always the optimum choice.",Jiaxin Qi; Yan Cui; Jinli Ou; Jianqiang Huang; Gaogang Xie,,2025-07-05T18:37:16Z,http://arxiv.org/abs/2507.04125v1
2507.04981v3,Classification of autoimmune diseases from Peripheral blood TCR   repertoires by multimodal multi-instance learning,"T cell receptor (TCR) repertoires encode critical immunological signatures for autoimmune diseases, yet their clinical application remains limited by sequence sparsity and low witness rates. We developed EAMil, a multi-instance deep learning framework that leverages TCR sequencing data to diagnose systemic lupus erythematosus (SLE) and rheumatoid arthritis (RA) with exceptional accuracy. By integrating PrimeSeq feature extraction with ESMonehot encoding and enhanced gate attention mechanisms, our model achieved state-of-the-art performance with AUCs of 98.95% for SLE and 97.76% for RA. EAMil successfully identified disease-associated genes with over 90% concordance with established differential analyses and effectively distinguished disease-specific TCR genes. The model demonstrated robustness in classifying multiple disease categories, utilizing the SLEDAI score to stratify SLE patients by disease severity as well as to diagnose the site of damage in SLE patients, and effectively controlling for confounding factors such as age and gender. This interpretable framework for immune receptor analysis provides new insights for autoimmune disease detection and classification with broad potential clinical applications across immune-mediated conditions.",Ruihao Zhang; Mao chen; Fei Ye; Dandan Meng; Yixuan Huang; Xiao Liu,,2025-07-07T13:24:41Z,http://arxiv.org/abs/2507.04981v3
2507.05247v1,Multi-Disease Deep Learning Framework for GWAS: Beyond Feature Selection   Constraints,"Traditional GWAS has advanced our understanding of complex diseases but often misses nonlinear genetic interactions. Deep learning offers new opportunities to capture complex genomic patterns, yet existing methods mostly depend on feature selection strategies that either constrain analysis to known pathways or risk data leakage when applied across the full dataset. Further, covariates can inflate predictive performance without reflecting true genetic signals. We explore different deep learning architecture choices for GWAS and demonstrate that careful architectural choices can outperform existing methods under strict no-leakage conditions. Building on this, we extend our approach to a multi-label framework that jointly models five diseases, leveraging shared genetic architecture for improved efficiency and discovery. Applied to five million SNPs across 37,000 samples, our method achieves competitive predictive performance (AUC 0.68-0.96), offering a scalable, leakage-free, and biologically meaningful approach for multi-disease GWAS analysis.",Iqra Farooq; Sara Atito; Ayse Demirkan; Inga Prokopenko; Muhammad Rana,,2025-07-07T17:55:13Z,http://arxiv.org/abs/2507.05247v1
2507.05990v1,Multivariate regression with missing response data for modelling   regional DNA methylation QTLs,"Identifying genetic regulators of DNA methylation (mQTLs) with multivariate models enhances statistical power, but is challenged by missing data from bisulfite sequencing. Standard imputation-based methods can introduce bias, limiting reliable inference. We propose \texttt{missoNet}, a novel convex estimation framework that jointly estimates regression coefficients and the precision matrix from data with missing responses. By using unbiased surrogate estimators, our three-stage procedure avoids imputation while simultaneously performing variable selection and learning the conditional dependence structure among responses. We establish theoretical error bounds, and our simulations demonstrate that \texttt{missoNet} consistently outperforms existing methods in both prediction and sparsity recovery. In a real-world mQTL analysis of the CARTaGENE cohort, \texttt{missoNet} achieved superior predictive accuracy and false-discovery control on a held-out validation set, identifying known and credible novel genetic associations. The method offers a robust, efficient, and theoretically grounded tool for genomic analyses, and is available as an R package.",Shomoita Alam; Yixiao Zeng; Sasha Bernatsky; Marie Hudson; Inés Colmegna; David A. Stephens; Celia M. T. Greenwood; Archer Y. Yang,,2025-07-08T13:50:05Z,http://arxiv.org/abs/2507.05990v1
2507.06113v1,A Statistical Framework for Co-Mediators of Zero-Inflated Single-Cell   RNA-Seq Data,"Single-cell RNA sequencing (scRNA-seq) has revolutionized the study of cellular heterogeneity, enabling detailed molecular profiling at the individual cell level. However, integrating high-dimensional single-cell data into causal mediation analysis remains challenging due to zero inflation and complex mediator structures. We propose a novel mediation framework leveraging zero-inflated negative binomial models to characterize cell-level mediator distributions and beta regression for zero-inflation proportions. Subject-level mediators are aggregated from cell-level data to perform mediation analysis assessing causal pathways linking gene expression to clinical outcomes. Extensive simulation studies demonstrate improved power and controlled false discovery rates. We further illustrate the utility of this approach through application to ROSMAP single-cell transcriptomic data, uncovering biologically meaningful mediation effects that enhance understanding of disease mechanisms.",Seungjun Ahn; Zhigang Li,,2025-07-08T15:52:47Z,http://arxiv.org/abs/2507.06113v1
2507.08751v1,ML-Based Automata Simplification for Symbolic Accelerators,"Symbolic accelerators are increasingly used for symbolic data processing in domains such as genomics, NLP, and cybersecurity. However, these accelerators face scalability issues due to excessive memory use and routing complexity, especially when targeting a large set. We present AutoSlim, a machine learning-based graph simplification framework designed to reduce the complexity of symbolic accelerators built on Non-deterministic Finite Automata (NFA) deployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest classification to prune low-impact transitions based on edge scores and structural features, significantly reducing automata graph density while preserving semantic correctness. Unlike prior tools, AutoSlim targets automated score-aware simplification with weighted transitions, enabling efficient ranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in NAPOLY+ and conducted performance measurements including latency, throughput, and resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs and over 30 percent pruning in transitions, while scaling to graphs an order of magnitude larger than existing benchmarks. Our results also demonstrate how hardware interconnection (fanout) heavily influences hardware cost and that AutoSlim's pruning mitigates resource blowup.",Tiffany Yu; Rye Stahle-Smith; Darssan Eswaramoorthi; Rasha Karakchi,,2025-07-11T17:02:33Z,http://arxiv.org/abs/2507.08751v1
2507.09391v1,Geometric Generative Modeling with Noise-Conditioned Graph Networks,"Generative modeling of graphs with spatial structure is essential across many applications from computer graphics to spatial genomics. Recent flow-based generative models have achieved impressive results by gradually adding and then learning to remove noise from these graphs. Existing models, however, use graph neural network architectures that are independent of the noise level, limiting their expressiveness. To address this issue, we introduce \textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural networks that dynamically modify their architecture according to the noise level during generation. Our theoretical and empirical analysis reveals that as noise increases, (1) graphs require information from increasingly distant neighbors and (2) graphs can be effectively represented at lower resolutions. Based on these insights, we develop Dynamic Message Passing (DMP), a specific instantiation of NCGNs that adapts both the range and resolution of message passing to the noise level. DMP consistently outperforms noise-independent architectures on a variety of domains including $3$D point clouds, spatiotemporal transcriptomics, and images. Code is available at https://github.com/peterpaohuang/ncgn.",Peter Pao-Huang; Mitchell Black; Xiaojie Qiu,,2025-07-12T20:19:05Z,http://arxiv.org/abs/2507.09391v1
2507.10250v1,DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in   Histopathology,"Accurate and timely cancer diagnosis from histopathological slides is vital for effective clinical decision-making. This paper introduces DepViT-CAD, a deployable AI system for multi-class cancer diagnosis in histopathology. At its core is MAViT, a novel Multi-Attention Vision Transformer designed to capture fine-grained morphological patterns across diverse tumor types. MAViT was trained on expert-annotated patches from 1008 whole-slide images, covering 11 diagnostic categories, including 10 major cancers and non-tumor tissue. DepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer Genome Atlas and 50 routine clinical cases from pathology labs, achieving diagnostic sensitivities of 94.11% and 92%, respectively. By combining state-of-the-art transformer architecture with large-scale real-world validation, DepViT-CAD offers a robust and scalable approach for AI-assisted cancer diagnostics. To support transparency and reproducibility, software and code will be made publicly available at GitHub.",Ashkan Shakarami; Lorenzo Nicole; Rocco Cappellesso; Angelo Paolo Dei Tos; Stefano Ghidoni,,2025-07-14T13:17:46Z,http://arxiv.org/abs/2507.10250v1
2507.12207v1,BuildEvo: Designing Building Energy Consumption Forecasting Heuristics   via LLM-driven Evolution,"Accurate building energy forecasting is essential, yet traditional heuristics often lack precision, while advanced models can be opaque and struggle with generalization by neglecting physical principles. This paper introduces BuildEvo, a novel framework that uses Large Language Models (LLMs) to automatically design effective and interpretable energy prediction heuristics. Within an evolutionary process, BuildEvo guides LLMs to construct and enhance heuristics by systematically incorporating physical insights from building characteristics and operational data (e.g., from the Building Data Genome Project 2). Evaluations show BuildEvo achieves state-of-the-art performance on benchmarks, offering improved generalization and transparent prediction logic. This work advances the automated design of robust, physically grounded heuristics, promoting trustworthy models for complex energy systems.",Subin Lin; Chuanbo Hua,,2025-07-16T13:07:24Z,http://arxiv.org/abs/2507.12207v1
2507.15630v1,Testing Homogeneity in a heteroscedastic contaminated normal mixture,"Large-scale simultaneous hypothesis testing appears in many areas such as microarray studies, genome-wide association studies, brain imaging, disease mapping and astronomical surveys. A well-known inference method is to control the false discovery rate. One popular approach is to model the $z$-scores derived from the individual $t$-tests and then use this model to control the false discovery rate. We propose a new class of contaminated normal mixtures for modelling $z$-scores. We further design an EM-test for testing homogeneity in this class of mixture models. We show that the EM-test statistic has a shifted mixture of chi-squared limiting distribution. Simulation results show that the proposed testing procedure has accurate type I error and significantly larger power than its competitors under a variety of model specifications. A real-data example is analyzed to exemplify the application of the proposed method.",Xiaoqing Niu; Pengfei Li; Yuejiao Fu,,2025-07-21T13:53:01Z,http://arxiv.org/abs/2507.15630v1
2507.17172v1,Local graph estimation: Interpretable network discovery for complex data,"Large, complex datasets often include a small set of variables of primary interest, such as clinical outcomes or known biomarkers, whose relation to the broader system is the main focus of analysis. In these situations, exhaustively estimating the entire network may obscure insights into the scientific question at hand. To address this common scenario, we introduce local graph estimation, a statistical framework that focuses on inferring substructures around target variables rather than recovering the full network of inter-variable relationships. We show that traditional graph estimation methods often fail to recover local structure, and present pathwise feature selection (PFS) as an alternative approach. PFS estimates local subgraphs by iteratively applying feature selection and propagating uncertainty along network paths. We prove that PFS provides path discovery with finite-sample false discovery control and yields highly interpretable results, even in settings with mixed variable types and nonlinear dependencies. Applied to two cancer studies -- one analyzing county-level cancer incidence and mortality across the U.S., and another integrating gene, microRNA, protein, and clinical data from The Cancer Genome Atlas -- PFS uncovers biologically plausible networks that reveal both known and novel associations.",Omar Melikechi; David B. Dunson; Noureddine Melikechi; Jeffrey W. Miller,,2025-07-23T03:36:00Z,http://arxiv.org/abs/2507.17172v1
2507.18570v1,Hybrid Tokenization Strategy for DNA Language Model using Byte Pair   Encoding and K-MER Methods,"This paper presents a novel hybrid tokenization strategy that enhances the performance of DNA Language Models (DLMs) by combining 6-mer tokenization with Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at capturing local DNA sequence structures but often faces challenges, including uneven token distribution and a limited understanding of global sequence context. To address these limitations, we propose merging unique 6mer tokens with optimally selected BPE tokens generated through 600 BPE cycles. This hybrid approach ensures a balanced and context-aware vocabulary, enabling the model to capture both short and long patterns within DNA sequences simultaneously. A foundational DLM trained on this hybrid vocabulary was evaluated using next-k-mer prediction as a fine-tuning task, demonstrating significantly improved performance. The model achieved prediction accuracies of 10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming state-of-the-art models such as NT, DNABERT2, and GROVER. These results highlight the ability of the hybrid tokenization strategy to preserve both the local sequence structure and global contextual information in DNA modeling. This work underscores the importance of advanced tokenization methods in genomic language modeling and lays a robust foundation for future applications in downstream DNA sequence analysis and biological research.",Ganesh Sapkota; Md Hasibur Rahman,,2025-07-24T16:45:23Z,http://arxiv.org/abs/2507.18570v1
2507.19523v1,Language Models for Controllable DNA Sequence Design,"We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their application to DNA sequence generation remains largely underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer Generator for Controllable Generation, which leverages cross-modal encoding to integrate diverse biological signals. ATGC-Gen is instantiated with both decoder-only and encoder-only transformer architectures, allowing flexible training and generation under either autoregressive or masked recovery objectives. We evaluate ATGC-Gen on representative tasks including promoter and enhancer sequence design, and further introduce a new dataset based on ChIP-Seq experiments for modeling protein binding specificity. Our experiments demonstrate that ATGC-Gen can generate fluent, diverse, and biologically relevant sequences aligned with the desired properties. Compared to prior methods, our model achieves notable improvements in controllability and functional relevance, highlighting the potential of language models in advancing programmable genomic design. The source code is released at (https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).",Xingyu Su; Xiner Li; Yuchao Lin; Ziqian Xie; Degui Zhi; Shuiwang Ji,,2025-07-19T06:23:17Z,http://arxiv.org/abs/2507.19523v1
2507.19564v1,Consistency and Central Limit Results for the Maximum Likelihood   Estimator in the Admixture Model,"In the Admixture Model, the probability of an individual having a certain number of alleles at a specific marker depends on the allele frequencies in $K$ ancestral populations and the fraction of the individual's genome originating from these ancestral populations.   This study investigates consistency and central limit results of maximum likelihood estimators (MLEs) for the ancestry and the allele frequencies in the Admixture Model, complimenting previous work by \cite{pfaff2004information, pfaffelhuber2022central}. Specifically, we prove consistency of the MLE, if we estimate the allele frequencies and the ancestries. Furthermore, we prove central limit theorems, if we estimate the ancestry of a finite number of individuals and the allele frequencies of finitely many markers, also addressing the case where the true ancestry lies on the boundary of the parameter space.   Finally, we use the new theory to quantify the uncertainty of the MLEs for the data of \citet{10002015global}.",Carola Sophia Heinzel,,2025-07-25T16:10:43Z,http://arxiv.org/abs/2507.19564v1
2507.19893v1,Retrospective score tests versus prospective score tests for genetic   association with case-control data,"Since the seminal work by Prentice and Pyke (1979), the prospective logistic likelihood has become the standard method of analysis for retrospectively collected case-control data, in particular for testing the association between a single genetic marker and a disease outcome in genetic case-control studies. When studying multiple genetic markers with relatively small effects, especially those with rare variants, various aggregated approaches based on the same prospective likelihood have been developed to integrate subtle association evidence among all considered markers. In this paper we show that using the score statistic derived from a prospective likelihood is not optimal in the analysis of retrospectively sampled genetic data. We develop the locally most powerful genetic aggregation test derived through the retrospective likelihood under a random effect model assumption. In contrast to the fact that the disease prevalence information cannot be used to improve the efficiency for the estimation of odds ratio parameters in logistic regression models, we show that it can be utilized to enhance the testing power in genetic association studies. Extensive simulations demonstrate the advantages of the proposed method over the existing ones. One real genome-wide association study is analyzed for illustration.",Yukun Liu; Pengfei Li; Lei Song; Kai Yu; Jing Qin,,2025-07-26T09:43:22Z,http://arxiv.org/abs/2507.19893v1
2507.20838v1,BuildSTG: A Multi-building Energy Load Forecasting Method using   Spatio-Temporal Graph Neural Network,"Due to the extensive availability of operation data, data-driven methods show strong capabilities in predicting building energy loads. Buildings with similar features often share energy patterns, reflected by spatial dependencies in their operational data, which conventional prediction methods struggle to capture. To overcome this, we propose a multi-building prediction approach using spatio-temporal graph neural networks, comprising graph representation, graph learning, and interpretation. First, a graph is built based on building characteristics and environmental factors. Next, a multi-level graph convolutional architecture with attention is developed for energy prediction. Lastly, a method interpreting the optimized graph structure is introduced. Experiments on the Building Data Genome Project 2 dataset confirm superior performance over baselines such as XGBoost, SVR, FCNN, GRU, and Naive, highlighting the method's robustness, generalization, and interpretability in capturing meaningful building similarities and spatial relationships.",Yongzheng Liu; Yiming Wang; Po Xu; Yingjie Xu; Yuntian Chen; Dongxiao Zhang,,2025-07-28T13:47:36Z,http://arxiv.org/abs/2507.20838v1
2507.22004v2,Horseshoe Forests for High-Dimensional Causal Survival Analysis,"We develop a Bayesian tree ensemble model to estimate heterogeneous treatment effects in censored survival data with high-dimensional covariates. Instead of imposing sparsity through the tree structure, we place a horseshoe prior directly on the step heights to achieve adaptive global-local shrinkage. This strategy allows flexible regularisation and reduces noise. We develop a reversible jump Gibbs sampler to accommodate the non-conjugate horseshoe prior within the tree ensemble framework. We show through extensive simulations that the method accurately estimates treatment effects in high-dimensional covariate spaces, at various sparsity levels, and under non-linear treatment effect functions. We further illustrate the practical utility of the proposed approach by a re-analysis of pancreatic ductal adenocarcinoma (PDAC) survival data from The Cancer Genome Atlas.",Tijn Jacobs; Wessel N. van Wieringen; Stéphanie L. van der Pas,,2025-07-29T16:53:44Z,http://arxiv.org/abs/2507.22004v2
2507.22543v1,Pre-trained Models Perform the Best When Token Distributions Follow   Zipf's Law,"Tokenization is a fundamental step in natural language processing (NLP) and other sequence modeling domains, where the choice of vocabulary size significantly impacts model performance. Despite its importance, selecting an optimal vocabulary size remains underexplored, typically relying on heuristics or dataset-specific choices. In this work, we propose a principled method for determining the vocabulary size by analyzing token frequency distributions through Zipf's law. We show that downstream task performance correlates with how closely token distributions follow power-law behavior, and that aligning with Zipfian scaling improves both model efficiency and effectiveness. Extensive experiments across NLP, genomics, and chemistry demonstrate that models consistently achieve peak performance when the token distribution closely adheres to Zipf's law, establishing Zipfian alignment as a robust and generalizable criterion for vocabulary size selection.",Yanjin He; Qingkai Zeng; Meng Jiang,,2025-07-30T10:16:23Z,http://arxiv.org/abs/2507.22543v1
2508.00969v1,Masked Omics Modeling for Multimodal Representation Learning across   Histopathology and Molecular Profiles,"Self-supervised learning has driven major advances in computational pathology by enabling models to learn rich representations from hematoxylin and eosin (H&E)-stained cancer tissue. However, histopathology alone often falls short for molecular characterization and understanding clinical outcomes, as important information is contained in high-dimensional omics profiles like transcriptomics, methylomics, or genomics. In this work, we introduce MORPHEUS, a unified transformer-based pre-training framework that encodes both histopathology and multi-omics data into a shared latent space. At its core, MORPHEUS relies on a masked modeling objective applied to randomly selected omics portions, encouraging the model to learn biologically meaningful cross-modal relationships. The same pre-trained network can be applied to histopathology alone or in combination with any subset of omics modalities, seamlessly adapting to the available inputs. Additionally, MORPHEUS enables any-to-any omics generation, enabling one or more omics profiles to be inferred from any subset of modalities, including H&E alone. Pre-trained on a large pan-cancer cohort, MORPHEUS consistently outperforms state-of-the-art methods across diverse modality combinations and tasks, positioning itself as a promising framework for developing multimodal foundation models in oncology. The code is available at: https://github.com/Lucas-rbnt/MORPHEUS",Lucas Robinet; Ahmad Berjaoui; Elizabeth Cohen-Jonathan Moyal,,2025-08-01T15:29:26Z,http://arxiv.org/abs/2508.00969v1
2508.04742v1,Discovery of Disease Relationships via Transcriptomic Signature Analysis   Powered by Agentic AI,"Modern disease classification often overlooks molecular commonalities hidden beneath divergent clinical presentations. This study introduces a transcriptomics-driven framework for discovering disease relationships by analyzing over 1300 disease-condition pairs using GenoMAS, a fully automated agentic AI system. Beyond identifying robust gene-level overlaps, we develop a novel pathway-based similarity framework that integrates multi-database enrichment analysis to quantify functional convergence across diseases. The resulting disease similarity network reveals both known comorbidities and previously undocumented cross-category links. By examining shared biological pathways, we explore potential molecular mechanisms underlying these connections-offering functional hypotheses that go beyond symptom-based taxonomies. We further show how background conditions such as obesity and hypertension modulate transcriptomic similarity, and identify therapeutic repurposing opportunities for rare diseases like autism spectrum disorder based on their molecular proximity to better-characterized conditions. In addition, this work demonstrates how biologically grounded agentic AI can scale transcriptomic analysis while enabling mechanistic interpretation across complex disease landscapes. All results are publicly accessible at github.com/KeeeeChen/Pathway_Similarity_Network.",Ke Chen; Haohan Wang,,2025-08-06T04:25:40Z,http://arxiv.org/abs/2508.04742v1
2508.07465v1,MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease   Classification,"Integrating multi-omics data, such as DNA methylation, mRNA expression, and microRNA (miRNA) expression, offers a comprehensive view of the biological mechanisms underlying disease. However, the high dimensionality and complex interactions among omics layers present major challenges for predictive modeling. We propose Multi-Omics integration with Tree-generated Graph Neural Network (MOTGNN), a novel and interpretable framework for binary disease classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform omics-specific supervised graph construction, followed by modality-specific Graph Neural Networks (GNNs) for hierarchical representation learning, and a deep feedforward network for cross-omics integration. On three real-world disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance (e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains computational efficiency through sparse graphs (2.1-2.8 edges per node) and provides built-in interpretability, revealing both top-ranked biomarkers and the relative contributions of each omics modality. These results highlight MOTGNN's potential to improve both predictive accuracy and interpretability in multi-omics disease modeling.",Tiantian Yang; Zhiqian Chen,,2025-08-10T19:35:53Z,http://arxiv.org/abs/2508.07465v1
2508.08014v1,ShortCake: An integrated platform for efficient and reproducible   single-cell analysis,"Motivation: Recent advances in single-cell analysis have introduced new computational challenges. Researchers often need to use multiple analysis tools written in different programming languages while managing version conflicts between related packages within a single workflow. For the research community, minimizing the time spent on environment setup and installation issues is essential. Results: We present ShortCake, a containerized platform that integrates a suite of single-cell analysis tools written in R and Python. ShortCake isolates competing Python tools into separate virtual environments that can be easily accessed within a Jupyter notebook. This enables users to effortlessly transition between various environments, including R, even within a single notebook. Additionally, ShortCake offers multiple ``flavors,'' enabling users to select container images tailored to their specific needs. ShortCake provides a unified environment with fixed versions of various tools, thus streamlining workflows, reducing setup time, and improving reproducibility. Availability and implementation: The ShortCake image is available on DockerHub (https://hub.docker.com/r/rnakato/shortcake). The source code is available on GitHub (https://github.com/rnakato/ShortCake).",Ryuichiro Nakato; Luis Augusto Eijy Nagai,,2025-08-11T14:21:11Z,http://arxiv.org/abs/2508.08014v1
2508.12731v1,Mechanism of Quercetin in Inhibiting Triple-Negative Breast Cancer by   Regulating T Cell-Related Targets: An Analysis Based on Single-Cell   Sequencing and Network Pharmacology,"Objective: To investigate the mechanism by which quercetin inhibits triple-negative breast cancer (TNBC) through regulating T-cell-related targets, providing a novel strategy for TNBC immunotherapy.Methods: Single-cell RNA sequencing (GSE161529 dataset) and network pharmacology were integrated. PCA and UMAP clustering identified T-cell subsets and differentially expressed genes in TNBC microenvironment. TNBC-related targets were screened via CTD and OMIM databases, with functional pathways analyzed by GO/KEGG enrichment. Molecular docking and PPI networks validated interactions between quercetin and core targets.Results: Quercetin intersected with 79 TNBC targets, including AKT1, EGFR, and MMP9, enriched in EGFR inhibitor resistance and endocrine resistance pathways. Molecular docking revealed the highest affinity between quercetin and GSK3B (-13.2 kJ/mol). AKT1 and MMP9 expression correlated with patient survival.Conclusion: Quercetin may reverse TNBC immunosuppression by multi-target modulation of T-cell function, but clinical application requires solutions for its low bioavailability, such as delivery systems or combination therapies.",Ruiqi Chen; Liang Hang; Fengyun Wang,,2025-08-18T08:54:05Z,http://arxiv.org/abs/2508.12731v1
2508.12871v1,Stochastic Multistability of Clonallike States in the Eigen Model: a   Fidelity Catastrophe,"The Eigen model is a prototypical toy model of evolution that is synonymous with the so-called error catastrophe: when mutation rates are sufficiently high, the genetic variant with the largest replication rate does not occupy the largest fraction of the total population because it acts as a source for the other variants. Here we show that, in the stochastic version of the Eigen model, there is also a fidelity catastrophe. This arises due to the state-dependence of fluctuations and occurs when rates of mutation fall beneath a certain threshold, which we calculate. The result is a type of noise-induced multistability whereupon the system stochastically switches between short-lived regimes of effectively clonal behavior by different genetic variants. Most notably, when the number of possible variants -- typically $\sim4^L$, with $L\gg 1$ the length of the genome -- is significantly larger than the population size, there is only a vanishingly small interval of mutation rates for which the Eigen model is neither in the fidelity- nor error-catastrophe regimes, seemingly subverting traditional expectations for evolutionary systems.",Emanuele Crosato; Richard E. Spinney; Richard G. Morris,,2025-08-18T12:10:24Z,http://arxiv.org/abs/2508.12871v1
2508.14020v1,A Biased Random Key Genetic Algorithm for Solving the Longest Run   Subsequence Problem,"The longest run subsequence (LRS) problem is an NP-hard combinatorial optimization problem belonging to the class of subsequence problems from bioinformatics. In particular, the problem plays a role in genome reassembly. In this paper, we present a solution to the LRS problem using a Biased Random Key Genetic Algorithm (BRKGA). Our approach places particular focus on the computational efficiency of evaluating individuals, which involves converting vectors of gray values into valid solutions to the problem. For comparison purposes, a Max-Min Ant System is developed and implemented. This is in addition to the application of the integer linear programming solver CPLEX for solving all considered problem instances. The computation results show that the proposed BRKGA is currently a state-of-the-art technique for the LRS problem. Nevertheless, the results also show that there is room for improvement, especially in the context of input strings based on large alphabet sizes.",Christian Blum; Pedro Pinacho-Davidson,,2025-08-19T17:27:29Z,http://arxiv.org/abs/2508.14020v1
2508.14502v1,SATURN: Autoregressive Image Generation Guided by Scene Graphs,"State-of-the-art text-to-image models excel at photorealistic rendering but often struggle to capture the layout and object relationships implied by complex prompts. Scene graphs provide a natural structural prior, yet previous graph-guided approaches have typically relied on heavy GAN or diffusion pipelines, which lag behind modern autoregressive architectures in both speed and fidelity. We introduce SATURN (Structured Arrangement of Triplets for Unified Rendering Networks), a lightweight extension to VAR-CLIP that translates a scene graph into a salience-ordered token sequence, enabling a frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from 56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78, outperforming prior methods such as SG2IM and SGDiff without requiring extra modules or multi-stage training. Qualitative results further confirm improvements in object count fidelity and spatial relation accuracy, showing that SATURN effectively combines structural awareness with state-of-the-art autoregressive fidelity.",Thanh-Nhan Vo; Trong-Thuan Nguyen; Tam V. Nguyen; Minh-Triet Tran,,2025-08-20T07:45:08Z,http://arxiv.org/abs/2508.14502v1
2508.14924v1,A U-Statistic-based random forest approach for genetic interaction study,"Variations in complex traits are influenced by multiple genetic variants, environmental risk factors, and their interactions. Though substantial progress has been made in identifying single genetic variants associated with complex traits, detecting the gene-gene and gene-environment interactions remains a great challenge. When a large number of genetic variants and environmental risk factors are involved, searching for interactions is limited to pair-wise interactions due to the exponentially increased feature space and computational intensity. Alternatively, recursive partitioning approaches, such as random forests, have gained popularity in high-dimensional genetic association studies. In this article, we propose a U-Statistic-based random forest approach, referred to as Forest U-Test, for genetic association studies with quantitative traits. Through simulation studies, we showed that the Forest U-Test outperformed existing methods. The proposed method was also applied to study Cannabis Dependence CD, using three independent datasets from the Study of Addiction: Genetics and Environment. A significant joint association was detected with an empirical p-value less than 0.001. The finding was also replicated in two independent datasets with p-values of 5.93e-19 and 4.70e-17, respectively.",Ming Li; Ruo-Sin Peng; Changshuai Wei; Qing Lu,,2025-08-19T06:22:20Z,http://arxiv.org/abs/2508.14924v1
2508.16548v1,Stochastic modelling reveals that chromatin folding buffers epigenetic   landscapes against sirtuin depletion during DNA damage,"Epigenetic landscapes, represented by patterns of chemical modifications on histone tails, are essential for maintaining cell identity and tissue homeostasis. These landscapes are shaped by multiple factors, including local biochemical signals and the three-dimensional organisation of chromatin. However, their response to genomic stress, such as DNA double-strand breaks (DSBs), remains incompletely understood. Here, we use a stochastic model of histone modification dynamics integrated with chromatin architecture to investigate how local depletion of sirtuins, histone deacetylases involved in DSB repair, destabilises epigenetic patterns. Our simulations recapitulate experimental findings in which sirtuin relocalisation to DSB sites leads to the epigenetic erosion and suggest that the resulting landscape depends on enzyme levels and chromatin geometry. Importantly, chromatin regions with large domains of long-range contacts are more resilient to epigenetic destabilisation. These findings suggest that chromatin folding can buffer against relocation of histone-modifying enzymes, highlighting a structural mechanism for preserving epigenetic integrity under stress.",Daria Stepanova; Helen M. Byrne; Tomás Alarcón,,2025-08-22T17:13:13Z,http://arxiv.org/abs/2508.16548v1
2508.17345v1,ShortListing Model: A Streamlined SimplexDiffusion for Discrete Variable   Generation,"Generative modeling of discrete variables is challenging yet crucial for applications in natural language processing and biological sequence design. We introduce the Shortlisting Model (SLM), a novel simplex-based diffusion model inspired by progressive candidate pruning. SLM operates on simplex centroids, reducing generation complexity and enhancing scalability. Additionally, SLM incorporates a flexible implementation of classifier-free guidance, enhancing unconditional generation performance. Extensive experiments on DNA promoter and enhancer design, protein design, character-level and large-vocabulary language modeling demonstrate the competitive performance and strong potential of SLM. Our code can be found at https://github.com/GenSI-THUAIR/SLM",Yuxuan Song; Zhe Zhang; Yu Pei; Jingjing Gong; Qiying Yu; Zheng Zhang; Mingxuan Wang; Hao Zhou; Jingjing Liu; Wei-Ying Ma,,2025-08-24T13:03:02Z,http://arxiv.org/abs/2508.17345v1
2508.18058v1,Comprehensively stratifying MCIs into distinct risk subtypes based on   brain imaging genetics fusion learning,"Mild cognitive impairment (MCI) is the prodromal stage of Alzheimer's disease (AD) and thus enrolling MCI subjects to undergo clinical trials is worthwhile. However, MCI groups usually show significant diversity and heterogeneity in the pathology and symptom, which pose great challenge to accurately select appropriate subjects. This study aimed to stratify MCI subjects into distinct subgroups with substantial difference in the risk of transitioning to AD by fusing multimodal brain imaging genetic data. The integrated imaging genetics method comprised three modules, i.e., the whole-genome-oriented risk genetic information extraction module (RGE), the genetic-to-brain mapping module (RG2PG), and the genetic-guided pseudo-brain fusion module (CMPF). We used data from AD Neuroimaging Initiative (ADNI) and identified two MCI subtypes, called low-risk MCI (lsMCI) and high-risk MCI (hsMCI). We also validated that the two subgroups showed distinct patterns of in terms of multiple biomarkers including genetics, demographics, fluid biomarkers, brain imaging features, clinical symptoms and cognitive functioning at baseline, as well as their longitudinal developmental trajectories. Furthermore, we also identified potential biomarkers that may implicate the risk of MCIs, providing critical insights for patient stratification at early stage.",Muheng Shang; Jin Zhang; Junwei Han; Lei Du,,2025-08-25T14:18:43Z,http://arxiv.org/abs/2508.18058v1
2509.00123v1,Friend or Foe,"A fundamental challenge in microbial ecology is determining whether bacteria compete or cooperate in different environmental conditions. With recent advances in genome-scale metabolic models, we are now capable of simulating interactions between thousands of pairs of bacteria in thousands of different environmental settings at a scale infeasible experimentally. These approaches can generate tremendous amounts of data that can be exploited by state-of-the-art machine learning algorithms to uncover the mechanisms driving interactions. Here, we present Friend or Foe, a compendium of 64 tabular environmental datasets, consisting of more than 26M shared environments for more than 10K pairs of bacteria sampled from two of the largest collections of metabolic models. The Friend or Foe datasets are curated for a wide range of machine learning tasks -- supervised, unsupervised, and generative -- to address specific questions underlying bacterial interactions. We benchmarked a selection of the most recent models for each of these tasks and our results indicate that machine learning can be successful in this application to microbial ecology. Going beyond, analyses of the Friend or Foe compendium can shed light on the predictability of bacterial interactions and highlight novel research directions into how bacteria infer and navigate their relationships.",Oleksandr Cherendichenko; Josephine Solowiej-Wedderburn; Laura M. Carroll; Eric Libby,,2025-08-29T06:37:55Z,http://arxiv.org/abs/2509.00123v1
2509.00349v1,Computational approaches for virus host prediction: A review of methods   and applications,"Accurate prediction of virus-host interactions is critical for understanding viral ecology and developing applications like phage therapy. However, the growing number of computational tools has created a complex landscape, making direct performance comparison challenging due to inconsistent benchmarks and varying usability. Here, we provide a systematic review and a rigorous benchmark of 27 virus-host prediction tools. We formulate the host prediction task into two primary frameworks, link prediction and multi-class classification, and construct two benchmark datasets to evaluate tool performance in distinct scenarios: a database-centric dataset (RefSeq-VHDB) and a metagenomic discovery dataset (MetaHiC-VHDB). Our results reveal that no single tool is universally optimal. Performance is highly context-dependent, with tools like CHERRY and iPHoP demonstrating robust, broad applicability, while others, such as RaFAH and PHIST, excel in specific contexts. We further identify a critical trade-off between predictive accuracy, prediction rate, and computational cost. This work serves as a practical guide for researchers and establishes a standardized benchmark to drive future innovation in deciphering complex virus-host interactions.",Jiayu Shang; Cheng Peng; Jiaojiao Guan; Dehan Cai; Donglin Wang; Yanni Sun,,2025-08-30T04:19:23Z,http://arxiv.org/abs/2509.00349v1
2509.01001v1,Generalized promotion time cure model: A new modeling framework to   identify cell-type-specific genes and improve survival prognosis,"Single-cell technologies provide an unprecedented opportunity for dissecting the interplay between the cancer cells and the associated tumor microenvironment, and the produced high-dimensional omics data should also augment existing survival modeling approaches for identifying tumor cell type-specific genes predictive of cancer patient survival. However, there is no statistical model to integrate multiscale data including individual-level survival data, multicellular-level cell composition data and cellular-level single-cell omics covariates. We propose a class of Bayesian generalized promotion time cure models (GPTCMs) for the multiscale data integration to identify cell-type-specific genes and improve cancer prognosis. We demonstrate with simulations in both low- and high-dimensional settings that the proposed Bayesian GPTCMs are able to identify cell-type-associated covariates and improve survival prediction.",Zhi Zhao; Fatih Kizilaslan; Shixiong Wang; Manuela Zucknick,,2025-08-31T21:35:57Z,http://arxiv.org/abs/2509.01001v1
2509.01541v1,Graph Contrastive Learning versus Untrained Baselines: The Role of   Dataset Size,"Graph Contrastive Learning (GCL) has emerged as a leading paradigm for self-supervised learning on graphs, with strong performance reported on standardized datasets and growing applications ranging from genomics to drug discovery. We ask a basic question: does GCL actually outperform untrained baselines? We find that GCL's advantage depends strongly on dataset size and task difficulty. On standard datasets, untrained Graph Neural Networks (GNNs), simple multilayer perceptrons, and even handcrafted statistics can rival or exceed GCL. On the large molecular dataset ogbg-molhiv, we observe a crossover: GCL lags at small scales but pulls ahead beyond a few thousand graphs, though this gain eventually plateaus. On synthetic datasets, GCL accuracy approximately scales with the logarithm of the number of graphs and its performance gap (compared with untrained GNNs) varies with respect to task complexity. Moving forward, it is crucial to identify the role of dataset size in benchmarks and applications, as well as to design GCL algorithms that avoid performance plateaus.",Smayan Khanna; Doruk Efe Gökmen; Risi Kondor; Vincenzo Vitelli,,2025-09-01T15:16:28Z,http://arxiv.org/abs/2509.01541v1
2509.02648v1,Optimizing Prognostic Biomarker Discovery in Pancreatic Cancer Through   Hybrid Ensemble Feature Selection and Multi-Omics Data,"Prediction of patient survival using high-dimensional multi-omics data requires systematic feature selection methods that ensure predictive performance, sparsity, and reliability for prognostic biomarker discovery. We developed a hybrid ensemble feature selection (hEFS) approach that combines data subsampling with multiple prognostic models, integrating both embedded and wrapper-based strategies for survival prediction. Omics features are ranked using a voting-theory-inspired aggregation mechanism across models and subsamples, while the optimal number of features is selected via a Pareto front, balancing predictive accuracy and model sparsity without any user-defined thresholds. When applied to multi-omics datasets from three pancreatic cancer cohorts, hEFS identifies significantly fewer and more stable biomarkers compared to the conventional, late-fusion CoxLasso models, while maintaining comparable discrimination performance. Implemented within the open-source mlr3fselect R package, hEFS offers a robust, interpretable, and clinically valuable tool for prognostic modelling and biomarker discovery in high-dimensional survival settings.",John Zobolas; Anne-Marie George; Alberto López; Sebastian Fischer; Marc Becker; Tero Aittokallio,,2025-09-02T11:09:24Z,http://arxiv.org/abs/2509.02648v1
2509.03330v1,Network-driven discovery of repurposable drugs targeting hallmarks of   aging,"Despite the thousands of genes implicated in age-related phenotypes, effective interventions for aging remain elusive, a lack of advance rooted in the multifactorial nature of longevity and the functional interconnectedness of the molecular components implicated in aging. Here, we introduce a network medicine framework that integrates 2,358 longevity-associated genes onto the human interactome to identify existing drugs that can modulate aging processes. We find that genes associated with each hallmark of aging form a connected subgraph, or hallmark module, a discovery enabling us to measure the proximity of 6,442 clinically approved or experimental compounds to each hallmark. We then introduce a transcription-based metric, $pAGE$, which evaluates whether the drug-induced expression shifts reinforce or counteract known age-related expression changes. By integrating network proximity and $pAGE$, we identify multiple drug repurposing candidate that not only target specific hallmarks but act to reverse their aging-associated transcriptional changes. Our findings are interpretable, revealing for each drug the molecular mechanisms through which it modulates the hallmark, offering an experimentally falsifiable framework to leverage genomic discoveries to accelerate drug repurposing for longevity.",Bnaya Gross; Joseph Ehlert; Vadim N. Gladyshev; Joseph Loscalzo; Albert-László Barabási,,2025-09-03T14:05:56Z,http://arxiv.org/abs/2509.03330v1
2508.13255v2,FAIR sharing of Chromatin Tracing datasets using the newly developed 4DN   FISH Omics Format,"A key output of the NIH Common Fund 4D Nucleome (4DN) project is the open publication of datasets on the structure of the human cell nucleus and genome. In recent years, multiplexed Fluorescence In Situ Hybridization (FISH) and FISH-omics methods have rapidly expanded, enabling quantification of chromatin organization in single cells, sometimes alongside RNA and protein measurements. These approaches have deepened our understanding of how 3D chromosome architecture relates to transcriptional activity and cell development in health and disease. However, results from Chromatin Tracing FISH-omics experiments remain difficult to share, reuse, and analyze due to the absence of standardized data-exchange specifications. Building on the recent release of microscopy metadata standards, we introduce the 4DN FISH Omics Format-Chromatin Tracing (FOF-CT), a community-developed standard for processed results from diverse imaging techniques. Current studies generally use one of two representations: ball-and-stick, where genomic segments appear as individual fluorescence spots, or volumetric, representing them as clouds of single-molecule localizations. This manuscript focuses on ball-and-stick methods, including those from the pioneering study of Wang et al. (2016) and related techniques. We describe the FOF-CT structure and present newly deposited datasets in the 4DN Data Portal and the OME Image Data Resource (IDR), highlighting their potential for reuse, integration, and modeling. We also outline example analysis pipelines and illustrate biological insights enabled by standardized, FAIR-compliant Chromatin Tracing datasets.",Rahi Navelkar; Andrea Cosolo; Bogdan Bintu; Yubao Cheng; Vincent Gardeux; Silvia Gutnik; Taihei Fujimori; Antonina Hafner; Atishay Jay; Bojing Blair Jia; Adam Paul Jussila; Gerard Llimos; Antonios Lioutas; Nuno MC Martins; William J Moore; Yodai Takei; Frances Wong; Kaifu Yang; Huaiying Zhang; Quan Zhu; Magda Bienko; Lacramioara Bintu; Long Cai; Bart Deplancke; Marcelo Nollmann; Susan E Mango; Bing Ren; Peter J Park; Ahilya N Sawh; Andrew Schroeder; Jason R Swedlow; Golnaz Vahedi; Chao-Ting Wu; Sarah Aufmkolk; Alistair N Boettiger; Irene Farabella; Caterina Strambio-De-Castillia; Siyuan Wang,,2025-08-18T16:16:42Z,http://arxiv.org/abs/2508.13255v2
2501.00755v1,An AI-powered Bayesian generative modeling approach for causal inference   in observational studies,"Causal inference in observational studies with high-dimensional covariates presents significant challenges. We introduce CausalBGM, an AI-powered Bayesian generative modeling approach that captures the causal relationship among covariates, treatment, and outcome variables. The core innovation of CausalBGM lies in its ability to estimate the individual treatment effect (ITE) by learning individual-specific distributions of a low-dimensional latent feature set (e.g., latent confounders) that drives changes in both treatment and outcome. This approach not only effectively mitigates confounding effects but also provides comprehensive uncertainty quantification, offering reliable and interpretable causal effect estimates at the individual level. CausalBGM adopts a Bayesian model and uses a novel iterative algorithm to update the model parameters and the posterior distribution of latent features until convergence. This framework leverages the power of AI to capture complex dependencies among variables while adhering to the Bayesian principles. Extensive experiments demonstrate that CausalBGM consistently outperforms state-of-the-art methods, particularly in scenarios with high-dimensional covariates and large-scale datasets. Its Bayesian foundation ensures statistical rigor, providing robust and well-calibrated posterior intervals. By addressing key limitations of existing methods, CausalBGM emerges as a robust and promising framework for advancing causal inference in modern applications in fields such as genomics, healthcare, and social sciences. CausalBGM is maintained at the website https://causalbgm.readthedocs.io/.",Qiao Liu; Wing Hung Wong,,2025-01-01T06:52:45Z,http://arxiv.org/abs/2501.00755v1
2501.03923v1,Explainable AI model reveals disease-related mechanisms in single-cell   RNA-seq data,"Neurodegenerative diseases (NDDs) are complex and lack effective treatment due to their poorly understood mechanism. The increasingly used data analysis from Single nucleus RNA Sequencing (snRNA-seq) allows to explore transcriptomic events at a single cell level, yet face challenges in interpreting the mechanisms underlying a disease. On the other hand, Neural Network (NN) models can handle complex data to offer insights but can be seen as black boxes with poor interpretability. In this context, explainable AI (XAI) emerges as a solution that could help to understand disease-associated mechanisms when combined with efficient NN models. However, limited research explores XAI in single-cell data. In this work, we implement a method for identifying disease-related genes and the mechanistic explanation of disease progression based on NN model combined with SHAP. We analyze available Huntington's disease (HD) data to identify both HD-altered genes and mechanisms by adding Gene Set Enrichment Analysis (GSEA) comparing two methods, differential gene expression analysis (DGE) and NN combined with SHAP approach. Our results show that DGE and SHAP approaches offer both common and differential sets of altered genes and pathways, reinforcing the usefulness of XAI methods for a broader perspective of disease.",Mohammad Usman; Olga Varea; Petia Radeva; Josep Canals; Jordi Abante; Daniel Ortiz,,2025-01-07T16:35:29Z,http://arxiv.org/abs/2501.03923v1
2501.04181v1,Deep Learning-based Feature Discovery for Decoding Phenotypic Plasticity   in Pediatric High-Grade Gliomas Single-Cell Transcriptomics,"By use of complex network dynamics and graph-based machine learning, we identified critical determinants of lineage-specific plasticity across the single-cell transcriptomics of pediatric high-grade glioma (pHGGs) subtypes: IDHWT glioblastoma and K27M-mutant glioma. Our study identified network interactions regulating glioma morphogenesis via the tumor-immune microenvironment, including neurodevelopmental programs, calcium dynamics, iron metabolism, metabolic reprogramming, and feedback loops between MAPK/ERK and WNT signaling. These relationships highlight the emergence of a hybrid spectrum of cellular states navigating a disrupted neuro-differentiation hierarchy. We identified transition genes such as DKK3, NOTCH2, GATAD1, GFAP, and SEZ6L in IDHWT glioblastoma, and H3F3A, ANXA6, HES6/7, SIRT2, FXYD6, PTPRZ1, MEIS1, CXXC5, and NDUFAB1 in K27M subtypes. We also identified MTRNR2L1, GAPDH, IGF2, FKBP variants, and FXYD7 as transition genes that influence cell fate decision-making across both subsystems. Our findings suggest pHGGs are developmentally trapped in states exhibiting maladaptive behaviors, and hybrid cellular identities. In effect, tumor heterogeneity (metastability) and plasticity emerge as stress-response patterns to immune-inflammatory microenvironments and oxidative stress. Furthermore, we show that pHGGs are steered by developmental trajectories from radial glia predominantly favoring neocortical cell fates, in telencephalon and prefrontal cortex (PFC) differentiation. By addressing underlying patterning processes and plasticity networks as therapeutic vulnerabilities, our findings provide precision medicine strategies aimed at modulating glioma cell fates and overcoming therapeutic resistance. We suggest transition therapy toward neuronal-like lineage differentiation as a potential therapy to help stabilize pHGG plasticity and aggressivity.",Abicumaran Uthamacumaran,,2025-01-07T23:20:59Z,http://arxiv.org/abs/2501.04181v1
2501.04520v3,Inferring resource competition in microbial communities from time series,"The competition for resources is a defining feature of microbial communities. In many contexts, from soils to host-associated communities, highly diverse microbes are organized into metabolic groups or guilds with similar resource preferences. The resource preferences of individual taxa that give rise to these guilds are critical for understanding fluxes of resources through the community and the structure of diversity in the system. However, inferring the metabolic capabilities of individual taxa, and their competition with other taxa, within a community is challenging and unresolved. Here we address this gap in knowledge by leveraging dynamic measurements of abundances in communities. We show that simple correlations are often misleading in predicting resource competition. We show that spectral methods such as the cross-power spectral density (CPSD) and coherence that account for time-delayed effects are superior metrics for inferring the structure of resource competition in communities. We first demonstrate this fact on synthetic data generated from consumer-resource models with time-dependent resource availability, where taxa are organized into groups or guilds with similar resource preferences. By applying spectral methods to oceanic plankton time-series data, we demonstrate that these methods detect interaction structures among species with similar genomic sequences. Our results indicate that analyzing temporal data across multiple timescales can reveal the underlying structure of resource competition within communities.",Xiaowen Chen; Kyle Crocker; Seppe Kuehn; Aleksandra M. Walczak; Thierry Mora,,2025-01-08T14:13:12Z,http://arxiv.org/abs/2501.04520v3
2501.04869v1,Transcriptome signature for the identification of bevacizumab responders   in ovarian cancer,"The standard of care for ovarian cancer comprises cytoreductive surgery, followed by adjuvant platinum-based chemotherapy plus taxane therapy and maintenance therapy with the antiangiogenic compound bevacizumab and/or a PARP inhibitor. Nevertheless, there is currently no clear clinical indication for the use of bevacizumab, highlighting the urgent need for biomarkers to assess the response to bevacizumab. In the present study, based on a novel RNA-seq dataset (n=181) and a previously published microarray-based dataset (n=377), we have identified an expression signature potentially associated with benefit from bevacizumab addition and assumed to reflect cancer stemness acquisition driven by activation of CTCFL. Patients with this signature demonstrated improved overall survival when bevacizumab was added to standard chemotherapy in both novel (HR=0.41(0.23-0.74), adj.p-value=7.70e-03) and previously published cohorts (HR=0.51(0.34-0.75), adj.p-value=3.25e-03), while no significant differences in survival explained by treatment were observed in patients negative for this signature. In addition to the CTCFL signature, we found several other reproducible expression signatures which may also represent biomarker candidates not related to established molecular subtypes of ovarian cancer and require further validation studies based on additional RNA-seq data.",Olga Zolotareva; Karen Legler; Olga Tsoy; Anna Esteve; Alexey Sergushichev; Vladimir Sukhov; Jan Baumbach; Kathrin Eylmann; Minyue Qi; Malik Alawi; Stefan Kommoss; Barbara Schmalfeldt; Leticia Oliveira-Ferrer,,2025-01-08T22:40:58Z,http://arxiv.org/abs/2501.04869v1
2501.06606v1,The tardigrade as an emerging model organism for systems neuroscience,"We present the case for developing the tardigrade (Hypsibius exemplaris) into a model organism for systems neuroscience. These microscopic, transparent animals (~300-500 microns) are among the smallest known to possess both limbs (eight) and eyes (two), with a nervous system of only a few hundred neurons organized into a multi-lobed brain, ventral nerve cord, and a series of ganglia along the body. Despite their neuroanatomical simplicity, tardigrades exhibit complex behaviors, including multi-limbed walking gaits, individual limb grasping, phototaxis, and transitions between active and dormant states. These behaviors position tardigrades as a uniquely powerful system for addressing certain fundamental questions in systems neuroscience, such as: How do nervous systems coordinate multi-limbed behaviors? How are top-down and bottom-up motor control systems integrated? How is stereovision-guided navigation implemented? What mechanisms underlie neural resilience and recovery during environmental stress? We review current knowledge of tardigrade neuroanatomy, behavior, and genomics, and we identify opportunities and challenges for leveraging their unique biology. We propose developing essential neuroscientific tools for tardigrades, including genetic engineering and live neuroimaging, alongside behavioral assays linking neural activity to outputs. Leveraging their evolutionary ties to Caenorhabditis elegans and Drosophila melanogaster, we can adapt existing toolkits to accelerate tardigrade research - providing a bridge between simpler invertebrate systems and more complex neural architectures.",Ana M. Lyons; Saul Kato,,2025-01-11T18:12:11Z,http://arxiv.org/abs/2501.06606v1
2501.06805v1,A Pan-cancer Classification Model using Multi-view Feature Selection   Method and Ensemble Classifier,"Accurately identifying cancer samples is crucial for precise diagnosis and effective patient treatment. Traditional methods falter with high-dimensional and high feature-to-sample count ratios, which are critical for classifying cancer samples. This study aims to develop a novel feature selection framework specifically for transcriptome data and propose two ensemble classifiers. For feature selection, we partition the transcriptome dataset vertically based on feature types. Then apply the Boruta feature selection process on each of the partitions, combine the results, and apply Boruta again on the combined result. We repeat the process with different parameters of Boruta and prepare the final feature set. Finally, we constructed two ensemble ML models based on LR, SVM and XGBoost classifiers with max voting and averaging probability approach. We used 10-fold cross-validation to ensure robust and reliable classification performance. With 97.11\% accuracy and 0.9996 AUC value, our approach performs better compared to existing state-of-the-art methods to classify 33 types of cancers. A set of 12 types of cancer is traditionally challenging to differentiate between each other due to their similarity in tissue of origin. Our method accurately identifies over 90\% of samples from these 12 types of cancers, which outperforms all known methods presented in existing literature. The gene set enrichment analysis reveals that our framework's selected features have enriched the pathways highly related to cancers. This study develops a feature selection framework to select features highly related to cancer development and leads to identifying different types of cancer samples with higher accuracy.",Tareque Mohmud Chowdhury; Farzana Tabassum; Sabrina Islam; Abu Raihan Mostofa Kamal,,2025-01-12T13:06:01Z,http://arxiv.org/abs/2501.06805v1
2501.07016v1,A Multi-Modal Deep Learning Framework for Pan-Cancer Prognosis,"Prognostic task is of great importance as it closely related to the survival analysis of patients, the optimization of treatment plans and the allocation of resources. The existing prognostic models have shown promising results on specific datasets, but there are limitations in two aspects. On the one hand, they merely explore certain types of modal data, such as patient histopathology WSI and gene expression analysis. On the other hand, they adopt the per-cancer-per-model paradigm, which means the trained models can only predict the prognostic effect of a single type of cancer, resulting in weak generalization ability. In this paper, a deep-learning based model, named UMPSNet, is proposed. Specifically, to comprehensively understand the condition of patients, in addition to constructing encoders for histopathology images and genomic expression profiles respectively, UMPSNet further integrates four types of important meta data (demographic information, cancer type information, treatment protocols, and diagnosis results) into text templates, and then introduces a text encoder to extract textual features. In addition, the optimal transport OT-based attention mechanism is utilized to align and fuse features of different modalities. Furthermore, a guided soft mixture of experts (GMoE) mechanism is introduced to effectively address the issue of distribution differences among multiple cancer datasets. By incorporating the multi-modality of patient data and joint training, UMPSNet outperforms all SOTA approaches, and moreover, it demonstrates the effectiveness and generalization ability of the proposed learning paradigm of a single model for multiple cancer types. The code of UMPSNet is available at https://github.com/binging512/UMPSNet.",Binyu Zhang; Shichao Li; Junpeng Jian; Zhu Meng; Limei Guo; Zhicheng Zhao,,2025-01-13T02:29:42Z,http://arxiv.org/abs/2501.07016v1
2501.08281v2,From Neural Representations to Interpretable Logic Rules,"As deep neural networks continue to excel across various domains, their black-box nature has raised concerns about transparency and trust. In particular, interpretability has become increasingly essential for applications that demand high safety and knowledge rigor, such as drug discovery, autonomous driving, and genomics. However, progress in understanding even the simplest deep neural networks - such as fully connected networks - has been limited, despite their role as foundational elements in state-of-the-art models like ResNet and Transformer. In this paper, we address this challenge by introducing NeuroLogic, a novel approach for decoding interpretable logic rules from neural networks. NeuroLogic leverages neural activation patterns to capture the model's critical decision-making processes, translating them into logical rules represented by hidden predicates. Thanks to its flexible design in the grounding phase, NeuroLogic can be adapted to a wide range of neural networks. For simple fully connected neural networks, hidden predicates can be grounded in certain split patterns of original input features to derive decision-tree-like rules. For large, complex vision neural networks, NeuroLogic grounds hidden predicates into high-level visual concepts that are understandable to humans. Our empirical study demonstrates that NeuroLogic can extract global and interpretable rules from state-of-the-art models such as ResNet, a task at which existing work struggles. We believe NeuroLogic can help pave the way for understanding the black-box nature of neural networks.",Chuqin Geng; Xiaojie Xu; Anqi Xing; Ziyu Zhao; Xujie Si,,2025-01-14T17:57:26Z,http://arxiv.org/abs/2501.08281v2
2501.13893v1,Pix2Cap-COCO: Advancing Visual Comprehension via Pixel-Level Captioning,"We present Pix2Cap-COCO, the first panoptic pixel-level caption dataset designed to advance fine-grained visual understanding. To achieve this, we carefully design an automated annotation pipeline that prompts GPT-4V to generate pixel-aligned, instance-specific captions for individual objects within images, enabling models to learn more granular relationships between objects and their contexts. This approach results in 167,254 detailed captions, with an average of 22.94 words per caption. Building on Pix2Cap-COCO, we introduce a novel task, panoptic segmentation-captioning, which challenges models to recognize instances in an image and provide detailed descriptions for each simultaneously. To benchmark this task, we design a robust baseline based on X-Decoder. The experimental results demonstrate that Pix2Cap-COCO is a particularly challenging dataset, as it requires models to excel in both fine-grained visual understanding and detailed language generation. Furthermore, we leverage Pix2Cap-COCO for Supervised Fine-Tuning (SFT) on large multimodal models (LMMs) to enhance their performance. For example, training with Pix2Cap-COCO significantly improves the performance of GPT4RoI, yielding gains in CIDEr +1.4%, ROUGE +0.4%, and SPICE +0.5% on Visual Genome dataset, and strengthens its region understanding ability on the ViP-BENCH, with an overall improvement of +5.1%, including notable increases in recognition accuracy +11.2% and language generation quality +22.2%.",Zuyao You; Junke Wang; Lingyu Kong; Bo He; Zuxuan Wu,,2025-01-23T18:08:57Z,http://arxiv.org/abs/2501.13893v1
2501.15489v1,AI in Oncology: Transforming Cancer Detection through Machine Learning   and Deep Learning Applications,"Artificial intelligence (AI) has potential to revolutionize the field of oncology by enhancing the precision of cancer diagnosis, optimizing treatment strategies, and personalizing therapies for a variety of cancers. This review examines the limitations of conventional diagnostic techniques and explores the transformative role of AI in diagnosing and treating cancers such as lung, breast, colorectal, liver, stomach, esophageal, cervical, thyroid, prostate, and skin cancers. The primary objective of this paper is to highlight the significant advancements that AI algorithms have brought to oncology within the medical industry. By enabling early cancer detection, improving diagnostic accuracy, and facilitating targeted treatment delivery, AI contributes to substantial improvements in patient outcomes. The integration of AI in medical imaging, genomic analysis, and pathology enhances diagnostic precision and introduces a novel, less invasive approach to cancer screening. This not only boosts the effectiveness of medical facilities but also reduces operational costs. The study delves into the application of AI in radiomics for detailed cancer characterization, predictive analytics for identifying associated risks, and the development of algorithm-driven robots for immediate diagnosis. Furthermore, it investigates the impact of AI on addressing healthcare challenges, particularly in underserved and remote regions. The overarching goal of this platform is to support the development of expert recommendations and to provide universal, efficient diagnostic procedures. By reviewing existing research and clinical studies, this paper underscores the pivotal role of AI in improving the overall cancer care system. It emphasizes how AI-enabled systems can enhance clinical decision-making and expand treatment options, thereby underscoring the importance of AI in advancing precision oncology",Muhammad Aftab; Faisal Mehmood; Chengjuan Zhang; Alishba Nadeem; Zigang Dong; Yanan Jiang; Kangdongs Liu,,2025-01-26T11:32:43Z,http://arxiv.org/abs/2501.15489v1
2501.15598v1,Diffusion Generative Modeling for Spatially Resolved Gene Expression   Inference from Histology Images,"Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA sequence abundance by systematically connecting cell morphology depicted in Hematoxylin and Eosin (H&E) stained histology images to spatially resolved gene expressions. ST is a time-consuming, expensive yet powerful experimental technique that provides new opportunities to understand cancer mechanisms at a fine-grained molecular level, which is critical for uncovering new approaches for disease diagnosis and treatments. Here, we present $\textbf{Stem}$ ($\textbf{S}$pa$\textbf{T}$ially resolved gene $\textbf{E}$xpression inference with diffusion $\textbf{M}$odel), a novel computational tool that leverages a conditional diffusion generative model to enable in silico gene expression inference from H&E stained images. Through better capturing the inherent stochasticity and heterogeneity in ST data, $\textbf{Stem}$ achieves state-of-the-art performance on spatial gene expression prediction and generates biologically meaningful gene profiles for new H&E stained images at test time. We evaluate the proposed algorithm on datasets with various tissue sources and sequencing platforms, where it demonstrates clear improvement over existing approaches. $\textbf{Stem}$ generates high-fidelity gene expression predictions that share similar gene variation levels as ground truth data, suggesting that our method preserves the underlying biological heterogeneity. Our proposed pipeline opens up the possibility of analyzing existing, easily accessible H&E stained histology images from a genomics point of view without physically performing gene expression profiling and empowers potential biological discovery from H&E stained histology images.",Sichen Zhu; Yuchen Zhu; Molei Tao; Peng Qiu,,2025-01-26T16:52:27Z,http://arxiv.org/abs/2501.15598v1
2501.15881v1,Multivariate Feature Selection and Autoencoder Embeddings of Ovarian   Cancer Clinical and Genetic Data,"This study explores a data-driven approach to discovering novel clinical and genetic markers in ovarian cancer (OC). Two main analyses were performed: (1) a nonlinear examination of an OC dataset using autoencoders, which compress data into a 3-dimensional latent space to detect potential intrinsic separability between platinum-sensitive and platinum-resistant groups; and (2) an adaptation of the informative variable identifier (IVI) to determine which features (clinical or genetic) are most relevant to disease progression. In the autoencoder analysis, a clearer pattern emerged when using clinical features and the combination of clinical and genetic data, indicating that disease progression groups can be distinguished more effectively after supervised fine tuning. For genetic data alone, this separability was less apparent but became more pronounced with a supervised approach. Using the IVI-based feature selection, key clinical variables (such as type of surgery and neoadjuvant chemotherapy) and certain gene mutations showed strong relevance, along with low-risk genetic factors. These findings highlight the strength of combining machine learning tools (autoencoders) with feature selection methods (IVI) to gain insights into ovarian cancer progression. They also underscore the potential for identifying new biomarkers that integrate clinical and genomic indicators, ultimately contributing to improved patient stratification and personalized treatment strategies.",Luis Bote-Curiel; Sergio Ruiz-Llorente; Sergio Muñoz-Romero; Mónica Yagüe-Fernández; Arantzazu Barquín; Jesús García-Donas; José Luis Rojo-Álvarez,,2025-01-27T09:07:07Z,http://arxiv.org/abs/2501.15881v1
2501.16652v1,Molecular-driven Foundation Model for Oncologic Pathology,"Foundation models are reshaping computational pathology by enabling transfer learning, where models pre-trained on vast datasets can be adapted for downstream diagnostic, prognostic, and therapeutic response tasks. Despite these advances, foundation models are still limited in their ability to encode the entire gigapixel whole-slide images without additional training and often lack complementary multimodal data. Here, we introduce Threads, a slide-level foundation model capable of generating universal representations of whole-slide images of any size. Threads was pre-trained using a multimodal learning approach on a diverse cohort of 47,171 hematoxylin and eosin (H&E)-stained tissue sections, paired with corresponding genomic and transcriptomic profiles - the largest such paired dataset to be used for foundation model development to date. This unique training paradigm enables Threads to capture the tissue's underlying molecular composition, yielding powerful representations applicable to a wide array of downstream tasks. In extensive benchmarking across 54 oncology tasks, including clinical subtyping, grading, mutation prediction, immunohistochemistry status determination, treatment response prediction, and survival prediction, Threads outperformed all baselines while demonstrating remarkable generalizability and label efficiency. It is particularly well suited for predicting rare events, further emphasizing its clinical utility. We intend to make the model publicly available for the broader community.",Anurag Vaidya; Andrew Zhang; Guillaume Jaume; Andrew H. Song; Tong Ding; Sophia J. Wagner; Ming Y. Lu; Paul Doucet; Harry Robertson; Cristina Almagro-Perez; Richard J. Chen; Dina ElHarouni; Georges Ayoub; Connor Bossi; Keith L. Ligon; Georg Gerber; Long Phi Le; Faisal Mahmood,,2025-01-28T02:35:02Z,http://arxiv.org/abs/2501.16652v1
2501.18650v1,Constructing Cell-type Taxonomy by Optimal Transport with Relaxed   Marginal Constraints,"The rapid emergence of single-cell data has facilitated the study of many different biological conditions at the cellular level. Cluster analysis has been widely applied to identify cell types, capturing the essential patterns of the original data in a much more concise form. One challenge in the cluster analysis of cells is matching clusters extracted from datasets of different origins or conditions. Many existing algorithms cannot recognize new cell types present in only one of the two samples when establishing a correspondence between clusters obtained from two samples. Additionally, when there are more than two samples, it is advantageous to align clusters across all samples simultaneously rather than performing pairwise alignment. Our approach aims to construct a taxonomy for cell clusters across all samples to better annotate these clusters and effectively extract features for downstream analysis. A new system for constructing cell-type taxonomy has been developed by combining the technique of Optimal Transport with Relaxed Marginal Constraints (OT-RMC) and the simultaneous alignment of clusters across multiple samples. OT-RMC allows us to address challenges that arise when the proportions of clusters vary substantially between samples or when some clusters do not appear in all the samples. Experiments on more than twenty datasets demonstrate that the taxonomy constructed by this new system can yield highly accurate annotation of cell types. Additionally, sample-level features extracted based on the taxonomy result in accurate classification of samples.",Sebastian Pena; Lin Lin; Jia Li,,2025-01-29T21:29:25Z,http://arxiv.org/abs/2501.18650v1
2501.18794v1,Survey and Improvement Strategies for Gene Prioritization with Large   Language Models,"Rare diseases are challenging to diagnose due to limited patient data and genetic diversity. Despite advances in variant prioritization, many cases remain undiagnosed. While large language models (LLMs) have performed well in medical exams, their effectiveness in diagnosing rare genetic diseases has not been assessed. To identify causal genes, we benchmarked various LLMs for gene prioritization. Using multi-agent and Human Phenotype Ontology (HPO) classification, we categorized patients based on phenotypes and solvability levels. As gene set size increased, LLM performance deteriorated, so we used a divide-and-conquer strategy to break the task into smaller subsets. At baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking causal genes correctly. The multi-agent and HPO approaches helped distinguish confidently solved cases from challenging ones, highlighting the importance of known gene-phenotype associations and phenotype specificity. We found that cases with specific phenotypes or clear associations were more accurately solved. However, we observed biases toward well-studied genes and input order sensitivity, which hindered gene prioritization. Our divide-and-conquer strategy improved accuracy by overcoming these biases. By utilizing HPO classification, novel multi-agent techniques, and our LLM strategy, we improved causal gene identification accuracy compared to our baseline evaluation. This approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved cases, and accelerates gene discovery, supporting the development of targeted diagnostics and therapies.",Matthew Neeley; Guantong Qi; Guanchu Wang; Ruixiang Tang; Dongxue Mao; Chaozhong Liu; Sasidhar Pasupuleti; Bo Yuan; Fan Xia; Pengfei Liu; Zhandong Liu; Xia Hu,,2025-01-30T23:03:03Z,http://arxiv.org/abs/2501.18794v1
2502.00979v1,MorphoITH: A Framework for Deconvolving Intra-Tumor Heterogeneity Using   Tissue Morphology,"The ability of tumors to evolve and adapt by developing subclones in different genetic and epigenetic states is a major challenge in oncology. Traditional tools like multi-regional sequencing used to study tumor evolution and the resultant intra-tumor heterogeneity (ITH) are often impractical because of their resource-intensiveness and limited scalability. Here, we present MorphoITH, a novel framework that leverages histopathology slides to deconvolve molecular ITH through tissue morphology. MorphoITH integrates a self-supervised deep learning similarity measure to capture phenotypic variation across multiple dimensions (cytology, architecture, and microenvironment) with rigorous methods to eliminate spurious sources of variation. Using a prototype of ITH, clear cell renal cell carcinoma (ccRCC), we show that MorphoITH captures clinically-significant biological features, such as vascular architecture and nuclear grades. Furthermore, we find that MorphoITH recognizes differential biological states corresponding to subclonal changes in key driver genes (BAP1/PBRM1/SETD2). Finally, by applying MorphoITH to a multi-regional sequencing experiment, we postulate evolutionary trajectories that largely recapitulate genetic evolution. In summary, MorphoITH provides a scalable phenotypic lens that bridges the gap between histopathology and genomics, advancing precision oncology.",Aleksandra Weronika Nielsen; Hafez Eslami Manoochehri; Hua Zhong; Vandana Panwar; Vipul Jarmale; Jay Jasti; Mehrdad Nourani; Dinesh Rakheja; James Brugarolas; Payal Kapur; Satwik Rajaram,,2025-02-03T01:18:59Z,http://arxiv.org/abs/2502.00979v1
2502.01689v1,scGSDR: Harnessing Gene Semantics for Single-Cell Pharmacological   Profiling,"The rise of single-cell sequencing technologies has revolutionized the exploration of drug resistance, revealing the crucial role of cellular heterogeneity in advancing precision medicine. By building computational models from existing single-cell drug response data, we can rapidly annotate cellular responses to drugs in subsequent trials. To this end, we developed scGSDR, a model that integrates two computational pipelines grounded in the knowledge of cellular states and gene signaling pathways, both essential for understanding biological gene semantics. scGSDR enhances predictive performance by incorporating gene semantics and employs an interpretability module to identify key pathways contributing to drug resistance phenotypes. Our extensive validation, which included 16 experiments covering 11 drugs, demonstrates scGSDR's superior predictive accuracy, when trained with either bulk-seq or scRNA-seq data, achieving high AUROC, AUPR, and F1 Scores. The model's application has extended from single-drug predictions to scenarios involving drug combinations. Leveraging pathways of known drug target genes, we found that scGSDR's cell-pathway attention scores are biologically interpretable, which helped us identify other potential drug-related genes. Literature review of top-ranking genes in our predictions such as BCL2, CCND1, the AKT family, and PIK3CA for PLX4720; and ICAM1, VCAM1, NFKB1, NFKBIA, and RAC1 for Paclitaxel confirmed their relevance. In conclusion, scGSDR, by incorporating gene semantics, enhances predictive modeling of cellular responses to diverse drugs, proving invaluable for scenarios involving both single drug and combination therapies and effectively identifying key resistance-related pathways, thus advancing precision medicine and targeted therapy development.",Yu-An Huang; Xiyue Cao; Zhu-Hong You; Yue-Chao Li; Xuequn Shang; Zhi-An Huang,,2025-02-02T15:43:20Z,http://arxiv.org/abs/2502.01689v1
2502.02629v1,Graph Structure Learning for Tumor Microenvironment with Cell Type   Annotation from non-spatial scRNA-seq data,"The exploration of cellular heterogeneity within the tumor microenvironment (TME) via single-cell RNA sequencing (scRNA-seq) is essential for understanding cancer progression and response to therapy. Current scRNA-seq approaches, however, lack spatial context and rely on incomplete datasets of ligand-receptor interactions (LRIs), limiting accurate cell type annotation and cell-cell communication (CCC) inference. This study addresses these challenges using a novel graph neural network (GNN) model that enhances cell type prediction and cell interaction analysis. Our study utilized a dataset consisting of 49,020 cells from 19 patients across three cancer types: Leukemia, Breast Invasive Carcinoma, and Colorectal Cancer. The proposed scGSL model demonstrated robust performance, achieving an average accuracy of 84.83%, precision of 86.23%, recall of 81.51%, and an F1 score of 80.92% across all datasets. These metrics represent a significant enhancement over existing methods, which typically exhibit lower performance metrics. Additionally, by reviewing existing literature on gene interactions within the TME, the scGSL model proves to robustly identify biologically meaningful gene interactions in an unsupervised manner, validated by significant expression differences in key gene pairs across various cancers. The source code and data used in this paper can be found in https://github.com/LiYuechao1998/scGSL.",Yu-An Huang; Yue-Chao Li; Hai-Ru You; Jie Pan; Xiyue Cao; Xinyuan Li; Zhi-An Huang; Zhu-Hong You,,2025-02-04T18:28:25Z,http://arxiv.org/abs/2502.02629v1
2502.03569v2,Controllable Sequence Editing for Biological and Clinical Trajectories,"Conditional generation models for longitudinal sequences can generate new or modified trajectories given a conditioning input. While effective at generating entire sequences, these models typically lack control over the timing and scope of the edits. Most existing approaches either operate on univariate sequences or assume that the condition affects all variables and time steps. However, many scientific and clinical applications require more precise interventions, where a condition takes effect only after a specific time and influences only a subset of variables. We introduce CLEF, a controllable sequence editing model for conditional generation of immediate and delayed effects in multivariate longitudinal sequences. CLEF learns temporal concepts that encode how and when a condition alters future sequence evolution. These concepts allow CLEF to apply targeted edits to the affected time steps and variables while preserving the rest of the sequence. We evaluate CLEF on 6 datasets spanning cellular reprogramming and patient health trajectories, comparing against 9 state-of-the-art baselines. CLEF improves immediate sequence editing accuracy by up to 36.01% (MAE). Unlike prior models, CLEF enables one-step conditional generation at arbitrary future times, outperforming them in delayed sequence editing by up to 65.71% (MAE). We test CLEF under counterfactual inference assumptions and show up to 63.19% (MAE) improvement on zero-shot conditional generation of counterfactual trajectories. In a case study of patients with type 1 diabetes mellitus, CLEF identifies clinical interventions that generate realistic counterfactual trajectories shifted toward healthier outcomes.",Michelle M. Li; Kevin Li; Yasha Ektefaie; Ying Jin; Yepeng Huang; Shvat Messica; Tianxi Cai; Marinka Zitnik,,2025-02-05T19:33:12Z,http://arxiv.org/abs/2502.03569v2
2502.03938v1,Unravelling Causal Genetic Biomarkers of Alzheimer's Disease via Neuron   to Gene-token Backtracking in Neural Architecture: A Groundbreaking   Reverse-Gene-Finder Approach,"Alzheimer's Disease (AD) affects over 55 million people globally, yet the key genetic contributors remain poorly understood. Leveraging recent advancements in genomic foundation models, we present the innovative Reverse-Gene-Finder technology, a ground-breaking neuron-to-gene-token backtracking approach in a neural network architecture to elucidate the novel causal genetic biomarkers driving AD onset. Reverse-Gene-Finder comprises three key innovations. Firstly, we exploit the observation that genes with the highest probability of causing AD, defined as the most causal genes (MCGs), must have the highest probability of activating those neurons with the highest probability of causing AD, defined as the most causal neurons (MCNs). Secondly, we utilize a gene token representation at the input layer to allow each gene (known or novel to AD) to be represented as a discrete and unique entity in the input space. Lastly, in contrast to the existing neural network architectures, which track neuron activations from the input layer to the output layer in a feed-forward manner, we develop an innovative backtracking method to track backwards from the MCNs to the input layer, identifying the Most Causal Tokens (MCTs) and the corresponding MCGs. Reverse-Gene-Finder is highly interpretable, generalizable, and adaptable, providing a promising avenue for application in other disease scenarios.",Victor OK Li; Yang Han; Jacqueline CK Lam,,2025-02-06T10:24:02Z,http://arxiv.org/abs/2502.03938v1
2502.04684v3,G2PDiffusion: Cross-Species Genotype-to-Phenotype Prediction via   Evolutionary Diffusion,"Understanding how genes influence phenotype across species is a fundamental challenge in genetic engineering, which will facilitate advances in various fields such as crop breeding, conservation biology, and personalized medicine. However, current phenotype prediction models are limited to individual species and expensive phenotype labeling process, making the genotype-to-phenotype prediction a highly domain-dependent and data-scarce problem. To this end, we suggest taking images as morphological proxies, facilitating cross-species generalization through large-scale multimodal pretraining. We propose the first genotype-to-phenotype diffusion model (G2PDiffusion) that generates morphological images from DNA considering two critical evolutionary signals, i.e., multiple sequence alignments (MSA) and environmental contexts. The model contains three novel components: 1) a MSA retrieval engine that identifies conserved and co-evolutionary patterns; 2) an environment-aware MSA conditional encoder that effectively models complex genotype-environment interactions; and 3) an adaptive phenomic alignment module to improve genotype-phenotype consistency. Extensive experiments show that integrating evolutionary signals with environmental context enriches the model's understanding of phenotype variability across species, thereby offering a valuable and promising exploration into advanced AI-assisted genomic analysis.",Mengdi Liu; Zhangyang Gao; Hong Chang; Stan Z. Li; Shiguang Shan; Xilin Chen,,2025-02-07T06:16:31Z,http://arxiv.org/abs/2502.04684v3
2502.06253v3,Find Central Dogma Again: Leveraging Multilingual Transfer in Large   Language Models,"In recent years, large language models (LLMs) have achieved state-of-the-art results in various biological sequence analysis tasks, such as sequence classification, structure prediction, and function prediction. Similar to advancements in AI for other scientific fields, deeper research into biological LLMs has begun to focus on using these models to rediscover important existing biological laws or uncover entirely new patterns in biological sequences. This study leverages GPT-like LLMs to utilize language transfer capabilities to rediscover the genetic code rules of the central dogma. In our experimental design, we transformed the central dogma into a binary classification problem of aligning DNA sequences with protein sequences, where positive examples are matching DNA and protein sequences, and negative examples are non-matching pairs. We first trained a GPT-2 model from scratch using a dataset comprising protein sequences, DNA sequences, and sequences from languages such as English and Chinese. Subsequently, we fine-tuned the model using the natural language sentences similarity judgment dataset from PAWS-X. When tested on a dataset for DNA and protein sequence alignment judgment, the fine-tuned model achieved a classification accuracy of 81%. The study also analyzed factors contributing to this zero-shot capability, including model training stability and types of training data. This research demonstrates that LLMs can, through the transfer of natural language capabilities and solely relying on the analysis of sequences themselves, rediscover the central dogma without prior knowledge of it. This study bridges natural language and genetic language, opening a new door for AI-driven biological research.",Wang Liang,,2025-02-10T08:37:21Z,http://arxiv.org/abs/2502.06253v3
2502.07299v2,Life-Code: Central Dogma Modeling with Multi-Omics Sequence Unification,"The interactions between DNA, RNA, and proteins are fundamental to biological processes, as illustrated by the central dogma of molecular biology. Although modern biological pre-trained models have achieved great success in analyzing these macromolecules individually, their interconnected nature remains underexplored. This paper follows the guidance of the central dogma to redesign both the data and model pipeline and offers a comprehensive framework, Life-Code, that spans different biological functions. As for data flow, we propose a unified pipeline to integrate multi-omics data by reverse-transcribing RNA and reverse-translating amino acids into nucleotide-based sequences. As for the model, we design a codon tokenizer and a hybrid long-sequence architecture to encode the interactions between coding and non-coding regions through masked modeling pre-training. To model the translation and folding process with coding sequences, Life-Code learns protein structures of the corresponding amino acids by knowledge distillation from off-the-shelf protein language models. Such designs enable Life-Code to capture complex interactions within genetic sequences, providing a more comprehensive understanding of multi-omics with the central dogma. Extensive experiments show that Life-Code achieves state-of-the-art results on various tasks across three omics, highlighting its potential for advancing multi-omics analysis and interpretation.",Zicheng Liu; Siyuan Li; Zhiyuan Chen; Fang Wu; Chang Yu; Qirong Yang; Yucheng Guo; Yujie Yang; Xiaoming Zhang; Stan Z. Li,,2025-02-11T06:53:59Z,http://arxiv.org/abs/2502.07299v2
2502.07366v2,RITHMS : An advanced stochastic framework for the simulation of   transgenerational hologenomic data,"A holobiont is made up of a host organism together with its microbiota. In the context of animal breeding, as the holobiont can be viewed as the single unit upon which selection operates, integrating microbiota data into genomic prediction models may be a promising approach to improve predictions of phenotypic and genetic values. Nevertheless, there is a paucity of hologenomic transgenerational data to address this hypothesis, and thus to fill this gap, we propose a new simulation framework. Our approach, an R Implementation of a Transgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source package, builds upon the MoBPS package and incorporates distinctive characteristics of the microbiota, notably vertical and horizontal transmission as well as modulation due to the environment and host genetics. In addition, RITHMS can account for a variety of selection strategies and is adaptable to different genetic architectures. We simulated transgenerational hologenomic data using RITHMS under a wide variety of scenarios, varying heritability, microbiability, and microbiota heritability. We found that simulated data accurately reflected expected characteristics, notably based on microbial diversity metrics, correlation between taxa, modulation of vertical and horizontal transmission, response to environmental effects and the evolution of phenotypic values depending on selection strategy. Our results support the relevance of our simulation framework and illustrate its possible use for building a selection index balancing genetic gain and microbial diversity. RITHMS is an advanced, flexible tool for generating transgenerational hologenomic data that incorporate the complex interplay between genetics, microbiota and environment.",Solène Pety; Mahendra Mariadassou; Ingrid David; Andrea Rau,"MaIAGE, GABI; MaIAGE; GenPhySE; GABI",2025-02-11T08:38:49Z,http://arxiv.org/abs/2502.07366v2
2502.08149v2,Generalized Class Discovery in Instance Segmentation,"This work addresses the task of generalized class discovery (GCD) in instance segmentation. The goal is to discover novel classes and obtain a model capable of segmenting instances of both known and novel categories, given labeled and unlabeled data. Since the real world contains numerous objects with long-tailed distributions, the instance distribution for each class is inherently imbalanced. To address the imbalanced distributions, we propose an instance-wise temperature assignment (ITA) method for contrastive learning and class-wise reliability criteria for pseudo-labels. The ITA method relaxes instance discrimination for samples belonging to head classes to enhance GCD. The reliability criteria are to avoid excluding most pseudo-labels for tail classes when training an instance segmentation network using pseudo-labels from GCD. Additionally, we propose dynamically adjusting the criteria to leverage diverse samples in the early stages while relying only on reliable pseudo-labels in the later stages. We also introduce an efficient soft attention module to encode object-specific representations for GCD. Finally, we evaluate our proposed method by conducting experiments on two settings: COCO$_{half}$ + LVIS and LVIS + Visual Genome. The experimental results demonstrate that the proposed method outperforms previous state-of-the-art methods.",Cuong Manh Hoang; Yeejin Lee; Byeongkeun Kang,,2025-02-12T06:26:05Z,http://arxiv.org/abs/2502.08149v2
2502.08539v3,Anytime-valid FDR control with the stopped e-BH procedure,"The recent e-Benjamini-Hochberg (e-BH) procedure for multiple hypothesis testing is known to control the false discovery rate (FDR) under arbitrary dependence between the input e-values. This paper points out an important subtlety when applying the e-BH procedure with e-processes, which are sequential generalizations of e-values (where the data are observed sequentially). Since adaptively stopped e-processes are e-values, the e-BH procedure can be repeatedly applied at every time step, and one can continuously monitor the e-processes and the rejection sets obtained. One would hope that the ""stopped e-BH procedure"" (se-BH) has an FDR guarantee for the rejection set obtained at any stopping time. However, while this is true if the data in different streams are independent, it is not true in full generality, because each stopped e-process is an e-value only for stopping times in its own local filtration, but the se-BH procedure employs a stopping time with respect to a global filtration. This can cause information to leak across time, allowing one stream to know its future by knowing past data of another stream. This paper formulates a simple causal condition under which local e-processes are also global e-processes and thus the se-BH procedure does indeed control the FDR. The condition excludes unobserved confounding from the past and is met under most reasonable scenarios including genomics.",Hongjian Wang; Sanjit Dandapanthula; Aaditya Ramdas,,2025-02-12T16:23:44Z,http://arxiv.org/abs/2502.08539v3
2502.08746v1,Orthology and Near-Cographs in the Context of Phylogenetic Networks,"Orthologous genes, which arise through speciation, play a key role in comparative genomics and functional inference. In particular, graph-based methods allow for the inference of orthology estimates without prior knowledge of the underlying gene or species trees. This results in orthology graphs, where each vertex represents a gene, and an edge exists between two vertices if the corresponding genes are estimated to be orthologs. Orthology graphs inferred under a tree-like evolutionary model must be cographs. However, real-world data often deviate from this property, either due to noise in the data, errors in inference methods or, simply, because evolution follows a network-like rather than a tree-like process. The latter, in particular, raises the question of whether and how orthology graphs can be derived from or, equivalently, are explained by phylogenetic networks. Here, we study the constraints imposed on orthology graphs when the underlying evolutionary history follows a phylogenetic network instead of a tree. We show that any orthology graph can be represented by a sufficiently complex level-k network. However, such networks lack biologically meaningful constraints. In contrast, level-1 networks provide a simpler explanation, and we establish characterizations for level-1 explainable orthology graphs, i.e., those derived from level-1 evolutionary histories. To this end, we employ modular decomposition, a classical technique for studying graph structures. Specifically, an arbitrary graph is level-1 explainable if and only if each primitive subgraph is a near-cograph (a graph in which the removal of a single vertex results in a cograph). Additionally, we present a linear-time algorithm to recognize level-1 explainable orthology graphs and to construct a level-1 network that explains them, if such a network exists.",Anna Lindeberg; Guillaume E. Scholz; Nicolas Wieseke; Marc Hellmuth,,2025-02-12T19:36:38Z,http://arxiv.org/abs/2502.08746v1
2502.09686v1,Leveraging Machine Learning and Deep Learning Techniques for Improved   Pathological Staging of Prostate Cancer,"Prostate cancer (Pca) continues to be a leading cause of cancer-related mortality in men, and the limitations in precision of traditional diagnostic methods such as the Digital Rectal Exam (DRE), Prostate-Specific Antigen (PSA) testing, and biopsies underscore the critical importance of accurate staging detection in enhancing treatment outcomes and improving patient prognosis. This study leverages machine learning and deep learning approaches, along with feature selection and extraction methods, to enhance PCa pathological staging predictions using RNA sequencing data from The Cancer Genome Atlas (TCGA). Gene expression profiles from 486 tumors were analyzed using advanced algorithms, including Random Forest (RF), Logistic Regression (LR), Extreme Gradient Boosting (XGB), and Support Vector Machine (SVM). The performance of the study is measured with respect to the F1-score, as well as precision and recall, all of which are calculated as weighted averages. The results reveal that the highest test F1-score, approximately 83%, was achieved by the Random Forest algorithm, followed by Logistic Regression at 80%, while both Extreme Gradient Boosting (XGB) and Support Vector Machine (SVM) scored around 79%. Furthermore, deep learning models with data augmentation achieved an accuracy of 71. 23%, while PCA-based dimensionality reduction reached an accuracy of 69.86%. This research highlights the potential of AI-driven approaches in clinical oncology, paving the way for more reliable diagnostic tools that can ultimately improve patient outcomes.",Raziehsadat Ghalamkarian; Marziehsadat Ghalamkarian; MortezaAli Ahmadi; Sayed Mohammad Ahmadi; Abolfazl Diyanat,,2025-02-13T14:53:09Z,http://arxiv.org/abs/2502.09686v1
2502.11144v2,Consistency of heritability estimation from summary statistics in   high-dimensional linear models,"In Genome-Wide Association Studies (GWAS), heritability is defined as the fraction of variance of an outcome explained by a large number of genetic predictors in a high-dimensional polygenic linear model. This work studies the asymptotic properties of the most common estimator of heritability from summary statistics called linkage disequilibrium score (LDSC) regression, together with a simpler and closely related estimator called GWAS heritability (GWASH). These estimators are analyzed in their basic versions and under various modifications used in practice including weighting and standardization. We show that, with some variations, two conditions which we call weak dependence (WD) and bounded-kurtosis effects (BKE) are sufficient for consistency of both the basic LDSC with fixed intercept and GWASH estimators, for both Gaussian and non-Gaussian predictors. For Gaussian predictors it is shown that these conditions are also necessary for consistency of GWASH (with truncation) and simulations suggest that necessity holds too when the predictors are non-Gaussian. We also show that, with properly truncated weights, weighting does not change the consistency results, but standardization of the predictors and outcome, as done in practice, introduces bias in both LDSC and GWASH if the two essential conditions are violated. Finally, we show that, when population stratification is present, all the estimators considered are biased, and the bias is not remedied by using the LDSC regression estimator with free intercept, as originally suggested by the authors of that estimator.",David Azriel; Samuel Davenport; Armin Schwartzman,,2025-02-16T14:26:14Z,http://arxiv.org/abs/2502.11144v2
2502.11329v1,Differentially private fine-tuned NF-Net to predict GI cancer type,"Based on global genomic status, the cancer tumor is classified as Microsatellite Instable (MSI) and Microsatellite Stable (MSS). Immunotherapy is used to diagnose MSI, whereas radiation and chemotherapy are used for MSS. Therefore, it is significant to classify a gastro-intestinal (GI) cancer tumor into MSI vs. MSS to provide appropriate treatment. The existing literature showed that deep learning could directly predict the class of GI cancer tumors from histological images. However, deep learning (DL) models are susceptible to various threats, including membership inference attacks, model extraction attacks, etc. These attacks render the use of DL models impractical in real-world scenarios. To make the DL models useful and maintain privacy, we integrate differential privacy (DP) with DL. In particular, this paper aims to predict the state of GI cancer while preserving the privacy of sensitive data. We fine-tuned the Normalizer Free Net (NF-Net) model. We obtained an accuracy of 88.98\% without DP to predict (GI) cancer status. When we fine-tuned the NF-Net using DP-AdamW and adaptive DP-AdamW, we got accuracies of 74.58% and 76.48%, respectively. Moreover, we investigate the Weighted Random Sampler (WRS) and Class weighting (CW) to solve the data imbalance. We also evaluated and analyzed the DP algorithms in different settings.",Sai Venkatesh Chilukoti; Imran Hossen Md; Liqun Shan; Vijay Srinivas Tida; Xiali Hei,,2025-02-17T01:04:47Z,http://arxiv.org/abs/2502.11329v1
2502.12831v2,The gene's eye-view of quantitative genetics,"Modelling the evolution of a continuous trait in a biological population is one of the oldest problems in evolutionary biology, which led to the birth of quantitative genetics. With the recent development of GWAS methods, it has become essential to link the evolution of the trait distribution to the underlying evolution of allelic frequencies at many loci, co-contributing to the trait value. The way most articles go about this is to make assumptions on the trait distribution, and use Wright's formula to model how the evolution of the trait translates on each individual locus. Here, we take a gene's eye-view of the system, starting from an explicit finite-loci model with selection, drift, recombination and mutation, in which the trait value is a direct product of the genome. We let the number of loci go to infinity under the assumption of strong recombination, and characterize the limit behavior of a given locus with a McKean-Vlasov SDE and the corresponding Fokker-Planck IPDE. In words, the selection on a typical locus depends on the mean behaviour of the other loci which can be approximated with the law of the focal locus. Results include the independence of two loci and explicit stationary distribution for allelic frequencies at a given locus (under some assumptions on the fitness function).",Philibert Courau; Amaury Lambert; Emmanuel Schertzer,,2025-02-18T12:52:53Z,http://arxiv.org/abs/2502.12831v2
2502.15064v1,Pseudoinverse Diffusion Models for Generative CT Image Reconstruction   from Low Dose Data,"Score-based diffusion models have significantly advanced generative deep learning for image processing. Measurement conditioned models have also been applied to inverse problems such as CT reconstruction. However, the conventional approach, culminating in white noise, often requires a high number of reverse process update steps and score function evaluations. To address this limitation, we propose an alternative forward process in score-based diffusion models that aligns with the noise characteristics of low-dose CT reconstructions, rather than converging to white noise. This method significantly reduces the number of required score function evaluations, enhancing efficiency and maintaining familiar noise textures for radiologists, Our approach not only accelerates the generative process but also retains CT noise correlations, a key aspect often criticized by clinicians for deep learning reconstructions. In this work, we rigorously define a matrix-controlled stochastic process for this purpose and validate it through computational experiments. Using a dataset from The Cancer Genome Atlas Liver Hepatocellular Carcinoma (TCGA-LIHC), we simulate low-dose CT measurements and train our model, comparing it with a baseline scalar diffusion process and conditional diffusion model. Our results demonstrate the superiority of our pseudoinverse diffusion model in terms of efficiency and the ability to produce high-quality reconstructions that are familiar in texture to medical professionals in a low number of score function evaluations. This advancement paves the way for more efficient and clinically practical diffusion models in medical imaging, particularly beneficial in scenarios demanding rapid reconstructions or lower radiation exposure.",Matthew Tivnan; Dufan Wu; Quanzheng Li,,2025-02-20T21:56:15Z,http://arxiv.org/abs/2502.15064v1
2502.18639v2,Quantum Machine Learning in Precision Medicine and Drug Discovery -- A   Game Changer for Tailored Treatments?,"The digitization of healthcare presents numerous challenges, including the complexity of biological systems, vast data generation, and the need for personalized treatment plans. Traditional computational methods often fall short, leading to delayed and sometimes ineffective diagnoses and treatments. Quantum Computing (QC) and Quantum Machine Learning (QML) offer transformative advancements with the potential to revolutionize medicine. This paper summarizes areas where QC promises unprecedented computational power, enabling faster, more accurate diagnostics, personalized treatments, and enhanced drug discovery processes. However, integrating quantum technologies into precision medicine also presents challenges, including errors in algorithms and high costs. We show that mathematically-based techniques for specifying, developing, and verifying software (formal methods) can enhance the reliability and correctness of QC. By providing a rigorous mathematical framework, formal methods help to specify, develop, and verify systems with high precision. In genomic data analysis, formal specification languages can precisely (1) define the behavior and properties of quantum algorithms designed to identify genetic markers associated with diseases. Model checking tools can systematically explore all possible states of the algorithm to (2) ensure it behaves correctly under all conditions, while theorem proving techniques provide mathematical (3) proof that the algorithm meets its specified properties, ensuring accuracy and reliability. Additionally, formal optimization techniques can (4) enhance the efficiency and performance of quantum algorithms by reducing resource usage, such as the number of qubits and gate operations. Therefore, we posit that formal methods can significantly contribute to enabling QC to realize its full potential as a game changer in precision medicine.",Markus Bertl; Alan Mott; Salvatore Sinno; Bhavika Bhalgamiya,,2025-02-25T20:59:22Z,http://arxiv.org/abs/2502.18639v2
2502.18674v1,bayesNMF: Fast Bayesian Poisson NMF with Automatically Learned Rank   Applied to Mutational Signatures,"Bayesian Non-Negative Matrix Factorization (NMF) is a method of interest across fields including genomics, neuroscience, and audio and image processing. Bayesian Poisson NMF is of particular importance for counts data, for example in cancer mutational signatures analysis. However, MCMC methods for Bayesian Poisson NMF require a computationally intensive augmentation. Further, identifying latent rank is necessary, but commonly used heuristic approaches are slow and potentially subjective, and methods that learn rank automatically are unable to provide posterior uncertainties. In this paper, we introduce bayesNMF, a computationally efficient Gibbs sampler for Bayesian Poisson NMF. The desired Poisson-likelihood NMF is paired with a Normal-likelihood NMF used for high overlap proposal distributions in approximate Metropolis steps, avoiding augmentation. We additionally define Bayesian factor inclusion (BFI) and sparse Bayesian factor inclusion (SBFI) as methods to identify rank automatically while preserving posterior uncertainty quantification on the learned matrices. We provide an open-source R software package with all models and plotting capabilities demonstrated in this paper on GitHub at jennalandy/bayesNMF. While our applications focus on mutational signatures, our software and results can be extended to any use of Bayesian Poisson NMF.",Jenna M. Landy; Nishanth Basava; Giovanni Parmigiani,,2025-02-25T22:14:39Z,http://arxiv.org/abs/2502.18674v1
2503.02781v1,Multimodal AI predicts clinical outcomes of drug combinations from   preclinical data,"Predicting clinical outcomes from preclinical data is essential for identifying safe and effective drug combinations. Current models rely on structural or target-based features to identify high-efficacy, low-toxicity drug combinations. However, these approaches fail to incorporate the multimodal data necessary for accurate, clinically-relevant predictions. Here, we introduce MADRIGAL, a multimodal AI model that learns from structural, pathway, cell viability, and transcriptomic data to predict drug combination effects across 953 clinical outcomes and 21842 compounds, including combinations of approved drugs and novel compounds in development. MADRIGAL uses a transformer bottleneck module to unify preclinical drug data modalities while handling missing data during training and inference--a major challenge in multimodal learning. It outperforms single-modality methods and state-of-the-art models in predicting adverse drug interactions. MADRIGAL performs virtual screening of anticancer drug combinations and supports polypharmacy management for type II diabetes and metabolic dysfunction-associated steatohepatitis (MASH). It identifies transporter-mediated drug interactions. MADRIGAL predicts resmetirom, the first and only FDA-approved drug for MASH, among therapies with the most favorable safety profile. It supports personalized cancer therapy by integrating genomic profiles from cancer patients. Using primary acute myeloid leukemia samples and patient-derived xenograft models, it predicts the efficacy of personalized drug combinations. Integrating MADRIGAL with a large language model allows users to describe clinical outcomes in natural language, improving safety assessment by identifying potential adverse interactions and toxicity risks. MADRIGAL provides a multimodal approach for designing combination therapies with improved predictive accuracy and clinical relevance.",Yepeng Huang; Xiaorui Su; Varun Ullanat; Ivy Liang; Lindsay Clegg; Damilola Olabode; Nicholas Ho; Bino John; Megan Gibbs; Marinka Zitnik,,2025-03-04T16:55:14Z,http://arxiv.org/abs/2503.02781v1
2503.03485v1,TEDDY: A Family Of Foundation Models For Understanding Single Cell   Biology,"Understanding the biological mechanism of disease is critical for medicine, and in particular drug discovery. AI-powered analysis of genome-scale biological data hold great potential in this regard. The increasing availability of single-cell RNA sequencing data has enabled the development of large foundation models for disease biology. However, existing foundation models either do not improve or only modestly improve over task-specific models in downstream applications. Here, we explored two avenues for improving the state-of-the-art. First, we scaled the pre-training dataset to 116 million cells, which is larger than those used by previous models. Second, we leveraged the availability of large-scale biological annotations as a form of supervision during pre-training. We trained the TEDDY family of models comprising six transformer-based state-of-the-art single-cell foundation models with 70 million, 160 million, and 400 million parameters. We vetted our models on two downstream evaluation tasks -- identifying the underlying disease state of held-out donors not seen during training and distinguishing healthy cells from diseased ones for disease conditions and donors not seen during training. Scaling experiments showed that performance improved predictably with both data volume and parameter count. Our models showed substantial improvement over existing work on the first task and more muted improvements on the second.",Alexis Chevalier; Soumya Ghosh; Urvi Awasthi; James Watkins; Julia Bieniewska; Nichita Mitrea; Olga Kotova; Kirill Shkura; Andrew Noble; Michael Steinbaugh; Julien Delile; Christoph Meier; Leonid Zhukov; Iya Khalil; Srayanta Mukherjee; Judith Mueller,,2025-03-05T13:24:57Z,http://arxiv.org/abs/2503.03485v1
2503.03837v1,"Materials Graph Library (MatGL), an open-source graph deep learning   library for materials science and chemistry","Graph deep learning models, which incorporate a natural inductive bias for a collection of atoms, are of immense interest in materials science and chemistry. Here, we introduce the Materials Graph Library (MatGL), an open-source graph deep learning library for materials science and chemistry. Built on top of the popular Deep Graph Library (DGL) and Python Materials Genomics (Pymatgen) packages, our intention is for MatGL to be an extensible ``batteries-included'' library for the development of advanced graph deep learning models for materials property predictions and interatomic potentials. At present, MatGL has efficient implementations for both invariant and equivariant graph deep learning models, including the Materials 3-body Graph Network (M3GNet), MatErials Graph Network (MEGNet), Crystal Hamiltonian Graph Network (CHGNet), TensorNet and SO3Net architectures. MatGL also includes a variety of pre-trained universal interatomic potentials (aka ``foundational materials models (FMM)'') and property prediction models are also included for out-of-box usage, benchmarking and fine-tuning. Finally, MatGL includes support for Pytorch Lightning for rapid training of models.",Tsz Wai Ko; Bowen Deng; Marcel Nassar; Luis Barroso-Luque; Runze Liu; Ji Qi; Elliott Liu; Gerbrand Ceder; Santiago Miret; Shyue Ping Ong,,2025-03-05T19:03:21Z,http://arxiv.org/abs/2503.03837v1
2503.05448v1,Joint graphical model estimation using Stein-type shrinkage for fast   large scale network inference in scRNAseq data,"Graphical modeling is a widely used tool for analyzing conditional dependencies between variables and traditional methods may struggle to capture shared and distinct structures in multi-group or multi-condition settings. Joint graphical modeling (JGM) extends this framework by simultaneously estimating network structures across multiple related datasets, allowing for a deeper understanding of commonalities and differences. This capability is particularly valuable in fields such as genomics and neuroscience, where identifying variations in network topology can provide critical biological insights. Existing JGM methodologies largely fall into two categories: regularization-based approaches, which introduce additional penalties to enforce structured sparsity, and Bayesian frameworks, which incorporate prior knowledge to improve network inference. In this study, we explore an alternative method based on two-target linear covariance matrix shrinkage. Formula for optimal shrinkage intensities is proposed which leads to the development of JointStein framework. Performance of JointStein framework is proposed through simulation benchmarking which demonstrates its effectiveness for large-scale single-cell RNA sequencing (scRNA-seq) data analysis. Finally, we apply our approach to glioblastoma scRNA-seq data, uncovering dynamic shifts in T cell network structures across disease progression stages. The result highlights potential of JointStein framework in extracting biologically meaningful insights from high-dimensional data.",Duong H. T. Vo; Nelofer Syed; Thomas Thorne,,2025-03-07T14:18:21Z,http://arxiv.org/abs/2503.05448v1
2503.10726v1,Prototype-Guided Cross-Modal Knowledge Enhancement for Adaptive Survival   Prediction,"Histo-genomic multimodal survival prediction has garnered growing attention for its remarkable model performance and potential contributions to precision medicine. However, a significant challenge in clinical practice arises when only unimodal data is available, limiting the usability of these advanced multimodal methods. To address this issue, this study proposes a prototype-guided cross-modal knowledge enhancement (ProSurv) framework, which eliminates the dependency on paired data and enables robust learning and adaptive survival prediction. Specifically, we first introduce an intra-modal updating mechanism to construct modality-specific prototype banks that encapsulate the statistics of the whole training set and preserve the modality-specific risk-relevant features/prototypes across intervals. Subsequently, the proposed cross-modal translation module utilizes the learned prototypes to enhance knowledge representation for multimodal inputs and generate features for missing modalities, ensuring robust and adaptive survival prediction across diverse scenarios. Extensive experiments on four public datasets demonstrate the superiority of ProSurv over state-of-the-art methods using either unimodal or multimodal input, and the ablation study underscores its feasibility for broad applicability. Overall, this study addresses a critical practical challenge in computational pathology, offering substantial significance and potential impact in the field.",Fengchun Liu; Linghan Cai; Zhikang Wang; Zhiyuan Fan; Jin-gang Yu; Hao Chen; Yongbing Zhang,,2025-03-13T11:38:11Z,http://arxiv.org/abs/2503.10726v1
2503.12286v1,Integrating Chain-of-Thought and Retrieval Augmented Generation Enhances   Rare Disease Diagnosis from Clinical Notes,"Background: Several studies show that large language models (LLMs) struggle with phenotype-driven gene prioritization for rare diseases. These studies typically use Human Phenotype Ontology (HPO) terms to prompt foundation models like GPT and LLaMA to predict candidate genes. However, in real-world settings, foundation models are not optimized for domain-specific tasks like clinical diagnosis, yet inputs are unstructured clinical notes rather than standardized terms. How LLMs can be instructed to predict candidate genes or disease diagnosis from unstructured clinical notes remains a major challenge. Methods: We introduce RAG-driven CoT and CoT-driven RAG, two methods that combine Chain-of-Thought (CoT) and Retrieval Augmented Generation (RAG) to analyze clinical notes. A five-question CoT protocol mimics expert reasoning, while RAG retrieves data from sources like HPO and OMIM (Online Mendelian Inheritance in Man). We evaluated these approaches on rare disease datasets, including 5,980 Phenopacket-derived notes, 255 literature-based narratives, and 220 in-house clinical notes from Childrens Hospital of Philadelphia. Results: We found that recent foundations models, including Llama 3.3-70B-Instruct and DeepSeek-R1-Distill-Llama-70B, outperformed earlier versions such as Llama 2 and GPT-3.5. We also showed that RAG-driven CoT and CoT-driven RAG both outperform foundation models in candidate gene prioritization from clinical notes; in particular, both methods with DeepSeek backbone resulted in a top-10 gene accuracy of over 40% on Phenopacket-derived clinical notes. RAG-driven CoT works better for high-quality notes, where early retrieval can anchor the subsequent reasoning steps in domain-specific evidence, while CoT-driven RAG has advantage when processing lengthy and noisy notes.",Da Wu; Zhanliang Wang; Quan Nguyen; Kai Wang,,2025-03-15T22:57:31Z,http://arxiv.org/abs/2503.12286v1
2503.12330v1,Computational identification of ketone metabolism as a key regulator of   sleep stability and circadian dynamics via real-time metabolic profiling,"Metabolism plays a crucial role in sleep regulation, yet its effects are challenging to track in real time. This study introduces a machine learning-based framework to analyze sleep patterns and identify how metabolic changes influence sleep at specific time points. We first established that sleep periods in Drosophila melanogaster function independently, with no causal relationship between different sleep episodes. Using gradient boosting models and explainable artificial intelligence techniques, we quantified the influence of time-dependent sleep features. Causal inference and autocorrelation analyses further confirmed that sleep states at different times are statistically independent, providing a robust foundation for exploring metabolic effects on sleep. Applying this framework to flies with altered monocarboxylate transporter 2 expression, we found that changes in ketone transport modified sleep stability and disrupted transitions between day and night sleep. In an Alzheimers disease model, metabolic interventions such as beta hydroxybutyrate supplementation and intermittent fasting selectively influenced the timing of day to night transitions rather than uniformly altering sleep duration. Autoencoder based similarity scoring and wavelet analysis reinforced that metabolic effects on sleep were highly time dependent. This study presents a novel approach to studying sleep-metabolism interactions, revealing that metabolic states exert their strongest influence at distinct time points, shaping sleep stability and circadian transitions.",Hao Huang; Kaijing Xu; Michael Lardelli,,2025-03-16T02:57:32Z,http://arxiv.org/abs/2503.12330v1
2503.13925v1,Reconstructing Cell Lineage Trees from Phenotypic Features with Metric   Learning,"How a single fertilized cell gives rise to a complex array of specialized cell types in development is a central question in biology. The cells grow, divide, and acquire differentiated characteristics through poorly understood molecular processes. A key approach to studying developmental processes is to infer the tree graph of cell lineage division and differentiation histories, providing an analytical framework for dissecting individual cells' molecular decisions during replication and differentiation. Although genetically engineered lineage-tracing methods have advanced the field, they are either infeasible or ethically constrained in many organisms. In contrast, modern single-cell technologies can measure high-content molecular profiles (e.g., transcriptomes) in a wide range of biological systems.   Here, we introduce CellTreeQM, a novel deep learning method based on transformer architectures that learns an embedding space with geometric properties optimized for tree-graph inference. By formulating lineage reconstruction as a tree-metric learning problem, we have systematically explored supervised, weakly supervised, and unsupervised training settings and present a Lineage Reconstruction Benchmark to facilitate comprehensive evaluation of our learning method. We benchmarked the method on (1) synthetic data modeled via Brownian motion with independent noise and spurious signals and (2) lineage-resolved single-cell RNA sequencing datasets. Experimental results show that CellTreeQM recovers lineage structures with minimal supervision and limited data, offering a scalable framework for uncovering cell lineage relationships in challenging animal models. To our knowledge, this is the first method to cast cell lineage inference explicitly as a metric learning task, paving the way for future computational models aimed at uncovering the molecular dynamics of cell lineage.",Da Kuang; Guanwen Qiu; Junhyong Kim,,2025-03-18T05:41:03Z,http://arxiv.org/abs/2503.13925v1
2503.21124v1,AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction,"The integration of pathologic images and genomic data for survival analysis has gained increasing attention with advances in multimodal learning. However, current methods often ignore biological characteristics, such as heterogeneity and sparsity, both within and across modalities, ultimately limiting their adaptability to clinical practice. To address these challenges, we propose AdaMHF: Adaptive Multimodal Hierarchical Fusion, a framework designed for efficient, comprehensive, and tailored feature extraction and fusion. AdaMHF is specifically adapted to the uniqueness of medical data, enabling accurate predictions with minimal resource consumption, even under challenging scenarios with missing modalities. Initially, AdaMHF employs an experts expansion and residual structure to activate specialized experts for extracting heterogeneous and sparse features. Extracted tokens undergo refinement via selection and aggregation, reducing the weight of non-dominant features while preserving comprehensive information. Subsequently, the encoded features are hierarchically fused, allowing multi-grained interactions across modalities to be captured. Furthermore, we introduce a survival prediction benchmark designed to resolve scenarios with missing modalities, mirroring real-world clinical conditions. Extensive experiments on TCGA datasets demonstrate that AdaMHF surpasses current state-of-the-art (SOTA) methods, showcasing exceptional performance in both complete and incomplete modality settings.",Shuaiyu Zhang; Xun Lin; Rongxiang Zhang; Yu Bai; Yong Xu; Tao Tan; Xunbin Zheng; Zitong Yu,,2025-03-27T03:27:55Z,http://arxiv.org/abs/2503.21124v1
2503.21274v2,Integrated data-driven biotechnology research environments,"In the past few decades, the life sciences have experienced an unprecedented accumulation of data, ranging from genomic sequences and proteomic profiles to heavy-content imaging, clinical assays, and commercial biological products for research. Traditional static databases have been invaluable in providing standardized and structured information. However, they fall short when it comes to facilitating exploratory data interrogation, real-time query, multidimensional comparison and dynamic visualization. Integrated data-driven research environments aiming at supporting user-driven data queries and visualization offer promising new avenues for making the best use of the vast and heterogeneous data streams collected in biological research. This perspective article discusses the potential of interactive and integrated frameworks, highlighting the importance of implementing this model in biotechnology research, while going through the state-of-the-art in database design, technical choices behind modern data management systems, and emerging needs in multidisciplinary research. Special attention is given to data interrogation strategies, user interface design, and comparative analysis capabilities, along with challenges such as data standardization and scalability in data-heavy applications. Conceptual features for developing interactive data environments along diverse life science domains are then presented in the user case of cell line selection for in vitro research to bridge the gap between research data generation, actionable biological insight, experimental design, and clinical relevance.",Rosalia Moreddu,,2025-03-27T08:54:01Z,http://arxiv.org/abs/2503.21274v2
2503.22467v2,An integrated method for clustering and association network inference,"High dimensional Gaussian graphical models provide a rigorous framework to describe a network of statistical dependencies between entities, such as genes in genomic regulation studies or species in ecology. Penalized methods, including the standard Graphical-Lasso, are well-known approaches to infer the parameters of these models. As the number of variables in the model (of entities in the network) grow, the network inference and interpretation become more complex. The Normal-Block model is introduced, a new model that clusters variables and consider a network at the cluster level. Normal-Block both adds structure to the network and reduces its size. The approach builds on Graphical-Lasso to add a penalty on the network's edges and limit the detection of spurious dependencies. A zero-inflated version of the model is also proposed to account for real-world data properties. For the inference procedure, two approaches are introduced, a straightforward method based on state-of-the-art approaches and an original, more rigorous method that simultaneously infers the clustering of variables and the association network between clusters, using a penalized variational Expectation-Maximization approach. An implementation of the model in R, in a package called \textbf{normalblockr}, is available on github\footnote{https://github.com/jeannetous/normalblockr}. The results of the models in terms of clustering and network inference are presented, using both simulated data and various types of real-world data (proteomics and words occurrences on webpages).",Jeanne Tous; Julien Chiquet,,2025-03-28T14:27:54Z,http://arxiv.org/abs/2503.22467v2
2503.22939v3,Interpretable Graph Kolmogorov-Arnold Networks for Multi-Cancer   Classification and Biomarker Identification using Multi-Omics Data,"The integration of heterogeneous multi-omics datasets at a systems level remains a central challenge for developing analytical and computational models in precision cancer diagnostics. This paper introduces Multi-Omics Graph Kolmogorov-Arnold Network (MOGKAN), a deep learning framework that utilizes messenger-RNA, micro-RNA sequences, and DNA methylation samples together with Protein-Protein Interaction (PPI) networks for cancer classification across 31 different cancer types. The proposed approach combines differential gene expression with DESeq2, Linear Models for Microarray (LIMMA), and Least Absolute Shrinkage and Selection Operator (LASSO) regression to reduce multi-omics data dimensionality while preserving relevant biological features. The model architecture is based on the Kolmogorov-Arnold theorem principle and uses trainable univariate functions to enhance interpretability and feature analysis. MOGKAN achieves classification accuracy of 96.28 percent and exhibits low experimental variability in comparison to related deep learning-based models. The biomarkers identified by MOGKAN were validated as cancer-related markers through Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analysis. By integrating multi-omics data with graph-based deep learning, our proposed approach demonstrates robust predictive performance and interpretability with potential to enhance the translation of complex multi-omics data into clinically actionable cancer diagnostics.",Fadi Alharbi; Nishant Budhiraja; Aleksandar Vakanski; Boyu Zhang; Murtada K. Elbashir; Harshith Guduru; Mohanad Mohammed,,2025-03-29T02:14:05Z,http://arxiv.org/abs/2503.22939v3
2504.00036v1,Improving Diseases Predictions Utilizing External Bio-Banks,"Machine learning has been successfully used in critical domains, such as medicine. However, extracting meaningful insights from biomedical data is often constrained by the lack of their available disease labels. In this research, we demonstrate how machine learning can be leveraged to enhance explainability and uncover biologically meaningful associations, even when predictive improvements in disease modeling are limited. We train LightGBM models from scratch on our dataset (10K) to impute metabolomics features and apply them to the UK Biobank (UKBB) for downstream analysis. The imputed metabolomics features are then used in survival analysis to assess their impact on disease-related risk factors. As a result, our approach successfully identified biologically relevant connections that were not previously known to the predictive models. Additionally, we applied a genome-wide association study (GWAS) on key metabolomics features, revealing a link between vascular dementia and smoking. Although being a well-established epidemiological relationship, this link was not embedded in the model's training data, which validated the method's ability to extract meaningful signals. Furthermore, by integrating survival models as inputs in the 10K data, we uncovered associations between metabolic substances and obesity, demonstrating the ability to infer disease risk for future patients without requiring direct outcome labels. These findings highlight the potential of leveraging external bio-banks to extract valuable biomedical insights, even in data-limited scenarios. Our results demonstrate that machine learning models trained on smaller datasets can still be used to uncover real biological associations when carefully integrated with survival analysis and genetic studies.",Hido Pinto; Eran Segal,,2025-03-30T13:05:20Z,http://arxiv.org/abs/2504.00036v1
2504.00844v1,PRISM-0: A Predicate-Rich Scene Graph Generation Framework for Zero-Shot   Open-Vocabulary Tasks,"In Scene Graphs Generation (SGG) one extracts structured representation from visual inputs in the form of objects nodes and predicates connecting them. This facilitates image-based understanding and reasoning for various downstream tasks. Although fully supervised SGG approaches showed steady performance improvements, they suffer from a severe training bias. This is caused by the availability of only small subsets of curated data and exhibits long-tail predicate distribution issues with a lack of predicate diversity adversely affecting downstream tasks. To overcome this, we introduce PRISM-0, a framework for zero-shot open-vocabulary SGG that bootstraps foundation models in a bottom-up approach to capture the whole spectrum of diverse, open-vocabulary predicate prediction. Detected object pairs are filtered and passed to a Vision Language Model (VLM) that generates descriptive captions. These are used to prompt an LLM to generate fine-andcoarse-grained predicates for the pair. The predicates are then validated using a VQA model to provide a final SGG. With the modular and dataset-independent PRISM-0, we can enrich existing SG datasets such as Visual Genome (VG). Experiments illustrate that PRIMS-0 generates semantically meaningful graphs that improve downstream tasks such as Image Captioning and Sentence-to-Graph Retrieval with a performance on par to the best fully supervised methods.",Abdelrahman Elskhawy; Mengze Li; Nassir Navab; Benjamin Busam,,2025-04-01T14:29:51Z,http://arxiv.org/abs/2504.00844v1
2504.01270v1,"Defining the relationship between cathepsin B and esophageal   adenocarcinoma: conjoint analysis of Mendelian randomization,   transcriptome-wide association studies, and single-cell RNA sequencing data","Background: Esophageal cancer poses a significant global health challenge, with the incidence of esophageal adenocarcinoma (EAC), a predominant subtype, increasing notably in Western countries. Cathepsins, a family of lysosomal proteolytic enzymes, have been implicated in the progression of various tumors. However, the causal relationship between the cathepsin family and EAC remains unresolved. Methods: To evaluate these potential causal associations, integrative analyses were conducted, integrating Mendelian randomization (MR), transcriptome-wide association study (TWAS), single-cell RNA sequencing (scRNA-seq), and single-cell expression quantitative trait locus (sc-eQTL) analyses. Results: Univariable and multivariable MR analyses demonstrated that elevated levels of cathepsin B (CTSB) were associated with a reduced risk of EAC. The TWAS analysis identified a negative association between CTSB expression in esophageal tissue and EAC, consistent with experimental validation using immunohistochemistry. The scRNA-seq data analysis indicated that CTSB expression was predominantly localized in macrophages infiltrating EAC. Colocalization analysis incorporating sc-eQTL data specific to macrophages confirmed a shared causal variant between CTSB and macrophages. Additionally, MR analysis of CTSB and macrophage scavenger receptor (MSR) types I and II established their interrelationship, suggesting that CTSB may influence the proinflammatory phenotype of macrophages, ultimately affecting EAC risk. Conclusions: This integrative analysis, utilizing MR, TWAS, scRNA-seq, and sc-eQTL data, identified a significant causal association between CTSB and EAC, potentially mediated through macrophage MSR regulation. These findings suggest that targeting cathepsin B could represent a novel strategy for the diagnosis and treatment of EAC.",Jialin Li; Shaokang Yang; Xinliang Gao; Mingbo Tang; Xiaobo Ma; Suyan Tian; Wei Liu,,2025-04-02T00:41:37Z,http://arxiv.org/abs/2504.01270v1
2504.03733v1,Artificial Intelligence and Deep Learning Algorithms for Epigenetic   Sequence Analysis: A Review for Epigeneticists and AI Experts,"Epigenetics encompasses mechanisms that can alter the expression of genes without changing the underlying genetic sequence. The epigenetic regulation of gene expression is initiated and sustained by several mechanisms such as DNA methylation, histone modifications, chromatin conformation, and non-coding RNA. The changes in gene regulation and expression can manifest in the form of various diseases and disorders such as cancer and congenital deformities. Over the last few decades, high throughput experimental approaches have been used to identify and understand epigenetic changes, but these laboratory experimental approaches and biochemical processes are time-consuming and expensive. To overcome these challenges, machine learning and artificial intelligence (AI) approaches have been extensively used for mapping epigenetic modifications to their phenotypic manifestations. In this paper we provide a narrative review of published research on AI models trained on epigenomic data to address a variety of problems such as prediction of disease markers, gene expression, enhancer promoter interaction, and chromatin states. The purpose of this review is twofold as it is addressed to both AI experts and epigeneticists. For AI researchers, we provided a taxonomy of epigenetics research problems that can benefit from an AI-based approach. For epigeneticists, given each of the above problems we provide a list of candidate AI solutions in the literature. We have also identified several gaps in the literature, research challenges, and recommendations to address these challenges.",Muhammad Tahir; Mahboobeh Norouzi; Shehroz S. Khan; James R. Davie; Soichiro Yamanaka; Ahmed Ashraf,,2025-04-01T01:02:34Z,http://arxiv.org/abs/2504.03733v1
2504.04749v2,Vision Transformers with Autoencoders and Explainable AI for Cancer   Patient Risk Stratification Using Whole Slide Imaging,"Cancer remains one of the leading causes of mortality worldwide, necessitating accurate diagnosis and prognosis. Whole Slide Imaging (WSI) has become an integral part of clinical workflows with advancements in digital pathology. While various studies have utilized WSIs, their extracted features may not fully capture the most relevant pathological information, and their lack of interpretability limits clinical adoption.   In this paper, we propose PATH-X, a framework that integrates Vision Transformers (ViT) and Autoencoders with SHAP (Shapley Additive Explanations) to enhance model explainability for patient stratification and risk prediction using WSIs from The Cancer Genome Atlas (TCGA). A representative image slice is selected from each WSI, and numerical feature embeddings are extracted using Google's pre-trained ViT. These features are then compressed via an autoencoder and used for unsupervised clustering and classification tasks. Kaplan-Meier survival analysis is applied to evaluate stratification into two and three risk groups. SHAP is used to identify key contributing features, which are mapped onto histopathological slices to provide spatial context.   PATH-X demonstrates strong performance in breast and glioma cancers, where a sufficient number of WSIs enabled robust stratification. However, performance in lung cancer was limited due to data availability, emphasizing the need for larger datasets to enhance model reliability and clinical applicability.",Ahmad Hussein; Mukesh Prasad; Ali Anaissi; Ali Braytee,,2025-04-07T05:48:42Z,http://arxiv.org/abs/2504.04749v2
2504.05403v1,A Novel Approach to Linking Histology Images with DNA Methylation,"DNA methylation is an epigenetic mechanism that regulates gene expression by adding methyl groups to DNA. Abnormal methylation patterns can disrupt gene expression and have been linked to cancer development. To quantify DNA methylation, specialized assays are typically used. However, these assays are often costly and have lengthy processing times, which limits their widespread availability in routine clinical practice. In contrast, whole slide images (WSIs) for the majority of cancer patients can be more readily available. As such, given the ready availability of WSIs, there is a compelling need to explore the potential relationship between WSIs and DNA methylation patterns. To address this, we propose an end-to-end graph neural network based weakly supervised learning framework to predict the methylation state of gene groups exhibiting coherent patterns across samples. Using data from three cohorts from The Cancer Genome Atlas (TCGA) - TCGA-LGG (Brain Lower Grade Glioma), TCGA-GBM (Glioblastoma Multiforme) ($n$=729) and TCGA-KIRC (Kidney Renal Clear Cell Carcinoma) ($n$=511) - we demonstrate that the proposed approach achieves significantly higher AUROC scores than the state-of-the-art (SOTA) methods, by more than $20\%$. We conduct gene set enrichment analyses on the gene groups and show that majority of the gene groups are significantly enriched in important hallmarks and pathways. We also generate spatially enriched heatmaps to further investigate links between histological patterns and DNA methylation states. To the best of our knowledge, this is the first study that explores association of spatially resolved histological patterns with gene group methylation states across multiple cancer types using weakly supervised deep learning.",Manahil Raza; Muhammad Dawood; Talha Qaiser; Nasir M. Rajpoot,,2025-04-07T18:19:01Z,http://arxiv.org/abs/2504.05403v1
2504.05454v1,GraphPINE: Graph Importance Propagation for Interpretable Drug Response   Prediction,"Explainability is necessary for many tasks in biomedical research. Recent explainability methods have focused on attention, gradient, and Shapley value. These do not handle data with strong associated prior knowledge and fail to constrain explainability results based on known relationships between predictive features.   We propose GraphPINE, a graph neural network (GNN) architecture leveraging domain-specific prior knowledge to initialize node importance optimized during training for drug response prediction. Typically, a manual post-prediction step examines literature (i.e., prior knowledge) to understand returned predictive features. While node importance can be obtained for gradient and attention after prediction, node importance from these methods lacks complementary prior knowledge; GraphPINE seeks to overcome this limitation. GraphPINE differs from other GNN gating methods by utilizing an LSTM-like sequential format. We introduce an importance propagation layer that unifies 1) updates for feature matrix and node importance and 2) uses GNN-based graph propagation of feature values. This initialization and updating mechanism allows for informed feature learning and improved graph representation.   We apply GraphPINE to cancer drug response prediction using drug screening and gene data collected for over 5,000 gene nodes included in a gene-gene graph with a drug-target interaction (DTI) graph for initial importance. The gene-gene graph and DTIs were obtained from curated sources and weighted by article count discussing relationships between drugs and genes. GraphPINE achieves a PR-AUC of 0.894 and ROC-AUC of 0.796 across 952 drugs. Code is available at https://anonymous.4open.science/r/GraphPINE-40DE.",Yoshitaka Inoue; Tianfan Fu; Augustin Luna,,2025-04-07T19:42:12Z,http://arxiv.org/abs/2504.05454v1
2504.07734v1,"On-Chip and Off-Chip TIA Amplifiers for Nanopore Signal Readout Design,   Performance and Challenges: A Review","Advancements in biomedical research have driven continuous innovations in sensing and diagnostic technologies. Among these, nanopore based single molecule sensing and sequencing is rapidly emerging as a powerful and versatile sensing methodology. Advancements in nanopore based approaches require concomitant improvements in the electronic readout methods employed, from the point of low noise, bandwidth and form factor. This article focuses on current sensing circuits designed and employed for ultra low noise nanopore signal readout, addressing the fundamental limitations of traditional off chip transimpedance amplifiers (TIAs), which suffer from high input parasitic capacitance, bandwidth constraints, and increased noise at high frequencies. This review explores the latest design schemes and circuit structures classified into on-chip and off-chip TIA designs, highlighting their design implementation, performance, respective challenges and explores the interplay between noise performance, capacitance, and bandwidth across diverse transimpedance amplifier (TIA) configurations. Emphasis is placed on characterizing noise response under varying parasitic capacitance and operational frequencies, a systematic evaluation not extensively addressed in prior literature while also considering the allowable input current compliance range limitations. The review also compares the widely used Axopatch 200B system to the designs reported in literature. The findings offer valuable insights into optimizing TIA designs for enhanced signal integrity in high speed and high sensitivity applications focusing on noise reduction, impedance matching, DC blocking, and offset cancellation techniques.",K. Ashoka Deepthi; Manoj Varma; Arup Polley,,2025-04-10T13:29:08Z,http://arxiv.org/abs/2504.07734v1
2504.07881v2,An LLM-Driven Multi-Agent Debate System for Mendelian Diseases,"Accurate diagnosis of Mendelian diseases is crucial for precision therapy and assistance in preimplantation genetic diagnosis. However, existing methods often fall short of clinical standards or depend on extensive datasets to build pretrained machine learning models. To address this, we introduce an innovative LLM-Driven multi-agent debate system (MD2GPS) with natural language explanations of the diagnostic results. It utilizes a language model to transform results from data-driven and knowledge-driven agents into natural language, then fostering a debate between these two specialized agents. This system has been tested on 1,185 samples across four independent datasets, enhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a challenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in 12 patients, reducing the diagnostic time by 90%. The methods within each module of this multi-agent debate system are also replaceable, facilitating its adaptation for diagnosing and researching other complex diseases.",Xinyang Zhou; Yongyong Ren; Qianqian Zhao; Daoyi Huang; Xinbo Wang; Tingting Zhao; Zhixing Zhu; Wenyuan He; Shuyuan Li; Yan Xu; Yu Sun; Yongguo Yu; Shengnan Wu; Jian Wang; Guangjun Yu; Dake He; Bo Ban; Hui Lu,,2025-04-10T15:55:34Z,http://arxiv.org/abs/2504.07881v2
2504.10388v1,Inferring genotype-phenotype maps using attention models,"Predicting phenotype from genotype is a central challenge in genetics. Traditional approaches in quantitative genetics typically analyze this problem using methods based on linear regression. These methods generally assume that the genetic architecture of complex traits can be parameterized in terms of an additive model, where the effects of loci are independent, plus (in some cases) pairwise epistatic interactions between loci. However, these models struggle to analyze more complex patterns of epistasis or subtle gene-environment interactions. Recent advances in machine learning, particularly attention-based models, offer a promising alternative. Initially developed for natural language processing, attention-based models excel at capturing context-dependent interactions and have shown exceptional performance in predicting protein structure and function. Here, we apply attention-based models to quantitative genetics. We analyze the performance of this attention-based approach in predicting phenotype from genotype using simulated data across a range of models with increasing epistatic complexity, and using experimental data from a recent quantitative trait locus mapping study in budding yeast. We find that our model demonstrates superior out-of-sample predictions in epistatic regimes compared to standard methods. We also explore a more general multi-environment attention-based model to jointly analyze genotype-phenotype maps across multiple environments and show that such architectures can be used for ""transfer learning"" - predicting phenotypes in novel environments with limited training data.",Krishna Rijal; Caroline M. Holmes; Samantha Petti; Gautam Reddy; Michael M. Desai; Pankaj Mehta,,2025-04-14T16:32:17Z,http://arxiv.org/abs/2504.10388v1
2504.11610v1,Generalized probabilistic canonical correlation analysis for multi-modal   data integration with full or partial observations,"Background: The integration and analysis of multi-modal data are increasingly essential across various domains including bioinformatics. As the volume and complexity of such data grow, there is a pressing need for computational models that not only integrate diverse modalities but also leverage their complementary information to improve clustering accuracy and insights, especially when dealing with partial observations with missing data. Results: We propose Generalized Probabilistic Canonical Correlation Analysis (GPCCA), an unsupervised method for the integration and joint dimensionality reduction of multi-modal data. GPCCA addresses key challenges in multi-modal data analysis by handling missing values within the model, enabling the integration of more than two modalities, and identifying informative features while accounting for correlations within individual modalities. The model demonstrates robustness to various missing data patterns and provides low-dimensional embeddings that facilitate downstream clustering and analysis. In a range of simulation settings, GPCCA outperforms existing methods in capturing essential patterns across modalities. Additionally, we demonstrate its applicability to multi-omics data from TCGA cancer datasets and a multi-view image dataset. Conclusion: GPCCA offers a useful framework for multi-modal data integration, effectively handling missing data and providing informative low-dimensional embeddings. Its performance across cancer genomics and multi-view image data highlights its robustness and potential for broad application. To make the method accessible to the wider research community, we have released an R package, GPCCA, which is available at https://github.com/Kaversoniano/GPCCA.",Tianjian Yang; Wei Vivian Li,,2025-04-15T20:49:31Z,http://arxiv.org/abs/2504.11610v1
2504.12353v1,TransST: Transfer Learning Embedded Spatial Factor Modeling of Spatial   Transcriptomics Data,"Background: Spatial transcriptomics have emerged as a powerful tool in biomedical research because of its ability to capture both the spatial contexts and abundance of the complete RNA transcript profile in organs of interest. However, limitations of the technology such as the relatively low resolution and comparatively insufficient sequencing depth make it difficult to reliably extract real biological signals from these data. To alleviate this challenge, we propose a novel transfer learning framework, referred to as TransST, to adaptively leverage the cell-labeled information from external sources in inferring cell-level heterogeneity of a target spatial transcriptomics data.   Results: Applications in several real studies as well as a number of simulation settings show that our approach significantly improves existing techniques. For example, in the breast cancer study, TransST successfully identifies five biologically meaningful cell clusters, including the two subgroups of cancer in situ and invasive cancer; in addition, only TransST is able to separate the adipose tissues from the connective issues among all the studied methods.   Conclusions: In summary, the proposed method TransST is both effective and robust in identifying cell subclusters and detecting corresponding driving biomarkers in spatial transcriptomics data.",Shuo Shuo Liu; Shikun Wang; Yuxuan Chen; Anil K. Rustgi; Ming Yuan; Jianhua Hu,,2025-04-15T22:03:38Z,http://arxiv.org/abs/2504.12353v1
2504.13023v1,ChatEXAONEPath: An Expert-level Multimodal Large Language Model for   Histopathology Using Whole Slide Images,"Recent studies have made significant progress in developing large language models (LLMs) in the medical domain, which can answer expert-level questions and demonstrate the potential to assist clinicians in real-world clinical scenarios. Studies have also witnessed the importance of integrating various modalities with the existing LLMs for a better understanding of complex clinical contexts, which are innately multi-faceted by nature. Although studies have demonstrated the ability of multimodal LLMs in histopathology to answer questions from given images, they lack in understanding of thorough clinical context due to the patch-level data with limited information from public datasets. Thus, developing WSI-level MLLMs is significant in terms of the scalability and applicability of MLLMs in histopathology. In this study, we introduce an expert-level MLLM for histopathology using WSIs, dubbed as ChatEXAONEPath. We present a retrieval-based data generation pipeline using 10,094 pairs of WSIs and histopathology reports from The Cancer Genome Atlas (TCGA). We also showcase an AI-based evaluation protocol for a comprehensive understanding of the medical context from given multimodal information and evaluate generated answers compared to the original histopathology reports. We demonstrate the ability of diagnosing the given histopathology images using ChatEXAONEPath with the acceptance rate of 62.9% from 1,134 pairs of WSIs and reports. Our proposed model can understand pan-cancer WSIs and clinical context from various cancer types. We argue that our proposed model has the potential to assist clinicians by comprehensively understanding complex morphology of WSIs for cancer diagnosis through the integration of multiple modalities.",Sangwook Kim; Soonyoung Lee; Jongseong Jang,,2025-04-17T15:33:17Z,http://arxiv.org/abs/2504.13023v1
2504.16961v1,A Novel Graph Transformer Framework for Gene Regulatory Network   Inference,"The inference of gene regulatory networks (GRNs) is a foundational stride towards deciphering the fundamentals of complex biological systems. Inferring a possible regulatory link between two genes can be formulated as a link prediction problem. Inference of GRNs via gene coexpression profiling data may not always reflect true biological interactions, as its susceptibility to noise and misrepresenting true biological regulatory relationships. Most GRN inference methods face several challenges in the network reconstruction phase. Therefore, it is important to encode gene expression values, leverege the prior knowledge gained from the available inferred network structures and positional informations of the input network nodes towards inferring a better and more confident GRN network reconstruction. In this paper, we explore the integration of multiple inferred networks to enhance the inference of Gene Regulatory Networks (GRNs). Primarily, we employ autoencoder embeddings to capture gene expression patterns directly from raw data, preserving intricate biological signals. Then, we embed the prior knowledge from GRN structures transforming them into a text-like representation using random walks, which are then encoded with a masked language model, BERT, to generate global embeddings for each gene across all networks. Additionally, we embed the positional encodings of the input gene networks to better identify the position of each unique gene within the graph. These embeddings are integrated into graph transformer-based model, termed GT-GRN, for GRN inference. The GT-GRN model effectively utilizes the topological structure of the ground truth network while incorporating the enriched encoded information. Experimental results demonstrate that GT-GRN significantly outperforms existing GRN inference methods, achieving superior accuracy and highlighting the robustness of our approach.",Binon Teji; Swarup Roy,,2025-04-23T06:24:26Z,http://arxiv.org/abs/2504.16961v1
2504.17162v1,A Comprehensive Review on RNA Subcellular Localization Prediction,"The subcellular localization of RNAs, including long non-coding RNAs (lncRNAs), messenger RNAs (mRNAs), microRNAs (miRNAs) and other smaller RNAs, plays a critical role in determining their biological functions. For instance, lncRNAs are predominantly associated with chromatin and act as regulators of gene transcription and chromatin structure, while mRNAs are distributed across the nucleus and cytoplasm, facilitating the transport of genetic information for protein synthesis. Understanding RNA localization sheds light on processes like gene expression regulation with spatial and temporal precision. However, traditional wet lab methods for determining RNA localization, such as in situ hybridization, are often time-consuming, resource-demanding, and costly. To overcome these challenges, computational methods leveraging artificial intelligence (AI) and machine learning (ML) have emerged as powerful alternatives, enabling large-scale prediction of RNA subcellular localization. This paper provides a comprehensive review of the latest advancements in AI-based approaches for RNA subcellular localization prediction, covering various RNA types and focusing on sequence-based, image-based, and hybrid methodologies that combine both data types. We highlight the potential of these methods to accelerate RNA research, uncover molecular pathways, and guide targeted disease treatments. Furthermore, we critically discuss the challenges in AI/ML approaches for RNA subcellular localization, such as data scarcity and lack of benchmarks, and opportunities to address them. This review aims to serve as a valuable resource for researchers seeking to develop innovative solutions in the field of RNA subcellular localization and beyond.",Cece Zhang; Xuehuan Zhu; Nick Peterson; Jieqiong Wang; Shibiao Wan,,2025-04-24T00:47:31Z,http://arxiv.org/abs/2504.17162v1
2505.00572v1,A Bioinformatic Study of Genetics Involved in Determining Mild Traumatic   Brain Injury Severity and Recovery,"Aim: This in silico study sought to identify specific biomarkers for mild traumatic brain injury (mTBI) through the analysis of publicly available gene and miRNA databases, hypothesizing their influence on neuronal structure, axonal integrity, and regeneration. Methods: This study implemented a three-step process: (1) Data searching for mTBI-related genes in Gene and MalaCard databases and literature review ; (2) Data analysis involved performing functional annotation through GO and KEGG, identifying hub genes using Cytoscape, mapping protein-protein interactions via DAVID and STRING, and predicting miRNA targets using miRSystem, miRWalk2.0, and mirDIP (3) RNA-sequencing analysis applied to the mTBI dataset GSE123336. Results: Eleven candidate hub genes associated with mTBI outcome were identified: APOE, S100B, GFAP, BDNF, AQP4, COMT, MBP, UCHL1, DRD2, ASIC1, and CACNA1A. Enrichment analysis linked these genes to neuron projection regeneration and synaptic plasticity. miRNAs linked to the mTBI candidate genes were hsa-miR-9-5p, hsa-miR-204-5p, hsa-miR-1908-5p, hsa-miR-16-5p, hsa-miR-10a-5p, has-miR-218-5p, has-miR-34a-5p, and has-miR-199b-5p. The RNA sequencing revealed 2664 differentially expressed miRNAs post-mTBI, with 17 showing significant changes at the time of injury and 48 hours post-injury. Two miRNAs were positively correlated with direct head hits. Conclusion: Our study indicates that specific genes and miRNAs, particularly hsa-miR-10a-5p, may influence mTBI outcomes. Our research may guide future mTBI diagnostics, emphasizing the need to measure and track these specific genes and miRNAs in diverse cohorts.",Mahnaz Tajik; Michael D Noseworthy,,2025-05-01T14:56:46Z,http://arxiv.org/abs/2505.00572v1
2505.01056v1,The Emergence of Chirality from Metabolism,"Molecular chirality is critical to biochemical function, but it is unknown when chiral selectivity first became important in the evolutionary transition from geochemistry to biochemistry during the emergence of life. Here, we identify key transitions in the selection of chiral molecules in metabolic evolution, showing how achiral molecules (lacking chiral centers) may have given rise to specific and abundant chiral molecules in the elaboration of metabolic networks from geochemically available precursor molecules. Simulated expansions of biosphere-scale metabolism suggest new hypotheses about the evolution of chiral molecules within biochemistry, including a prominent role for both achiral and chiral compounds as nucleation sites of early metabolic network growth, an increasing enrichment of molecules with more chiral centers as these networks expand, and conservation of broken chiral symmetries along reaction pathways as a general organizing principle. We also find an unexpected enrichment in large, non-polymeric achiral molecules. Leveraging metabolic data of 40,023 genomes and metagenomes, we analyzed the statistics of chiral and achiral molecules in the large-scale organization of metabolism, revealing a chiral-enriched phase of network organization evidenced by system-size dependent chiral scaling laws that differ for individuals and ecosystems. By uncovering how metabolic networks could lead to chiral selection, our findings open new avenues for bridging metabolism and genetics-first approaches to the origin of chirality, allowing tools for better timing of major transitions in molecular organization during the emergence of life, understanding the role of chirality in extant and synthetic metabolisms, and informing targets for chirality-based biosignatures.",John F. Malloy; Camerian Millsaps; Kamesh Narasimhan; Louie Slocombe; Cole Mathis; Leroy Cronin; Sara Imari Walker,,2025-05-02T07:04:57Z,http://arxiv.org/abs/2505.01056v1
2505.01696v1,Interpretable graph-based models on multimodal biomedical data   integration: A technical review and benchmarking,"Integrating heterogeneous biomedical data including imaging, omics, and clinical records supports accurate diagnosis and personalised care. Graph-based models fuse such non-Euclidean data by capturing spatial and relational structure, yet clinical uptake requires regulator-ready interpretability. We present the first technical survey of interpretable graph based models for multimodal biomedical data, covering 26 studies published between Jan 2019 and Sep 2024. Most target disease classification, notably cancer and rely on static graphs from simple similarity measures, while graph-native explainers are rare; post-hoc methods adapted from non-graph domains such as gradient saliency, and SHAP predominate. We group existing approaches into four interpretability families, outline trends such as graph-in-graph hierarchies, knowledge-graph edges, and dynamic topology learning, and perform a practical benchmark. Using an Alzheimer disease cohort, we compare Sensitivity Analysis, Gradient Saliency, SHAP and Graph Masking. SHAP and Sensitivity Analysis recover the broadest set of known AD pathways and Gene-Ontology terms, whereas Gradient Saliency and Graph Masking surface complementary metabolic and transport signatures. Permutation tests show all four beat random gene sets, but with distinct trade-offs: SHAP and Graph Masking offer deeper biology at higher compute cost, while Gradient Saliency and Sensitivity Analysis are quicker though coarser. We also provide a step-by-step flowchart covering graph construction, explainer choice and resource budgeting to help researchers balance transparency and performance. This review synthesises the state of interpretable graph learning for multimodal medicine, benchmarks leading techniques, and charts future directions, from advanced XAI tools to under-studied diseases, serving as a concise reference for method developers and translational scientists.",Alireza Sadeghi; Farshid Hajati; Ahmadreza Argha; Nigel H Lovell; Min Yang; Hamid Alinejad-Rokny,,2025-05-03T05:00:38Z,http://arxiv.org/abs/2505.01696v1
2505.02033v1,Quantum-Enhanced Classification of Brain Tumors Using DNA Microarray   Gene Expression Profiles,"DNA microarray technology enables the simultaneous measurement of expression levels of thousands of genes, thereby facilitating the understanding of the molecular mechanisms underlying complex diseases such as brain tumors and the identification of diagnostic genetic signatures. To derive meaningful biological insights from the high-dimensional and complex gene features obtained through this technology and to analyze gene properties in detail, classical AI-based approaches such as machine learning and deep learning are widely employed. However, these methods face various limitations in managing high-dimensional vector spaces and modeling the intricate relationships among genes. In particular, challenges such as hyperparameter tuning, computational costs, and high processing power requirements can hinder their efficiency. To overcome these limitations, quantum computing and quantum AI approaches are gaining increasing attention. Leveraging quantum properties such as superposition and entanglement, quantum methods enable more efficient parallel processing of high-dimensional data and offer faster and more effective solutions to problems that are computationally demanding for classical methods. In this study, a novel model called ""Deep VQC"" is proposed, based on the Variational Quantum Classifier approach. Developed using microarray data containing 54,676 gene features, the model successfully classified four different types of brain tumors-ependymoma, glioblastoma, medulloblastoma, and pilocytic astrocytoma-alongside healthy samples with high accuracy. Furthermore, compared to classical ML algorithms, our model demonstrated either superior or comparable classification performance. These results highlight the potential of quantum AI methods as an effective and promising approach for the analysis and classification of complex structures such as brain tumors based on gene expression features.",Emine Akpinar; Batuhan Hangun; Murat Oduncuoglu; Oguz Altun; Onder Eyecioglu; Zeynel Yalcin,,2025-05-04T08:43:31Z,http://arxiv.org/abs/2505.02033v1
2505.02278v1,Compositional Image-Text Matching and Retrieval by Grounding Entities,"Vision-language pretraining on large datasets of images-text pairs is one of the main building blocks of current Vision-Language Models. While with additional training, these models excel in various downstream tasks, including visual question answering, image captioning, and visual commonsense reasoning. However, a notable weakness of pretrained models like CLIP, is their inability to perform entity grounding and compositional image and text matching~\cite{Jiang2024ComCLIP, yang2023amc, Rajabi2023GroundedVSR, learninglocalizeCVPR24}. In this work we propose a novel learning-free zero-shot augmentation of CLIP embeddings that has favorable compositional properties. We compute separate embeddings of sub-images of object entities and relations that are localized by the state of the art open vocabulary detectors and dynamically adjust the baseline global image embedding. % The final embedding is obtained by computing a weighted combination of the sub-image embeddings. The resulting embedding is then utilized for similarity computation with text embedding, resulting in a average 1.5\% improvement in image-text matching accuracy on the Visual Genome and SVO Probes datasets~\cite{krishna2017visualgenome, svo}. Notably, the enhanced embeddings demonstrate superior retrieval performance, thus achieving significant gains on the Flickr30K and MS-COCO retrieval benchmarks~\cite{flickr30ke, mscoco}, improving the state-of-the-art Recall@1 by 12\% and 0.4\%, respectively. Our code is available at https://github.com/madhukarreddyvongala/GroundingCLIP.",Madhukar Reddy Vongala; Saurabh Srivastava; Jana Košecká,,2025-05-04T22:18:14Z,http://arxiv.org/abs/2505.02278v1
2505.03339v1,The insertion of Neomycin cassette impairs maternal and social behaviors   in Arc/Arg3.1 knock-out mice,"The Neomycin resistance cassette (Neo+) is commonly inserted in the genome of mice to generate knock-out (KO) models. The effect of gene deletion on social behaviors in mice is controversial between studies using different Neo+ and Neo-mouse lines, particularly Arc/Arg3.1 KO lines. In this study, we identified severe maternal behavior impairments in Neo+, but not Neo-Arc/Arg3.1 KO dams. These deficits resulted from reduced sociability and abnormal social information processing in Neo+ Arc/Arg3.1 KO dams, exacerbated by social communication impairments in pups. The expression of the Neo cassette product did not cause cytotoxicity, but led to altered ERK signaling, gene expression, and oxytocin system. However, oxytocin administration did not improve social impairments in Neo+ Arc/Arg3.1 KO animals. Interestingly, early social environment enrichment enhanced social interaction with familiar, but not unfamiliar conspecifics or maternal behavior. Overall, our findings reveal a major impact of the Neo cassette on behaviors, particularly social behaviors, in Arc/Arg3.1 KO mice, underscoring the need to re-examine phenotypes of animal models carrying the Neo cassette in neuroscience research.",Ana Dudas; Emilia Caire; Abdurahman Hassan a Kuku; Nicolas Azzopardi; Anil Annamneedi; Heba Elseedy; Gaëlle Lefort; Benoît Piegu; Romain Yvinec; Emmanuel Pecnard; Lucile Drobecq; Anne-Charlotte Trouillet; Angela Sirigu; Dietmar Kuhl; Pablo Chamero; Ora Ohana; Lucie P. Pellissier,"PRC; PRC; UKE; PRC; LE STUDIUM, SRM, PRC; INT; PRC; PRC; PRC, MUSCA; PRC; PRC; PRC; INT; UKE; PRC; UKE; PRC",2025-05-06T09:10:30Z,http://arxiv.org/abs/2505.03339v1
2505.03853v1,GRAPE: Heterogeneous Graph Representation Learning for Genetic   Perturbation with Coding and Non-Coding Biotype,"Predicting genetic perturbations enables the identification of potentially crucial genes prior to wet-lab experiments, significantly improving overall experimental efficiency. Since genes are the foundation of cellular life, building gene regulatory networks (GRN) is essential to understand and predict the effects of genetic perturbations. However, current methods fail to fully leverage gene-related information, and solely rely on simple evaluation metrics to construct coarse-grained GRN. More importantly, they ignore functional differences between biotypes, limiting the ability to capture potential gene interactions. In this work, we leverage pre-trained large language model and DNA sequence model to extract features from gene descriptions and DNA sequence data, respectively, which serve as the initialization for gene representations. Additionally, we introduce gene biotype information for the first time in genetic perturbation, simulating the distinct roles of genes with different biotypes in regulating cellular processes, while capturing implicit gene relationships through graph structure learning (GSL). We propose GRAPE, a heterogeneous graph neural network (HGNN) that leverages gene representations initialized with features from descriptions and sequences, models the distinct roles of genes with different biotypes, and dynamically refines the GRN through GSL. The results on publicly available datasets show that our method achieves state-of-the-art performance.",Changxi Chi; Jun Xia; Jingbo Zhou; Jiabei Cheng; Chang Yu; Stan Z. Li,,2025-05-06T03:35:24Z,http://arxiv.org/abs/2505.03853v1
2505.06067v1,Oncolytic mechanisms and immunotherapeutic potential of Newcastle   disease virus in cancer therapy,"Newcastle Disease Virus (NDV), classified as Avian orthoavulavirus 1 (avian paramyxovirus type 1), is a promising oncolytic agent that selectively targets and destroys cancer cells while sparing normal tissues. Its oncoselectivity exploits cancer-specific defects in antiviral defenses, particularly impaired Type I interferon signaling, and dysregulated apoptotic pathways, enabling robust viral replication and cytotoxicity in malignancies such as breast, colorectal, and melanoma. NDV induces intrinsic and extrinsic apoptosis through caspase activation and triggers immunogenic cell death via damage-associated molecular patterns, stimulating potent antitumours immune responses. Additionally, NDVs potential as a vaccine vector, expressing tumours-associated antigens, offers prospects for prophylactic and therapeutic cancer applications. This review provides a comprehensive analysis of NDVs morphology, classification, and molecular biology, focusing on its viral entry and replication mechanisms in host cells. It explores NDVs interactions with cancer cells, emphasizing its ability to induce cytotoxicity and immune activation. Understanding these mechanisms is critical for optimizing NDVs oncolytic potential and advancing its clinical translation. Future directions include enhancing NDV through genetic engineering, combining it with therapies like immune checkpoint inhibitors, and developing personalized medicine approaches tailored to tumours genomic profiles. These advancements position NDV as a versatile therapeutic agent in oncolytic virotherapy.",Umar Ahmad; Surializa Harun; Moussa Moise Diagne; Syahril Abdullah; Khatijah Yusoff; Abhi Veerakumarasivam,,2025-05-09T14:03:41Z,http://arxiv.org/abs/2505.06067v1
2505.06971v1,The promise and perils of AI in medicine,"What does Artificial Intelligence (AI) have to contribute to health care? And what should we be looking out for if we are worried about its risks? In this paper we offer a survey, and initial evaluation, of hopes and fears about the applications of artificial intelligence in medicine. AI clearly has enormous potential as a research tool, in genomics and public health especially, as well as a diagnostic aid. It's also highly likely to impact on the organisational and business practices of healthcare systems in ways that are perhaps under-appreciated. Enthusiasts for AI have held out the prospect that it will free physicians up to spend more time attending to what really matters to them and their patients. We will argue that this claim depends upon implausible assumptions about the institutional and economic imperatives operating in contemporary healthcare settings. We will also highlight important concerns about privacy, surveillance, and bias in big data, as well as the risks of over trust in machines, the challenges of transparency, the deskilling of healthcare practitioners, the way AI reframes healthcare, and the implications of AI for the distribution of power in healthcare institutions. We will suggest that two questions, in particular, are deserving of further attention from philosophers and bioethicists. What does care look like when one is dealing with data as much as people? And, what weight should we give to the advice of machines in our own deliberations about medical decisions?",Robert Sparrow; Joshua Hatherley,,2025-05-11T13:04:42Z,http://arxiv.org/abs/2505.06971v1
2505.07124v2,Learning from Samples: Inverse Problems over measures via Sharpened   Fenchel-Young Losses,"Estimating parameters from samples of an optimal probability distribution is essential in applications ranging from socio-economic modeling to biological system analysis. In these settings, the probability distribution arises as the solution to an optimization problem that captures either static interactions among agents or the dynamic evolution of a system over time. We introduce a general methodology based on a new class of loss functions, called sharpened Fenchel-Young losses, which measure the sub-optimality gap of the optimization problem over the space of probability measures. We provide explicit stability guarantees for two relevant settings in the context of optimal transport: The first is inverse unbalanced optimal transport (iUOT) with entropic regularization, where the parameters to estimate are cost functions that govern transport computations; this method has applications such as link prediction in machine learning. The second is inverse gradient flow (iJKO), where the objective is to recover a potential function that drives the evolution of a probability distribution via the Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is particularly relevant for understanding cell population dynamics in single-cell genomics. We also establish source conditions to ensure stability of our method under mirror stratifiable regularizers (such as l1 or nuclear norm) that promote structure. Finally, we present optimization algorithms specifically tailored to efficiently solve iUOT and iJKO problems. We validate our approach through numerical experiments on Gaussian distributions, where closed-form solutions are available, to demonstrate the practical performance of our methods.",Francisco Andrade; Gabriel Peyré; Clarice Poon,,2025-05-11T21:26:44Z,http://arxiv.org/abs/2505.07124v2
2505.10400v1,Computer simulations show that liquid-liquid phase separation enhances   self-assembly,"Biomolecular condensates are liquid- or gel-like droplets of proteins and nucleic acids formed at least in part through liquid-liquid phase separation. Condensates enable diverse functions of cells and the pathogens that infect them, including self-assembly reactions. For example, it has been shown that many viruses form condensates within their host cells to compartmentalize capsid assembly and packaging of the viral genome. Yet, the physical principles controlling condensate-mediated self-assembly remain incompletely understood. In this article we use coarse-grained molecular dynamics simulations to study the effect of a condensate on the assembly of icosahedral capsids. The capsid subunits are represented by simple shape-based models to enable simulating a wide range of length and time scales, while the condensate is modeled implicitly to study the effects of phase separation independent of the molecular details of biomolecular condensates. Our results show that condensates can significantly enhance assembly rates, yields, and robustness to parameter variations, consistent with previous theoretical predictions. However, extending beyond those predictions, the computational models also show that excluded volume enables control over the number of capsids that assemble within condensates. Moreover, long-lived aberrant off-pathway assembly intermediates can suppress yields within condensates. In addition to elucidating condensate-mediated assembly of viruses and other biological structures, these results may guide the use of condensates as a generic route to enhance and control self-assembly in human-engineered systems.",Layne B. Frechette; Naren Sundararajan; Fernando Caballero; Anthony Trubiano; Michael F. Hagan,,2025-05-15T15:23:12Z,http://arxiv.org/abs/2505.10400v1
2505.10852v1,MatTools: Benchmarking Large Language Models for Materials Science Tools,"Large language models (LLMs) are increasingly applied to materials science questions, including literature comprehension, property prediction, materials discovery and alloy design. At the same time, a wide range of physics-based computational approaches have been developed in which materials properties can be calculated. Here, we propose a benchmark application to evaluate the proficiency of LLMs to answer materials science questions through the generation and safe execution of codes based on such physics-based computational materials science packages. MatTools is built on two complementary components: a materials simulation tool question-answer (QA) benchmark and a real-world tool-usage benchmark. We designed an automated methodology to efficiently collect real-world materials science tool-use examples. The QA benchmark, derived from the pymatgen (Python Materials Genomics) codebase and documentation, comprises 69,225 QA pairs that assess the ability of an LLM to understand materials science tools. The real-world benchmark contains 49 tasks (138 subtasks) requiring the generation of functional Python code for materials property calculations. Our evaluation of diverse LLMs yields three key insights: (1)Generalists outshine specialists;(2)AI knows AI; and (3)Simpler is better. MatTools provides a standardized framework for assessing and improving LLM capabilities for materials science tool applications, facilitating the development of more effective AI systems for materials science and general scientific research.",Siyu Liu; Jiamin Xu; Beilin Ye; Bo Hu; David J. Srolovitz; Tongqi Wen,,2025-05-16T04:43:05Z,http://arxiv.org/abs/2505.10852v1
2505.12626v1,scSiameseClu: A Siamese Clustering Framework for Interpreting   single-cell RNA Sequencing Data,"Single-cell RNA sequencing (scRNA-seq) reveals cell heterogeneity, with cell clustering playing a key role in identifying cell types and marker genes. Recent advances, especially graph neural networks (GNNs)-based methods, have significantly improved clustering performance. However, the analysis of scRNA-seq data remains challenging due to noise, sparsity, and high dimensionality. Compounding these challenges, GNNs often suffer from over-smoothing, limiting their ability to capture complex biological information. In response, we propose scSiameseClu, a novel Siamese Clustering framework for interpreting single-cell RNA-seq data, comprising of 3 key steps: (1) Dual Augmentation Module, which applies biologically informed perturbations to the gene expression matrix and cell graph relationships to enhance representation robustness; (2) Siamese Fusion Module, which combines cross-correlation refinement and adaptive information fusion to capture complex cellular relationships while mitigating over-smoothing; and (3) Optimal Transport Clustering, which utilizes Sinkhorn distance to efficiently align cluster assignments with predefined proportions while maintaining balance. Comprehensive evaluations on seven real-world datasets demonstrate that~\methodname~outperforms state-of-the-art methods in single-cell clustering, cell type annotation, and cell type classification, providing a powerful tool for scRNA-seq data interpretation.",Ping Xu; Zhiyuan Ning; Pengjiang Li; Wenhao Liu; Pengyang Wang; Jiaxu Cui; Yuanchun Zhou; Pengfei Wang,,2025-05-19T02:17:09Z,http://arxiv.org/abs/2505.12626v1
2505.15481v1,A conditional coalescent for diploid exchangeable population models   given the pedigree,"We study coalescent processes conditional on the population pedigree under the exchangeable diploid bi-parental population model of \citet{BirknerEtAl2018}. While classical coalescent models average over all reproductive histories, thereby marginalizing the pedigree, our work analyzes the genealogical structure embedded within a fixed pedigree generated by the diploid Cannings model. In the large-population limit, we show that these conditional coalescent processes differ significantly from their marginal counterparts when the marginal coalescent process includes multiple mergers. We characterize the limiting process as an inhomogeneous $(\Psi,c)$-coalescent, where $\Psi$ encodes the timing and scale of multiple mergers caused by generations with large individual progeny (GLIPs), and $c$ is a constant rate governing binary mergers.   Our results reveal fundamental distinctions between quenched (conditional) and annealed (classical) genealogical models, demonstrate how the fixed pedigree structure impacts multi-locus statistics such as the site-frequency spectrum, and have implications for interpreting patterns of genetic variation among unlinked loci in the genomes of sampled individuals. They significantly extend the results of \citet{DiamantidisEtAl2024}, which considered a sample of size two under a specific Wright-Fisher model with a highly reproductive couple, and those of \citet{TyukinThesis2015}, where Kingman coalescent was the limiting process. Our proofs adapt coupling techniques from the theory of random walks in random environments.",Frederic Alberti; Matthias Birkner; Wai-Tong Louis Fan; John Wakeley,,2025-05-21T12:57:35Z,http://arxiv.org/abs/2505.15481v1
2505.15730v1,iBitter-Stack: A Multi-Representation Ensemble Learning Model for   Accurate Bitter Peptide Identification,"The identification of bitter peptides is crucial in various domains, including food science, drug discovery, and biochemical research. These peptides not only contribute to the undesirable taste of hydrolyzed proteins but also play key roles in physiological and pharmacological processes. However, experimental methods for identifying bitter peptides are time-consuming and expensive. With the rapid expansion of peptide sequence databases in the post-genomic era, the demand for efficient computational approaches to distinguish bitter from non-bitter peptides has become increasingly significant. In this study, we propose a novel stacking-based ensemble learning framework aimed at enhancing the accuracy and reliability of bitter peptide classification. Our method integrates diverse sequence-based feature representations and leverages a broad set of machine learning classifiers. The first stacking layer comprises multiple base classifiers, each trained on distinct feature encoding schemes, while the second layer employs logistic regression to refine predictions using an eight-dimensional probability vector. Extensive evaluations on a carefully curated dataset demonstrate that our model significantly outperforms existing predictive methods, providing a robust and reliable computational tool for bitter peptide identification. Our approach achieves an accuracy of 96.09\% and a Matthews Correlation Coefficient (MCC) of 0.9220 on the independent test set, underscoring its effectiveness and generalizability. To facilitate real-time usage and broader accessibility, we have also developed a user-friendly web server based on the proposed method, which is freely accessible at https://ibitter-stack-webserver.streamlit.app/. This tool enables researchers and practitioners to conveniently screen peptide sequences for bitterness in real-time applications.",Sarfraz Ahmad; Momina Ahsan; Muhammad Nabeel Asim; Andreas Dengel; Muhammad Imran Malik,,2025-05-21T16:35:29Z,http://arxiv.org/abs/2505.15730v1
2505.20321v1,BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge   Bases,"Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.",Mathew J. Koretsky; Maya Willey; Adi Asija; Owen Bianchi; Chelsea X. Alvarado; Tanay Nayak; Nicole Kuznetsov; Sungwon Kim; Mike A. Nalls; Daniel Khashabi; Faraz Faghri,,2025-05-23T17:58:07Z,http://arxiv.org/abs/2505.20321v1
2505.22688v2,Investigating the effectiveness of multimodal data in forecasting   SARS-COV-2 case surges,"The COVID-19 pandemic response relied heavily on statistical and machine learning models to predict key outcomes such as case prevalence and fatality rates. These predictions were instrumental in enabling timely public health interventions that helped break transmission cycles. While most existing models are grounded in traditional epidemiological data, the potential of alternative datasets, such as those derived from genomic information and human behavior, remains underexplored. In the current study, we investigated the usefulness of diverse modalities of feature sets in predicting case surges. Our results highlight the relative effectiveness of biological (e.g., mutations), public health (e.g., case counts, policy interventions) and human behavioral features (e.g., mobility and social media conversations) in predicting country-level case surges. Importantly, we uncover considerable heterogeneity in predictive performance across countries and feature modalities, suggesting that surge prediction models may need to be tailored to specific national contexts and pandemic phases. Overall, our work highlights the value of integrating alternative data sources into existing disease surveillance frameworks to enhance the prediction of pandemic dynamics.",Palur Venkata Raghuvamsi; Siyuan Brandon Loh; Prasanta Bhattacharya; Joses Ho; Raphael Lee Tze Chuen; Alvin X. Han; Sebastian Maurer-Stroh,,2025-05-28T01:00:02Z,http://arxiv.org/abs/2505.22688v2
2505.22746v1,StarBASE-GP: Biologically-Guided Automated Machine Learning for   Genotype-to-Phenotype Association Analysis,"We present the Star-Based Automated Single-locus and Epistasis analysis tool - Genetic Programming (StarBASE-GP), an automated framework for discovering meaningful genetic variants associated with phenotypic variation in large-scale genomic datasets. StarBASE-GP uses a genetic programming-based multi-objective optimization strategy to evolve machine learning pipelines that simultaneously maximize explanatory power (r2) and minimize pipeline complexity. Biological domain knowledge is integrated at multiple stages, including the use of nine inheritance encoding strategies to model deviations from additivity, a custom linkage disequilibrium pruning node that minimizes redundancy among features, and a dynamic variant recommendation system that prioritizes informative candidates for pipeline inclusion. We evaluate StarBASE-GP on a cohort of Rattus norvegicus (brown rat) to identify variants associated with body mass index, benchmarking its performance against a random baseline and a biologically naive version of the tool. StarBASE-GP consistently evolves Pareto fronts with superior performance, yielding higher accuracy in identifying both ground truth and novel quantitative trait loci, highlighting relevant targets for future validation. By incorporating evolutionary search and relevant biological theory into a flexible automated machine learning framework, StarBASE-GP demonstrates robust potential for advancing variant discovery in complex traits.",Jose Guadalupe Hernandez; Attri Ghosh; Philip J. Freda; Yufei Meng; Nicholas Matsumoto; Jason H. Moore,,2025-05-28T18:05:15Z,http://arxiv.org/abs/2505.22746v1
2505.23592v1,A Modern Theory of Cross-Validation through the Lens of Stability,"Modern data analysis and statistical learning are marked by complex data structures and black-box algorithms. Data complexity stems from technologies like imaging, remote sensing, wearables, and genomic sequencing. Simultaneously, black-box models -- especially deep neural networks -- have achieved impressive results. This combination raises new challenges for uncertainty quantification and statistical inference, which we term ""black-box inference.""   Black-box inference is difficult due to the lack of traditional modeling assumptions and the opaque behavior of modern estimators. These make it hard to characterize the distribution of estimation errors. A popular solution is post-hoc randomization, which, under mild assumptions like exchangeability, can yield valid uncertainty quantification. Such methods range from classical techniques like permutation tests, jackknife, and bootstrap, to recent innovations like conformal inference. These approaches typically need little knowledge of data distributions or the internal working of estimators. Many rely on the idea that estimators behave similarly under small data changes -- a concept formalized as stability. Over time, stability has become a key principle in data science, influencing generalization error, privacy, and adaptive inference.   This article investigates cross-validation (CV) -- a widely used resampling method -- through the lens of stability. We first review recent theoretical results on CV for estimating generalization error and model selection under stability. We then examine uncertainty quantification for CV-based risk estimates. Together, these insights yield new theory and tools, which we apply to topics like model selection, selective inference, and conformal prediction.",Jing Lei,,2025-05-29T16:04:04Z,http://arxiv.org/abs/2505.23592v1
2505.24155v1,Biological Pathway Guided Gene Selection Through Collaborative   Reinforcement Learning,"Gene selection in high-dimensional genomic data is essential for understanding disease mechanisms and improving therapeutic outcomes. Traditional feature selection methods effectively identify predictive genes but often ignore complex biological pathways and regulatory networks, leading to unstable and biologically irrelevant signatures. Prior approaches, such as Lasso-based methods and statistical filtering, either focus solely on individual gene-outcome associations or fail to capture pathway-level interactions, presenting a key challenge: how to integrate biological pathway knowledge while maintaining statistical rigor in gene selection? To address this gap, we propose a novel two-stage framework that integrates statistical selection with biological pathway knowledge using multi-agent reinforcement learning (MARL). First, we introduce a pathway-guided pre-filtering strategy that leverages multiple statistical methods alongside KEGG pathway information for initial dimensionality reduction. Next, for refined selection, we model genes as collaborative agents in a MARL framework, where each agent optimizes both predictive power and biological relevance. Our framework incorporates pathway knowledge through Graph Neural Network-based state representations, a reward mechanism combining prediction performance with gene centrality and pathway coverage, and collaborative learning strategies using shared memory and a centralized critic component. Extensive experiments on multiple gene expression datasets demonstrate that our approach significantly improves both prediction accuracy and biological interpretability compared to traditional methods.",Ehtesamul Azim; Dongjie Wang; Tae Hyun Hwang; Yanjie Fu; Wei Zhang,,2025-05-30T03:01:07Z,http://arxiv.org/abs/2505.24155v1
2505.24394v1,Refining Platelet Purification Methods: Enhancing Proteomics for   Clinical Applications,"Background: Platelet proteomics offers valuable insights for clinical research, yet isolating high-purity platelets remains a challenge. Current methods often lead to contamination or platelet loss, compromising data quality and reproducibility.   Objectives: This study aimed to optimize a platelet isolation technique that yields high-purity samples with minimal loss and to identify the most effective mass spectrometry-based proteomic method for analyzing platelet proteins with optimal coverage and sensitivity.   Methods: We refined an isolation protocol by adjusting centrifugation time to reduce blood volume requirements while preserving platelet yield and purity. Using this optimized method, we evaluated three proteomic approaches: Label-free Quantification with Data-Independent Acquisition (LFQ-DIA), Label-free Quantification with Data-Dependent Acquisition (LFQ-DDA), and Tandem Mass Tag labeling with DDA (TMT-DDA).   Results: LFQ-DIA demonstrated superior protein coverage and sensitivity compared to LFQ-DDA and TMT-DDA. The refined isolation protocol effectively minimized contamination and platelet loss. Additionally, age-related differences in platelet protein composition were observed, highlighting the importance of using age-matched controls in biomarker discovery studies.   Conclusions: The optimized platelet isolation protocol provides a cost-effective and reliable method for preparing high-purity samples for proteomics. LFQ-DIA is the most suitable approach for comprehensive platelet protein analysis. Age-related variation in platelet proteomes underscores the need for demographic matching in clinical proteomic research.",Vibecke Markhus; Katarina Fritz-Wallace; Olav Mjaavatten; Einar K. Kristoffersen; Dorota Goplen; Frode Selheim,,2025-05-30T09:24:18Z,http://arxiv.org/abs/2505.24394v1
2506.01069v1,Revolutionizing Blood Banks: AI-Driven Fingerprint-Blood Group   Correlation for Enhanced Safety,"Identification of a person is central in forensic science, security, and healthcare. Methods such as iris scanning and genomic profiling are more accurate but expensive, time-consuming, and more difficult to implement. This study focuses on the relationship between the fingerprint patterns and the ABO blood group as a biometric identification tool. A total of 200 subjects were included in the study, and fingerprint types (loops, whorls, and arches) and blood groups were compared. Associations were evaluated with statistical tests, including chi-square and Pearson correlation. The study found that the loops were the most common fingerprint pattern and the O+ blood group was the most prevalent. Even though there was some associative pattern, there was no statistically significant difference in the fingerprint patterns of different blood groups. Overall, the results indicate that blood group data do not significantly improve personal identification when used in conjunction with fingerprinting. Although the study shows weak correlation, it may emphasize the efforts of multi-modal based biometric systems in enhancing the current biometric systems. Future studies may focus on larger and more diverse samples, and possibly machine learning and additional biometrics to improve identification methods. This study addresses an element of the ever-changing nature of the fields of forensic science and biometric identification, highlighting the importance of resilient analytical methods for personal identification.",Malik A. Altayar; Muhyeeddin Alqaraleh; Mowafaq Salem Alzboon; Wesam T. Almagharbeh,,2025-06-01T16:18:24Z,http://arxiv.org/abs/2506.01069v1
2506.02129v1,Benchmarking Large Language Models for Polymer Property Predictions,"Machine learning has revolutionized polymer science by enabling rapid property prediction and generative design. Large language models (LLMs) offer further opportunities in polymer informatics by simplifying workflows that traditionally rely on large labeled datasets, handcrafted representations, and complex feature engineering. LLMs leverage natural language inputs through transfer learning, eliminating the need for explicit fingerprinting and streamlining training. In this study, we finetune general purpose LLMs -- open-source LLaMA-3-8B and commercial GPT-3.5 -- on a curated dataset of 11,740 entries to predict key thermal properties: glass transition, melting, and decomposition temperatures. Using parameter-efficient fine-tuning and hyperparameter optimization, we benchmark these models against traditional fingerprinting-based approaches -- Polymer Genome, polyGNN, and polyBERT -- under single-task (ST) and multi-task (MT) learning. We find that while LLM-based methods approach traditional models in performance, they generally underperform in predictive accuracy and efficiency. LLaMA-3 consistently outperforms GPT-3.5, likely due to its tunable open-source architecture. Additionally, ST learning proves more effective than MT, as LLMs struggle to capture cross-property correlations, a key strength of traditional methods. Analysis of molecular embeddings reveals limitations of general purpose LLMs in representing nuanced chemo-structural information compared to handcrafted features and domain-specific embeddings. These findings provide insight into the interplay between molecular embeddings and natural language processing, guiding LLM selection for polymer informatics.",Sonakshi Gupta; Akhlak Mahmood; Shivank Shukla; Rampi Ramprasad,,2025-06-02T18:01:07Z,http://arxiv.org/abs/2506.02129v1
2506.02213v1,Quantum Ensembling Methods for Healthcare and Life Science,"Learning on small data is a challenge frequently encountered in many real-world applications. In this work we study how effective quantum ensemble models are when trained on small data problems in healthcare and life sciences. We constructed multiple types of quantum ensembles for binary classification using up to 26 qubits in simulation and 56 qubits on quantum hardware. Our ensemble designs use minimal trainable parameters but require long-range connections between qubits. We tested these quantum ensembles on synthetic datasets and gene expression data from renal cell carcinoma patients with the task of predicting patient response to immunotherapy. From the performance observed in simulation and initial hardware experiments, we demonstrate how quantum embedding structure affects performance and discuss how to extract informative features and build models that can learn and generalize effectively. We present these exploratory results in order to assist other researchers in the design of effective learning on small data using ensembles. Incorporating quantum computing in these data constrained problems offers hope for a wide range of studies in healthcare and life sciences where biological samples are relatively scarce given the feature space to be explored.",Kahn Rhrissorrakrai; Kathleen E. Hamilton; Prerana Bangalore Parthsarathy; Aldo Guzman-Saenz; Tyler Alban; Filippo Utro; Laxmi Parida,,2025-06-02T19:54:51Z,http://arxiv.org/abs/2506.02213v1
2506.04515v1,The Latent Space Hypothesis: Toward Universal Medical Representation   Learning,"Medical data range from genomic sequences and retinal photographs to structured laboratory results and unstructured clinical narratives. Although these modalities appear disparate, many encode convergent information about a single underlying physiological state. The Latent Space Hypothesis frames each observation as a projection of a unified, hierarchically organized manifold -- much like shadows cast by the same three-dimensional object. Within this learned geometric representation, an individual's health status occupies a point, disease progression traces a trajectory, and therapeutic intervention corresponds to a directed vector. Interpreting heterogeneous evidence in a shared space provides a principled way to re-examine eponymous conditions -- such as Parkinson's or Crohn's -- that often mask multiple pathophysiological entities and involve broader anatomical domains than once believed. By revealing sub-trajectories and patient-specific directions of change, the framework supplies a quantitative rationale for personalised diagnosis, longitudinal monitoring, and tailored treatment, moving clinical practice away from grouping by potentially misleading labels toward navigation of each person's unique trajectory. Challenges remain -- bias amplification, data scarcity for rare disorders, privacy, and the correlation-causation divide -- but scale-aware encoders, continual learning on longitudinal data streams, and perturbation-based validation offer plausible paths forward.",Salil Patel,,2025-06-04T23:37:33Z,http://arxiv.org/abs/2506.04515v1
2506.04935v1,Resilient Pattern Mining,"Frequent pattern mining is a flagship problem in data mining. In its most basic form, it asks for the set of substrings of a given string $S$ of length $n$ that occur at least $\tau$ times in $S$, for some integer $\tau\in[1,n]$. We introduce a resilient version of this classic problem, which we term the $(\tau, k)$-Resilient Pattern Mining (RPM) problem. Given a string $S$ of length $n$ and two integers $\tau, k\in[1,n]$, RPM asks for the set of substrings of $S$ that occur at least $\tau$ times in $S$, even when the letters at any $k$ positions of $S$ are substituted by other letters. Unlike frequent substrings, resilient ones account for the fact that changes to string $S$ are often expensive to handle or are unknown.   We propose an exact $\mathcal{O}(n\log n)$-time and $\mathcal{O}(n)$-space algorithm for RPM, which employs advanced data structures and combinatorial insights. We then present experiments on real large-scale datasets from different domains demonstrating that: (I) The notion of resilient substrings is useful in analyzing genomic data and is more powerful than that of frequent substrings, in scenarios where resilience is required, such as in the case of versioned datasets; (II) Our algorithm is several orders of magnitude faster and more space-efficient than a baseline algorithm that is based on dynamic programming; and (III) Clustering based on resilient substrings is effective.",Pengxin Bian; Panagiotis Charalampopoulos; Lorraine A. K. Ayad; Manal Mohamed; Solon P. Pissis; Grigorios Loukides,,2025-06-05T12:10:19Z,http://arxiv.org/abs/2506.04935v1
2506.05443v1,UniPTMs: The First Unified Multi-type PTM Site Prediction Model via   Master-Slave Architecture-Based Multi-Stage Fusion Strategy and Hierarchical   Contrastive Loss,"As a core mechanism of epigenetic regulation in eukaryotes, protein post-translational modifications (PTMs) require precise prediction to decipher dynamic life activity networks. To address the limitations of existing deep learning models in cross-modal feature fusion, domain generalization, and architectural optimization, this study proposes UniPTMs: the first unified framework for multi-type PTM prediction. The framework innovatively establishes a ""Master-Slave"" dual-path collaborative architecture: The master path dynamically integrates high-dimensional representations of protein sequences, structures, and evolutionary information through a Bidirectional Gated Cross-Attention (BGCA) module, while the slave path optimizes feature discrepancies and recalibration between structural and traditional features using a Low-Dimensional Fusion Network (LDFN). Complemented by a Multi-scale Adaptive convolutional Pyramid (MACP) for capturing local feature patterns and a Bidirectional Hierarchical Gated Fusion Network (BHGFN) enabling multi-level feature integration across paths, the framework employs a Hierarchical Dynamic Weighting Fusion (HDWF) mechanism to intelligently aggregate multimodal features. Enhanced by a novel Hierarchical Contrastive loss function for feature consistency optimization, UniPTMs demonstrates significant performance improvements (3.2%-11.4% MCC and 4.2%-14.3% AP increases) over state-of-the-art models across five modification types and transcends the Single-Type Prediction Paradigm. To strike a balance between model complexity and performance, we have also developed a lightweight variant named UniPTMs-mini.",Yiyu Lin; Yan Wang; You Zhou; Xinye Ni; Jiahui Wu; Sen Yang,,2025-06-05T13:02:43Z,http://arxiv.org/abs/2506.05443v1
2506.06980v1,MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal   cancer sub-type classification,"Cancer subtype classification is crucial for personalized treatment and prognostic assessment. However, effectively integrating multi-omic data remains challenging due to the heterogeneous nature of genomic, epigenomic, and transcriptomic features. In this work, we propose Modality-Aware Cross-Attention MoXGATE, a novel deep-learning framework that leverages cross-attention and learnable modality weights to enhance feature fusion across multiple omics sources. Our approach effectively captures inter-modality dependencies, ensuring robust and interpretable integration. Through experiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA) datasets from TCGA, we demonstrate that MoXGATE outperforms existing methods, achieving 95\% classification accuracy. Ablation studies validate the effectiveness of cross-attention over simple concatenation and highlight the importance of different omics modalities. Moreover, our model generalizes well to unseen cancer types e.g., breast cancer, underscoring its adaptability. Key contributions include (1) a cross-attention-based multi-omic integration framework, (2) modality-weighted fusion for enhanced interpretability, (3) application of focal loss to mitigate data imbalance, and (4) validation across multiple cancer subtypes. Our results indicate that MoXGATE is a promising approach for multi-omic cancer subtype classification, offering improved performance and biological generalizability.",Sajib Acharjee Dip; Uddip Acharjee Shuvo; Dipanwita Mallick; Abrar Rahman Abir; Liqing Zhang,,2025-06-08T03:42:23Z,http://arxiv.org/abs/2506.06980v1
2506.07643v1,Synthetic Visual Genome,"Reasoning over visual relationships-spatial, functional, interactional, social, etc.-is considered to be a fundamental component of human cognition. Yet, despite the major advances in visual comprehension in multimodal language models (MLMs), precise reasoning over relationships and their generations remains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely annotated relationships capable of constructing high-quality dense scene graphs at scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by completing the missing relations of selected objects in existing scene graphs using a teacher MLM and a carefully designed filtering process to ensure high-quality. To generate more accurate and rich scene graphs at scale for any image, we introduce SG-EDIT: a self-distillation framework where GPT-4o further refines ROBIN's predicted scene graphs by removing unlikely relations and/or suggesting relevant ones. In total, our dataset contains 146K images and 5.6M relationships for 2.6M objects. Results show that our ROBIN-3B model, despite being trained on less than 3 million instances, outperforms similar-size models trained on over 300 million instances on relationship understanding benchmarks, and even surpasses larger models up to 13B parameters. Notably, it achieves state-of-the-art performance in referring expression comprehension with a score of 88.9, surpassing the previous best of 87.4. Our results suggest that training on the refined scene graph data is crucial to maintaining high performance across diverse visual reasoning task.",Jae Sung Park; Zixian Ma; Linjie Li; Chenhao Zheng; Cheng-Yu Hsieh; Ximing Lu; Khyathi Chandu; Quan Kong; Norimasa Kobori; Ali Farhadi; Yejin Choi; Ranjay Krishna,,2025-06-09T11:09:10Z,http://arxiv.org/abs/2506.07643v1
2506.08189v1,Open World Scene Graph Generation using Vision Language Models,"Scene-Graph Generation (SGG) seeks to recognize objects in an image and distill their salient pairwise relationships. Most methods depend on dataset-specific supervision to learn the variety of interactions, restricting their usefulness in open-world settings, involving novel objects and/or relations. Even methods that leverage large Vision Language Models (VLMs) typically require benchmark-specific fine-tuning. We introduce Open-World SGG, a training-free, efficient, model-agnostic framework that taps directly into the pretrained knowledge of VLMs to produce scene graphs with zero additional learning. Casting SGG as a zero-shot structured-reasoning problem, our method combines multimodal prompting, embedding alignment, and a lightweight pair-refinement strategy, enabling inference over unseen object vocabularies and relation sets. To assess this setting, we formalize an Open-World evaluation protocol that measures performance when no SGG-specific data have been observed either in terms of objects and relations. Experiments on Visual Genome, Open Images V6, and the Panoptic Scene Graph (PSG) dataset demonstrate the capacity of pretrained VLMs to perform relational understanding without task-level training.",Amartya Dutta; Kazi Sajeed Mehrab; Medha Sawhney; Abhilash Neog; Mridul Khurana; Sepideh Fatemi; Aanish Pradhan; M. Maruf; Ismini Lourentzou; Arka Daw; Anuj Karpatne,,2025-06-09T19:59:05Z,http://arxiv.org/abs/2506.08189v1
2506.08897v4,PlantDeBERTa: An Open Source Language Model for Plant Science,"The rapid advancement of transformer-based language models has catalyzed breakthroughs in biomedical and clinical natural language processing; however, plant science remains markedly underserved by such domain-adapted tools. In this work, we present PlantDeBERTa, a high-performance, open-source language model specifically tailored for extracting structured knowledge from plant stress-response literature. Built upon the DeBERTa architecture-known for its disentangled attention and robust contextual encoding-PlantDeBERTa is fine-tuned on a meticulously curated corpus of expert-annotated abstracts, with a primary focus on lentil (Lens culinaris) responses to diverse abiotic and biotic stressors. Our methodology combines transformer-based modeling with rule-enhanced linguistic post-processing and ontology-grounded entity normalization, enabling PlantDeBERTa to capture biologically meaningful relationships with precision and semantic fidelity. The underlying corpus is annotated using a hierarchical schema aligned with the Crop Ontology, encompassing molecular, physiological, biochemical, and agronomic dimensions of plant adaptation. PlantDeBERTa exhibits strong generalization capabilities across entity types and demonstrates the feasibility of robust domain adaptation in low-resource scientific fields.By providing a scalable and reproducible framework for high-resolution entity recognition, PlantDeBERTa bridges a critical gap in agricultural NLP and paves the way for intelligent, data-driven systems in plant genomics, phenomics, and agronomic knowledge discovery. Our model is publicly released to promote transparency and accelerate cross-disciplinary innovation in computational plant science.",Hiba Khey; Amine Lakhder; Salma Rouichi; Imane El Ghabi; Kamal Hejjaoui; Younes En-nahli; Fahd Kalloubi; Moez Amri,,2025-06-10T15:24:03Z,http://arxiv.org/abs/2506.08897v4
2506.10134v1,FAPS: A Fast Platform for Protein Structureomics Analysis,"Protein quantification and analysis are well-accepted approaches for biomarker discovery but are limited to identification without structural information. High-throughput omics data (i.e., genomics, transcriptomics, and proteomics) have become pervasive in cancer biology studies and reach well beyond more specialized areas such as metabolomics, epigenomics, pharmacogenomics, and interact-omics. However, large-scale analysis based on the structure of the biomolecules, namely structure-omics, is still underexplored due to a lack of handy tools. In response, we developed the Fast Analysis of Protein Structure (FAPS) database, a platform designed to advance quantitative proteomics to structure-omics analysis, which significantly shortens large-scale structure-omics from weeks to seconds. FAPS can serve as a new protein secondary structure database, providing a centralized and functional database for both simulated and experimentally determined bioinformatics statistics relating to secondary structure. Stored data is generated both through the structure simulation, currently SWISS-MODEL and AlphaFold, performed by high-performance computers, and the pre-existing UniProt database. FAPS provides user-friendly features that create a straightforward and effective way of accessing accurate data on the proportion of secondary structure in different protein chains, providing a fast numerical and visual reference for protein structure calculations and analysis. FAPS is accessible through http://fapsdb.org.",Lucas Wilken; Nihjum Paul; Troy Timmerman; Sara A. Tolba; Amara Arshad; Di Wu; Wenjie Xia; Bakhtiyor Rasulev; Rick Jansen; Dali Sun,,2025-06-11T19:29:58Z,http://arxiv.org/abs/2506.10134v1
2506.10275v1,VQC-MLPNet: An Unconventional Hybrid Quantum-Classical Architecture for   Scalable and Robust Quantum Machine Learning,"Variational Quantum Circuits (VQCs) offer a novel pathway for quantum machine learning, yet their practical application is hindered by inherent limitations such as constrained linear expressivity, optimization challenges, and acute sensitivity to quantum hardware noise. This work introduces VQC-MLPNet, a scalable and robust hybrid quantum-classical architecture designed to overcome these obstacles. By innovatively employing quantum circuits to dynamically generate parameters for classical Multi-Layer Perceptrons (MLPs) via amplitude encoding and parameterized quantum operations, VQC-MLPNet substantially expands representation capabilities and augments training stability. We provide rigorous theoretical guarantees via statistical learning techniques and Neural Tangent Kernel analysis, explicitly deriving upper bounds on approximation, uniform deviation, and optimization errors. These theoretical insights demonstrate exponential improvements in representation capacity relative to quantum circuit depth and the number of qubits, providing clear computational advantages over standalone quantum circuits and existing hybrid quantum architectures. Our theoretical claims are empirically corroborated through extensive experiments, including classifying semiconductor quantum-dot charge states and predicting genomic transcription factor binding sites, demonstrating resilient performance even under realistic IBM quantum noise simulations. This research establishes a theoretically sound and practically robust framework, advancing the frontiers of quantum-enhanced learning for unconventional computing paradigms in the Noisy Intermediate-Scale Quantum era and beyond.",Jun Qi; Chao-Han Yang; Pin-Yu Chen; Min-Hsiu Hsieh,,2025-06-12T01:38:15Z,http://arxiv.org/abs/2506.10275v1
2506.11152v1,HEIST: A Graph Foundation Model for Spatial Transcriptomics and   Proteomics Data,"Single-cell transcriptomics has become a great source for data-driven insights into biology, enabling the use of advanced deep learning methods to understand cellular heterogeneity and transcriptional regulation at the single-cell level. With the advent of spatial transcriptomics data we have the promise of learning about cells within a tissue context as it provides both spatial coordinates and transcriptomic readouts. However, existing models either ignore spatial resolution or the gene regulatory information. Gene regulation in cells can change depending on microenvironmental cues from neighboring cells, but existing models neglect gene regulatory patterns with hierarchical dependencies across levels of abstraction. In order to create contextualized representations of cells and genes from spatial transcriptomics data, we introduce HEIST, a hierarchical graph transformer-based foundation model for spatial transcriptomics and proteomics data. HEIST models tissue as spatial cellular neighborhood graphs, and each cell is, in turn, modeled as a gene regulatory network graph. The framework includes a hierarchical graph transformer that performs cross-level message passing and message passing within levels. HEIST is pre-trained on 22.3M cells from 124 tissues across 15 organs using spatially-aware contrastive learning and masked auto-encoding objectives. Unsupervised analysis of HEIST representations of cells, shows that it effectively encodes the microenvironmental influences in cell embeddings, enabling the discovery of spatially-informed subpopulations that prior models fail to differentiate. Further, HEIST achieves state-of-the-art results on four downstream task such as clinical outcome prediction, cell type annotation, gene imputation, and spatially-informed cell clustering across multiple technologies, highlighting the importance of hierarchical modeling and GRN-based representations.",Hiren Madhu; João Felipe Rocha; Tinglin Huang; Siddharth Viswanath; Smita Krishnaswamy; Rex Ying,,2025-06-11T12:29:01Z,http://arxiv.org/abs/2506.11152v1
2506.11491v2,"SemanticST: Spatially Informed Semantic Graph Learning for Clustering,   Integration, and Scalable Analysis of Spatial Transcriptomics","Spatial transcriptomics (ST) technologies enable gene expression profiling with spatial resolution, offering unprecedented insights into tissue organization and disease heterogeneity. However, current analysis methods often struggle with noisy data, limited scalability, and inadequate modelling of complex cellular relationships. We present SemanticST, a biologically informed, graph-based deep learning framework that models diverse cellular contexts through multi-semantic graph construction. SemanticST builds multiple context-specific graphs capturing spatial proximity, gene expression similarity, and tissue domain structure, and learns disentangled embeddings for each. These are fused using an attention-inspired strategy to yield a unified, biologically meaningful representation. A community-aware min-cut loss improves robustness over contrastive learning, particularly in sparse ST data. SemanticST supports mini-batch training, making it the first graph neural network scalable to large-scale datasets such as Xenium (500,000 cells). Benchmarking across four platforms (Visium, Slide-seq, Stereo-seq, Xenium) and multiple human and mouse tissues shows consistent 20 percentage gains in ARI, NMI, and trajectory fidelity over DeepST, GraphST, and IRIS. In re-analysis of breast cancer Xenium data, SemanticST revealed rare and clinically significant niches, including triple receptor-positive clusters, spatially distinct DCIS-to-IDC transition zones, and FOXC2 tumour-associated myoepithelial cells, suggesting non-canonical EMT programs with stem-like features. SemanticST thus provides a scalable, interpretable, and biologically grounded framework for spatial transcriptomics analysis, enabling robust discovery across tissue types and diseases, and paving the way for spatially resolved tissue atlases and next-generation precision medicine.",Roxana Zahedi; Ahmadreza Argha; Nona Farbehi; Ivan Bakhshayeshi; Youqiong Ye; Nigel H. Lovell; Hamid Alinejad-Rokny,,2025-06-13T06:30:48Z,http://arxiv.org/abs/2506.11491v2
2506.11942v1,"Viral Dark Matter: Illuminating Protein Function, Ecology, and   Biotechnological Promises","Viruses are the most abundant biological entities on Earth and play central roles in shaping microbiomes and influencing ecosystem functions. Yet, most viral genes remain uncharacterized, comprising what is commonly referred to as ""viral dark matter."" Metagenomic studies across diverse environments consistently show that 40-90% of viral genes lack known homologs or annotated functions. This persistent knowledge gap limits our ability to interpret viral sequence data, understand virus-host interactions, and assess the ecological or applied significance of viral genes. Among the most intriguing components of viral dark matter are auxiliary viral genes (AVGs), including auxiliary metabolic genes (AMGs), regulatory genes (AReGs), and host physiology-modifying genes (APGs), which may alter host function during infection and contribute to microbial metabolism, stress tolerance, or resistance. In this review, we explore recent advances in the discovery and functional characterization of viral dark matter. We highlight representative examples of novel viral proteins across diverse ecosystems including human microbiomes, soil, oceans, and extreme environments, and discuss what is known, and still unknown, about their roles. We then examine the bioinformatic and experimental challenges that hinder functional characterization, and present emerging strategies to overcome these barriers. Finally, we highlight both the fundamental and applied benefits that multidisciplinary efforts to characterize viral proteins can bring. By integrating computational predictions with experimental validation, and fostering collaboration across disciplines, we emphasize that illuminating viral dark matter is both feasible and essential for advancing microbial ecology and unlocking new tools for biotechnology.",James C. Kosmopoulos; Karthik Anantharaman,,2025-06-13T16:50:20Z,http://arxiv.org/abs/2506.11942v1
2506.12277v1,Evaluation of machine-learning models to measure individualized   treatment effects from randomized clinical trial data with time-to-event   outcomes,"In randomized clinical trials, regression models can be used to explore the relationships between patients' variables (e.g., clinical, pathological or lifestyle variables, and also biomarker or genomics data) and the magnitude of treatment effect. Our aim is to evaluate the value of flexible machine learning models that can incorporate interactions and nonlinear effects of high-dimensional data to estimate individualized treatment recommendations in the setting of such trials with time-to-event outcomes. We compare survival models based on neural networks (CoxCC and CoxTime) and random survival forests (Interaction Forests). A Cox model, including an adaptive LASSO penalty, is used as a benchmark. Specific metrics for individualized treatment recommendations are used: the C-for-Benefit, the E50-for-Benefit, and RMSE for treatment benefit. We conduct an extensive simulation study using 2 different data generation processes incorporating nonlinearity and interactions up to the third order. The models are applied to gene expression and clinical data from 2 breast cancer studies. The machine learning-based methods show reasonable performances on the simulation data sets, especially in terms of discrimination for Interaction Forests and calibration for the neural networks. They can be used to evaluate individualized treatment effects from randomized trials when nonlinear and interaction effects are expected to be present.",Elvire Roblin; Paul-Henry Cournède; Stefan Michiels,,2025-06-13T23:53:00Z,http://arxiv.org/abs/2506.12277v1
2506.12439v1,Interpretable Causal Representation Learning for Biological Data in the   Pathway Space,"Predicting the impact of genomic and drug perturbations in cellular function is crucial for understanding gene functions and drug effects, ultimately leading to improved therapies. To this end, Causal Representation Learning (CRL) constitutes one of the most promising approaches, as it aims to identify the latent factors that causally govern biological systems, thus facilitating the prediction of the effect of unseen perturbations. Yet, current CRL methods fail in reconciling their principled latent representations with known biological processes, leading to models that are not interpretable. To address this major issue, we present SENA-discrepancy-VAE, a model based on the recently proposed CRL method discrepancy-VAE, that produces representations where each latent factor can be interpreted as the (linear) combination of the activity of a (learned) set of biological processes. To this extent, we present an encoder, SENA-{\delta}, that efficiently compute and map biological processes' activity levels to the latent causal factors. We show that SENA-discrepancy-VAE achieves predictive performances on unseen combinations of interventions that are comparable with its original, non-interpretable counterpart, while inferring causal latent factors that are biologically meaningful.",Jesus de la Fuente; Robert Lehmann; Carlos Ruiz-Arenas; Jan Voges; Irene Marin-Goñi; Xabier Martinez-de-Morentin; David Gomez-Cabrero; Idoia Ochoa; Jesper Tegner; Vincenzo Lagani; Mikel Hernaez,,2025-06-14T10:33:19Z,http://arxiv.org/abs/2506.12439v1
2506.13344v1,LapDDPM: A Conditional Graph Diffusion Model for scRNA-seq Generation   with Spectral Adversarial Perturbations,"Generating high-fidelity and biologically plausible synthetic single-cell RNA sequencing (scRNA-seq) data, especially with conditional control, is challenging due to its high dimensionality, sparsity, and complex biological variations. Existing generative models often struggle to capture these unique characteristics and ensure robustness to structural noise in cellular networks. We introduce LapDDPM, a novel conditional Graph Diffusion Probabilistic Model for robust and high-fidelity scRNA-seq generation. LapDDPM uniquely integrates graph-based representations with a score-based diffusion model, enhanced by a novel spectral adversarial perturbation mechanism on graph edge weights. Our contributions are threefold: we leverage Laplacian Positional Encodings (LPEs) to enrich the latent space with crucial cellular relationship information; we develop a conditional score-based diffusion model for effective learning and generation from complex scRNA-seq distributions; and we employ a unique spectral adversarial training scheme on graph edge weights, boosting robustness against structural variations. Extensive experiments on diverse scRNA-seq datasets demonstrate LapDDPM's superior performance, achieving high fidelity and generating biologically-plausible, cell-type-specific samples. LapDDPM sets a new benchmark for conditional scRNA-seq data generation, offering a robust tool for various downstream biological applications.",Lorenzo Bini; Stephane Marchand-Maillet,,2025-06-16T10:35:32Z,http://arxiv.org/abs/2506.13344v1
2506.13817v1,DeepSeq: High-Throughput Single-Cell RNA Sequencing Data Labeling via   Web Search-Augmented Agentic Generative AI Foundation Models,"Generative AI foundation models offer transformative potential for processing structured biological data, particularly in single-cell RNA sequencing, where datasets are rapidly scaling toward billions of cells. We propose the use of agentic foundation models with real-time web search to automate the labeling of experimental data, achieving up to 82.5% accuracy. This addresses a key bottleneck in supervised learning for structured omics data by increasing annotation throughput without manual curation and human error. Our approach enables the development of virtual cell foundation models capable of downstream tasks such as cell-typing and perturbation prediction. As data volume grows, these models may surpass human performance in labeling, paving the way for reliable inference in large-scale perturbation screens. This application demonstrates domain-specific innovation in health monitoring and diagnostics, aligned with efforts like the Human Cell Atlas and Human Tumor Atlas Network.",Saleem A. Al Dajani; Abel Sanchez; John R. Williams,,2025-06-14T23:30:22Z,http://arxiv.org/abs/2506.13817v1
2506.15383v1,Global Ground Metric Learning with Applications to scRNA data,"Optimal transport provides a robust framework for comparing probability distributions. Its effectiveness is significantly influenced by the choice of the underlying ground metric. Traditionally, the ground metric has either been (i) predefined, e.g., as the Euclidean distance, or (ii) learned in a supervised way, by utilizing labeled data to learn a suitable ground metric for enhanced task-specific performance. Yet, predefined metrics typically cannot account for the inherent structure and varying importance of different features in the data, and existing supervised approaches to ground metric learning often do not generalize across multiple classes or are restricted to distributions with shared supports. To address these limitations, we propose a novel approach for learning metrics for arbitrary distributions over a shared metric space. Our method provides a distance between individual points like a global metric, but requires only class labels on a distribution-level for training. The learned global ground metric enables more accurate optimal transport distances, leading to improved performance in embedding, clustering and classification tasks. We demonstrate the effectiveness and interpretability of our approach using patient-level scRNA-seq data spanning multiple diseases.",Damin Kühn; Michael T. Schaub,,2025-06-18T11:53:13Z,http://arxiv.org/abs/2506.15383v1
2506.15764v1,Eukaryotic ancestry in a finite world,"Following genetic ancestry in eukaryote populations poses several open problems due to sexual reproduction and recombination. The history of extant genetic material is usually modeled backwards in time, but tracking chromosomes at a large scale is not trivial, as successive recombination events break them into several segments. For this reason, the behavior of the distribution of genetic segments across the ancestral population is not fully understood. Moreover, as individuals transmit only half of their genetic content to their offspring, after a few generations, it is possible that ghosts arise, that is, genealogical ancestors that transmit no genetic material to any individual.   While several theoretical predictions exist to estimate properties of ancestral segments or ghosts, most of them rely on simplifying assumptions such as an infinite population size or an infinite chromosome length. It is not clear how well these results hold in a finite universe, and current simulators either make other approximations or cannot handle the scale required to answer these questions. In this work, we use an exact back-in-time simulator of large diploid populations experiencing recombination that tracks genealogical and genetic ancestry, without approximations. We focus on the distinction between genealogical and genetic ancestry and, additionally, we explore the effects of genome structure on ancestral segment distribution and the proportion of genetic ancestors. Our study reveals that some of the theoretical predictions hold well in practice, but that, in several cases, it highlights discrepancies between theoretical predictions assuming infinite parameters and empirical results in finite populations, emphasizing the need for cautious application of mathematical models in biological contexts.",Juliette Luiselli; Manuel Lafond,,2025-06-18T17:57:50Z,http://arxiv.org/abs/2506.15764v1
2506.18578v1,Perfect phylogenies via the Minimum Uncovering Branching problem:   efficiently solvable cases,"In this paper, we present new efficiently solvable cases of the Minimum Uncovering Branching problem, an optimization problem with applications in cancer genomics introduced by Hujdurovi\'c, Husi\'c, Milani\v{c}, Rizzi, and Tomescu in 2018. The problem involves a family of finite sets, and the goal is to map each non-maximal set to exactly one set that contains it, minimizing the sum of uncovered elements across all sets in the family. Hujdurovi\'c et al. formulated the problem in terms of branchings of the digraph formed by the proper set inclusion relation on the input sets and studied the problem complexity based on properties of the corresponding partially ordered set, in particular, with respect to its height and width, defined respectively as the maximum cardinality of a chain and an antichain. They showed that the problem is APX-complete for instances of bounded height and that a constant-factor approximation algorithm exists for instances of bounded width, but left the exact complexity for bounded-width instances open. In this paper, we answer this question by proving that the problem is solvable in polynomial time. We derive this result by examining the structural properties of optimal solutions and reducing the problem to computing maximum matchings in bipartite graphs and maximum weight antichains in partially ordered sets. We also introduce a new polynomially computable lower bound and identify another condition for polynomial-time solvability.",Narmina Baghirova; Esther Galby; Martin Milanič,,2025-06-23T12:29:44Z,http://arxiv.org/abs/2506.18578v1
2506.18742v1,Conceptual Modelling for Life Sciences Based on Systemist Foundations,"All aspects of our society, including the life sciences, need a mechanism for people working within them to represent the concepts they employ to carry out their research. For the information systems being designed and developed to support researchers and scientists in conducting their work, conceptual models of the relevant domains are usually designed as both blueprints for a system being developed and as a means of communication between the designer and developer. Most conceptual modelling concepts are generic in the sense that they are applied with the same understanding across many applications. Problems in the life sciences, however, are especially complex and important, because they deal with humans, their well-being, and their interactions with the environment as well as other organisms. This work proposes a systemist perspective for creating a conceptual model of a life scientist's problem. We introduce the notion of a system and then show how it can be applied to the development of an information system for handling genomic-related information. We extend our discussion to show how the proposed systemist perspective can support the modelling of precision medicine. This research recognizes challenges in life sciences research of how to model problems to better represent the connections between physical and digital worlds. We propose a new notation that explicitly incorporates systemist thinking, as well as the components of systems based on recent ontological foundations. The new notation captures important semantics in the domain of life sciences. It may be used to facilitate understanding, communication and problem-solving more broadly. We also provide a precise, sound, ontologically supported characterization of the term system, as a basic construct for conceptual modelling in life sciences.",R. Lukyanenko; O. Pastor; V. C. Storey,,2025-05-23T15:31:24Z,http://arxiv.org/abs/2506.18742v1
2506.20549v2,Causal Inference for Latent Outcomes Learned with Factor Models,"In many fields$\unicode{x2013}$including genomics, epidemiology, natural language processing, social and behavioral sciences, and economics$\unicode{x2013}$it is increasingly important to address causal questions in the context of factor models or representation learning. In this work, we investigate causal effects on $\textit{latent outcomes}$ derived from high-dimensional observed data using nonnegative matrix factorization. To the best of our knowledge, this is the first study to formally address causal inference in this setting. A central challenge is that estimating a latent factor model can cause an individual's learned latent outcome to depend on other individuals' treatments, thereby violating the standard causal inference assumption of no interference. We formalize this issue as $\textit{learning-induced interference}$ and distinguish it from interference present in a data-generating process. To address this, we propose a novel, intuitive, and theoretically grounded algorithm to estimate causal effects on latent outcomes while mitigating learning-induced interference and improving estimation efficiency. We establish theoretical guarantees for the consistency of our estimator and demonstrate its practical utility through simulation studies and an application to cancer mutational signature analysis. All baseline and proposed methods are available in our open-source R package, ${\tt causalLFO}$.",Jenna M. Landy; Dafne Zorzetto; Roberta De Vito; Giovanni Parmigiani,,2025-06-25T15:47:23Z,http://arxiv.org/abs/2506.20549v2
2506.21943v1,Single-Trajectory Bayesian Modeling Reveals Multi-State Diffusion of the   MSH Sliding Clamp,"DNA mismatch repair (MMR) is the essential mechanism for preserving genomic integrity in various living organisms. In this process, MutS homologs (MSH) play crucial roles in identifying mismatched basepairs and recruiting downstream MMR proteins. The MSH protein exhibits distinct functions and diffusion dynamics before and after the recognition of mismatches while traversing along DNA. An ADP-bound MSH, known as the MSH searching clamp, scans DNA sequences via rotational diffusion along the DNA backbone. Upon recognizing a mismatch, the MSH combines with ATP molecules, forming a stable sliding clamp. Recent experimental evidence challenges the conventional view that the sliding clamp performs a simple Brownian motion. In this study, we explore the diffusion dynamics of the ATP-bound MSH sliding clamp through single-particle tracking experiments and introduce a Bayesian single-trajectory modeling framework to analyze its motion. Our quantitative analysis reveals that the diffusion characteristics defy explanation by a single-state diffusion mechanism. Instead, our in-depth model inference uncovers three distinct diffusion states, each characterized by specific diffusion coefficients. These states alternate over time, with cross-state transitions predominantly involving one intermediate state, and direct transitions between the slowest and the fastest states being scarce. We propose that these multi-state dynamics reflect underlying conformational changes in the MSH sliding clamp, highlighting a more intricate diffusion mechanism than previously appreciated.",Seongyu Park; Inho Yang; Jinseob Lee; Sinwoo Kim; Juana Martín-López; Richard Fishel; Jong-Bong Lee; Jae-Hyung Jeon,,2025-06-27T06:35:20Z,http://arxiv.org/abs/2506.21943v1
2506.22228v1,Uncovering smooth structures in single-cell data with PCS-guided   neighbor embeddings,"Single-cell sequencing is revolutionizing biology by enabling detailed investigations of cell-state transitions. Many biological processes unfold along continuous trajectories, yet it remains challenging to extract smooth, low-dimensional representations from inherently noisy, high-dimensional single-cell data. Neighbor embedding (NE) algorithms, such as t-SNE and UMAP, are widely used to embed high-dimensional single-cell data into low dimensions. But they often introduce undesirable distortions, resulting in misleading interpretations. Existing evaluation methods for NE algorithms primarily focus on separating discrete cell types rather than capturing continuous cell-state transitions, while dynamic modeling approaches rely on strong assumptions about cellular processes and specialized data. To address these challenges, we build on the Predictability-Computability-Stability (PCS) framework for reliable and reproducible data-driven discoveries. First, we systematically evaluate popular NE algorithms through empirical analysis, simulation, and theory, and reveal their key shortcomings, such as artifacts and instability. We then introduce NESS, a principled and interpretable machine learning approach to improve NE representations by leveraging algorithmic stability and to enable robust inference of smooth biological structures. NESS offers useful concepts, quantitative stability metrics, and efficient computational workflows to uncover developmental trajectories and cell-state transitions in single-cell data. Finally, we apply NESS to six single-cell datasets, spanning pluripotent stem cell differentiation, organoid development, and multiple tissue-specific lineage trajectories. Across these diverse contexts, NESS consistently yields useful biological insights, such as identification of transitional and stable cell states and quantification of transcriptional dynamics during development.",Rong Ma; Xi Li; Jingyuan Hu; Bin Yu,,2025-06-27T13:45:55Z,http://arxiv.org/abs/2506.22228v1
2506.24013v1,CoMMiT: Co-informed inference of microbiome-metabolome interactions via   transfer learning,"Recent multi-omic microbiome studies enable integrative analysis of microbes and metabolites, uncovering their associations with various host conditions. Such analyses require multivariate models capable of accounting for the complex correlation structures between microbes and metabolites. However, existing multivariate models often suffer from low statistical power for detecting microbiome-metabolome interactions due to small sample sizes and weak biological signals. To address these challenges, we introduce CoMMiT, Co-informed inference of Microbiome-Metabolome Interactions via novel Transfer learning models. Unlike conventional transfer-learning methods that borrow information from external datasets, CoMMiT leverages similarities across metabolites within a single cohort, reducing the risk of negative transfer often caused by differences in sequencing platforms and bioinformatic pipelines across studies. CoMMiT operates under the flexible assumption that auxiliary metabolites are collectively informative for the target metabolite, without requiring individual auxiliary metabolites to be informative. CoMMiT uses a novel data-driven approach to selecting the optimal set of auxiliary metabolites. Using this optimal set, CoMMiT employs a de-biasing framework to enable efficient calculation of p-values, facilitating the identification of statistically significant microbiome-metabolome interactions. Applying CoMMiT to a feeding study reveals biologically meaningful microbiome-metabolome interactions under a low glycemic load diet, demonstrating the diet-host link through gut metabolism.",Leiyue Li; Chenglong Ye; Tim Randolph; Meredith Hullar; Johanna Lampe; Marian Neuhouser; Daniel Raftery; Yue Wang,,2025-06-30T16:18:06Z,http://arxiv.org/abs/2506.24013v1
2507.05798v1,SPADE: Spatial-Aware Denoising Network for Open-vocabulary Panoptic   Scene Graph Generation with Long- and Local-range Context Reasoning,"Panoptic Scene Graph Generation (PSG) integrates instance segmentation with relation understanding to capture pixel-level structural relationships in complex scenes. Although recent approaches leveraging pre-trained vision-language models (VLMs) have significantly improved performance in the open-vocabulary setting, they commonly ignore the inherent limitations of VLMs in spatial relation reasoning, such as difficulty in distinguishing object relative positions, which results in suboptimal relation prediction. Motivated by the denoising diffusion model's inversion process in preserving the spatial structure of input images, we propose SPADE (SPatial-Aware Denoising-nEtwork) framework -- a novel approach for open-vocabulary PSG. SPADE consists of two key steps: (1) inversion-guided calibration for the UNet adaptation, and (2) spatial-aware context reasoning. In the first step, we calibrate a general pre-trained teacher diffusion model into a PSG-specific denoising network with cross-attention maps derived during inversion through a lightweight LoRA-based fine-tuning strategy. In the second step, we develop a spatial-aware relation graph transformer that captures both local and long-range contextual information, facilitating the generation of high-quality relation queries. Extensive experiments on benchmark PSG and Visual Genome datasets demonstrate that SPADE outperforms state-of-the-art methods in both closed- and open-set scenarios, particularly for spatial relationship prediction.",Xin Hu; Ke Qin; Guiduo Duan; Ming Li; Yuan-Fang Li; Tao He,,2025-07-08T09:03:24Z,http://arxiv.org/abs/2507.05798v1
2507.06049v1,FDR controlling procedures with dimension reduction and their   application to GWAS with linkage disequilibrium score,"Genome-wide association studies (GWAS) have led to the discovery of numerous single nucleotide polymorphisms (SNPs) associated with various phenotypes and complex diseases. However, the identified genetic variants do not fully explain the heritability of complex traits, known as the missing heritability problem. To address this challenge and accurately control false positives while maximizing true associations, we propose two approaches involving linkage disequilibrium (LD) scores as covariates. We apply principal component analysis (PCA), one of the dimensionality reduction techniques, to control the False Discovery Rate (FDR) in the presence of high-dimensional covariates. This method not only provides a convenient interpretation of how multiple covariates in high dimensions affect the control of FDR but also offers higher statistical power compared to cases where covariates are not used. Furthermore, we aim to investigate how covariates contribute to increasing the statistical power through various simulation experiments, comparing the results with real data examples to derive better interpretations. Using real-world datasets, including GWAS with Body Mass Index (BMI) as the phenotype, we evaluate the performance of our proposed approaches. By incorporating LD scores as covariates in FDR-controlled GWAS analyzes, we demonstrate their effectiveness in selecting informative LD scores and improving the identification of significant SNPs. Our methods alleviate computational burden and enhance interpretability while retaining essential information from LD scores. In general, our study contributes to the advancement of statistical methods in GWAS and provides practical guidance for researchers looking to improve the precision of genetic association analyses.",Dayeon Jung; Yewon Kim; Junyong Park,,2025-07-08T14:50:23Z,http://arxiv.org/abs/2507.06049v1
2507.06243v1,A Machine Learning Framework for Breast Cancer Treatment Classification   Using a Novel Dataset,"Breast cancer (BC) remains a significant global health challenge, with personalized treatment selection complicated by the disease's molecular and clinical heterogeneity. BC treatment decisions rely on various patient-specific clinical factors, and machine learning (ML) offers a powerful approach to predicting treatment outcomes. This study utilizes The Cancer Genome Atlas (TCGA) breast cancer clinical dataset to develop ML models for predicting the likelihood of undergoing chemotherapy or hormonal therapy. The models are trained using five-fold cross-validation and evaluated through performance metrics, including accuracy, precision, recall, specificity, sensitivity, F1-score, and area under the receiver operating characteristic curve (AUROC). Model uncertainty is assessed using bootstrap techniques, while SHAP values enhance interpretability by identifying key predictors. Among the tested models, the Gradient Boosting Machine (GBM) achieves the highest stable performance (accuracy = 0.7718, AUROC = 0.8252), followed by Extreme Gradient Boosting (XGBoost) (accuracy = 0.7557, AUROC = 0.8044) and Adaptive Boosting (AdaBoost) (accuracy = 0.7552, AUROC = 0.8016). These findings underscore the potential of ML in supporting personalized breast cancer treatment decisions through data-driven insights.",Md Nahid Hasan; Md Monzur Murshed; Md Mahadi Hasan; Faysal A. Chowdhury,,2025-06-23T18:33:15Z,http://arxiv.org/abs/2507.06243v1
2507.07454v1,Mix-Geneformer: Unified Representation Learning for Human and Mouse   scRNA-seq Data,"Single-cell RNA sequencing (scRNA-seq) enables single-cell transcriptomic profiling, revealing cellular heterogeneity and rare populations. Recent deep learning models like Geneformer and Mouse-Geneformer perform well on tasks such as cell-type classification and in silico perturbation. However, their species-specific design limits cross-species generalization and translational applications, which are crucial for advancing translational research and drug discovery. We present Mix-Geneformer, a novel Transformer-based model that integrates human and mouse scRNA-seq data into a unified representation via a hybrid self-supervised approach combining Masked Language Modeling (MLM) and SimCSE-based contrastive loss to capture both shared and species-specific gene patterns. A rank-value encoding scheme further emphasizes high-variance gene signals during training. Trained on about 50 million cells from diverse human and mouse organs, Mix-Geneformer matched or outperformed state-of-the-art baselines in cell-type classification and in silico perturbation tasks, achieving 95.8% accuracy on mouse kidney data versus 94.9% from the best existing model. It also successfully identified key regulatory genes validated by in vivo studies. By enabling scalable cross-species transcriptomic modeling, Mix-Geneformer offers a powerful tool for comparative transcriptomics and translational applications. While our results demonstrate strong performance, we also acknowledge limitations, such as the computational cost and variability in zero-shot transfer.",Yuki Nishio; Takayoshi Yamashita; Keita Ito; Tsubasa Hirakawa; Hironobu Fujiyoshi,,2025-07-10T06:15:17Z,http://arxiv.org/abs/2507.07454v1
2507.09890v1,Soft Graph Clustering for single-cell RNA Sequencing Data,"Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq) data analysis for elucidating cellular heterogeneity and diversity. Recent graph-based scRNA-seq clustering methods, particularly graph neural networks (GNNs), have significantly improved in tackling the challenges of high-dimension, high-sparsity, and frequent dropout events that lead to ambiguous cell population boundaries. However, their reliance on hard graph constructions derived from thresholded similarity matrices presents challenges:(i) The simplification of intercellular relationships into binary edges (0 or 1) by applying thresholds, which restricts the capture of continuous similarity features among cells and leads to significant information loss.(ii) The presence of significant inter-cluster connections within hard graphs, which can confuse GNN methods that rely heavily on graph structures, potentially causing erroneous message propagation and biased clustering outcomes. To tackle these challenges, we introduce scSGC, a Soft Graph Clustering for single-cell RNA sequencing data, which aims to more accurately characterize continuous similarities among cells through non-binary edge weights, thereby mitigating the limitations of rigid data structures. The scSGC framework comprises three core components: (i) a zero-inflated negative binomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed soft graph embedding module; and (iii) an optimal transport-based clustering optimization module. Extensive experiments across ten datasets demonstrate that scSGC outperforms 13 state-of-the-art clustering models in clustering accuracy, cell type annotation, and computational efficiency. These results highlight its substantial potential to advance scRNA-seq data analysis and deepen our understanding of cellular heterogeneity.",Ping Xu; Pengfei Wang; Zhiyuan Ning; Meng Xiao; Min Wu; Yuanchun Zhou,,2025-07-14T03:49:12Z,http://arxiv.org/abs/2507.09890v1
2507.10039v1,Towards Applying Large Language Models to Complement Single-Cell   Foundation Models,"Single-cell foundation models such as scGPT represent a significant advancement in single-cell omics, with an ability to achieve state-of-the-art performance on various downstream biological tasks. However, these models are inherently limited in that a vast amount of information in biology exists as text, which they are unable to leverage. There have therefore been several recent works that propose the use of LLMs as an alternative to single-cell foundation models, achieving competitive results. However, there is little understanding of what factors drive this performance, along with a strong focus on using LLMs as an alternative, rather than complementary approach to single-cell foundation models. In this study, we therefore investigate what biological insights contribute toward the performance of LLMs when applied to single-cell data, and introduce scMPT; a model which leverages synergies between scGPT, and single-cell representations from LLMs that capture these insights. scMPT demonstrates stronger, more consistent performance than either of its component models, which frequently have large performance gaps between each other across datasets. We also experiment with alternate fusion methods, demonstrating the potential of combining specialized reasoning models with scGPT to improve performance. This study ultimately showcases the potential for LLMs to complement single-cell foundation models and drive improvements in single-cell analysis.",Steven Palayew; Bo Wang; Gary Bader,,2025-07-14T08:16:58Z,http://arxiv.org/abs/2507.10039v1
2507.10128v1,Simultaneous Design of Microbe and Bioreactor,"When developing a biotechnological process, the microorganism is first designed, e.g., using metabolic engineering. Then, the optimum fermentation parameters are determined on a laboratory scale, and lastly, they are transferred to the bioreactor scale. However, this step-by-step approach is costly and time-consuming and may result in suboptimal configurations. Herein, we present the bilevel optimization formulation SimulKnockReactor, which connects bioreactor design with microbial strain design, an extension of our previous formulation, SimulKnock (Ziegler et al., 2024, AIChE J.). At the upper (bioreactor) level, we minimize investment and operation costs for agitation, aeration, and pH control by determining the size and operating conditions of a continuous stirred-tank reactor - without selecting specific devices like the stirrer type. The lower (cellular) level is based on flux balance analysis and implements optimal reaction knockouts predicted by the upper level. Our results with a core and a genome-scale metabolic model of Escherichia coli show that the substrate is the largest cost factor. Our simultaneous approach outperforms a sequential approach using OptKnock. Namely, the knockouts proposed by OptKnock cannot guarantee the required production capacity in all cases considered. In the case that both approaches deliver feasible results, the total annual costs are the same or lower with SimulKnockReactor, highlighting the advantage of combining cellular and bioreactor levels. This work is a further step towards a fully integrated bioprocess design.",Anita L. Ziegler; Marc-Daniel Stumm; Tim Prömper; Thomas Steimann; Jørgen Magnus; Alexander Mitsos,,2025-07-14T10:18:27Z,http://arxiv.org/abs/2507.10128v1
2507.10373v1,Post-reduction inference for confidence sets of models,"Sparsity in a regression context makes the model itself an object of interest, pointing to a confidence set of models as the appropriate presentation of evidence. A difficulty in areas such as genomics, where the number of candidate variables is vast, arises from the need for preliminary reduction prior to the assessment of models. The present paper considers a resolution using inferential separations fundamental to the Fisherian approach to conditional inference, namely, the sufficiency/co-sufficiency separation, and the ancillary/co-ancillary separation. The advantage of these separations is that no direction for departure from any hypothesised model is needed, avoiding issues that would otherwise arise from using the same data for reduction and for model assessment. In idealised cases with no nuisance parameters, the separations extract all the information in the data, solely for the purpose for which it is useful, without loss or redundancy. The extent to which estimation of nuisance parameters affects the idealised information extraction is illustrated in detail for the normal-theory linear regression model, extending immediately to a log-normal accelerated-life model for time-to-event outcomes. This idealised analysis provides insight into when sample-splitting is likely to perform as well as, or better than, the co-sufficient or ancillary tests, and when it may be unreliable. The considerations involved in extending the detailed implementation to canonical exponential-family and more general regression models are briefly discussed. As part of the analysis for the Gaussian model, we introduce a modified version of the refitted cross-validation estimator of Fan et al. (2012), whose distribution theory is exact in an appropriate conditional sense.",Heather Battey; Daniel Garcia Rasines; Yanbo Tang,,2025-07-14T15:14:27Z,http://arxiv.org/abs/2507.10373v1
2507.11588v2,SToFM: a Multi-scale Foundation Model for Spatial Transcriptomics,"Spatial Transcriptomics (ST) technologies provide biologists with rich insights into single-cell biology by preserving spatial context of cells. Building foundational models for ST can significantly enhance the analysis of vast and complex data sources, unlocking new perspectives on the intricacies of biological tissues. However, modeling ST data is inherently challenging due to the need to extract multi-scale information from tissue slices containing vast numbers of cells. This process requires integrating macro-scale tissue morphology, micro-scale cellular microenvironment, and gene-scale gene expression profile. To address this challenge, we propose SToFM, a multi-scale Spatial Transcriptomics Foundation Model. SToFM first performs multi-scale information extraction on each ST slice, to construct a set of ST sub-slices that aggregate macro-, micro- and gene-scale information. Then an SE(2) Transformer is used to obtain high-quality cell representations from the sub-slices. Additionally, we construct \textbf{SToCorpus-88M}, the largest high-resolution spatial transcriptomics corpus for pretraining. SToFM achieves outstanding performance on a variety of downstream tasks, such as tissue region semantic segmentation and cell type annotation, demonstrating its comprehensive understanding of ST data through capturing and integrating multi-scale information.",Suyuan Zhao; Yizhen Luo; Ganbo Yang; Yan Zhong; Hao Zhou; Zaiqing Nie,,2025-07-15T14:47:01Z,http://arxiv.org/abs/2507.11588v2
2507.14874v1,The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With   Graphs,"Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine (TM) both interpretable and efficient, while the power of Tsetlin automata enables accuracy comparable to deep learning on an increasing number of datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning interpretable deep clauses from graph-structured input. Moving beyond flat, fixed-length input, the GraphTM gets more versatile, supporting sequences, grids, relations, and multimodality. Through message passing, the GraphTM builds nested deep clauses to recognize sub-graph patterns with exponentially fewer clauses, increasing both interpretability and data utilization. For image classification, GraphTM preserves interpretability and achieves 3.86%-points higher accuracy on CIFAR-10 than a convolutional TM. For tracking action coreference, faced with increasingly challenging tasks, GraphTM outperforms other reinforcement learning methods by up to 20.6%-points. In recommendation systems, it tolerates increasing noise to a greater extent than a Graph Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training 2.5x faster than GCN. The GraphTM's application to these varied fields demonstrates how graph representation learning and deep clauses bring new possibilities for TM learning.",Ole-Christoffer Granmo; Youmna Abdelwahab; Per-Arne Andersen; Paul F. A. Clarke; Kunal Dumbre; Ylva Grønninsæter; Vojtech Halenka; Runar Helin; Lei Jiao; Ahmed Khalid; Rebekka Omslandseter; Rupsa Saha; Mayur Shende; Xuan Zhang,,2025-07-20T09:16:31Z,http://arxiv.org/abs/2507.14874v1
2507.16876v2,Machine learning-based multimodal prognostic models integrating   pathology images and high-throughput omic data for overall survival   prediction in cancer: a systematic review,"Multimodal machine learning integrating histopathology and molecular data shows promise for cancer prognostication. We systematically reviewed studies combining whole slide images (WSIs) and high-throughput omics to predict overall survival. Searches of EMBASE, PubMed, and Cochrane CENTRAL (12/08/2024), plus citation screening, identified eligible studies. Data extraction used CHARMS; bias was assessed with PROBAST+AI; synthesis followed SWiM and PRISMA 2020. Protocol: PROSPERO (CRD42024594745).   Forty-eight studies (all since 2017) across 19 cancer types met criteria; all used The Cancer Genome Atlas. Approaches included regularised Cox regression (n=4), classical ML (n=13), and deep learning (n=31). Reported c-indices ranged 0.550-0.857; multimodal models typically outperformed unimodal ones. However, all studies showed unclear/high bias, limited external validation, and little focus on clinical utility.   Multimodal WSI-omics survival prediction is a fast-growing field with promising results but needs improved methodological rigor, broader datasets, and clinical evaluation.   Funded by NPIC, Leeds Teaching Hospitals NHS Trust, UK (Project 104687), supported by UKRI Industrial Strategy Challenge Fund.",Charlotte Jennings; Andrew Broad; Lucy Godson; Emily Clarke; David Westhead; Darren Treanor,"National Pathology Imaging Cooperative, Leeds Teaching Hospitals NHS Trust, Leeds, UK; National Pathology Imaging Cooperative, Leeds Teaching Hospitals NHS Trust, Leeds, UK; National Pathology Imaging Cooperative, Leeds Teaching Hospitals NHS Trust, Leeds, UK; National Pathology Imaging Cooperative, Leeds Teaching Hospitals NHS Trust, Leeds, UK; University of Leeds, Leeds, UK; National Pathology Imaging Cooperative, Leeds Teaching Hospitals NHS Trust, Leeds, UK",2025-07-22T11:02:51Z,http://arxiv.org/abs/2507.16876v2
2507.17883v1,Differential Crosslinking and Contractile Motors Drive Nuclear Chromatin   Compaction,"During interphase, a typical cell nucleus features spatial compartmentalization of transcriptionally active euchromatin and repressed heterochromatin domains. In conventional nuclear organization, euchromatin predominantly occupies the nuclear interior, while heterochromatin, which is approximately 50% more dense than euchromatin, is positioned near the nuclear periphery. Peripheral chromatin organization can be further modulated by the nuclear lamina, which is itself a deformable structure. While a number of biophysical mechanisms for compartmentalization within rigid nuclei have been explored, we study a chromatin model consisting of an active, crosslinked polymer tethered to a deformable, polymeric lamina shell. Contractile motors, the deformability of the shell, and the spatial distribution of crosslinks all play pivotal roles in this compartmentalization. We find that a radial crosslink density distribution, even with a small linear differential of higher crosslinking density at the edge of the nucleus, combined with contractile motor activity, drives genomic segregation, in agreement with experimental observations. This arises from contractile motors preferentially drawing crosslinks into their vicinity at the nuclear periphery, forming high-density domains that promote heterochromatin formation. We also find an increased stiffness of nuclear wrinkles given the preferential heterochromatin compaction below the lamina shell, which is consistent with instantaneous nuclear stiffening under applied nanoindentation. We conclude with the potential for experimental validation of our model predictions.",Ligesh Theeyancheri; Edward J. Banigan; J. M. Schwarz,,2025-07-23T19:18:31Z,http://arxiv.org/abs/2507.17883v1
2507.18287v1,Dissecting the Dental Lung Cancer Axis via Mendelian Randomization and   Mediation Analysis,"Periodontitis and dental caries are common oral diseases affecting billions globally. While observational studies suggest links between these conditions and lung cancer, causality remains uncertain. This study used two sample Mendelian randomization (MR) to explore causal relationships between dental traits (periodontitis, dental caries) and lung cancer subtypes, and to assess mediation by pulmonary function. Genetic instruments were derived from the largest available genome wide association studies, including data from 487,823 dental caries and 506,594 periodontitis cases, as well as lung cancer data from the Transdisciplinary Research of Cancer in Lung consortium. Inverse variance weighting was the main analytical method; lung function mediation was assessed using the delta method. The results showed a significant positive causal effect of dental caries on overall lung cancer and its subtypes. Specifically, a one standard deviation increase in dental caries incidence was associated with a 188.0% higher risk of squamous cell lung carcinoma (OR = 2.880, 95% CI = 1.236--6.713, p = 0.014), partially mediated by declines in forced vital capacity (FVC) and forced expiratory volume in one second (FEV1), accounting for 5.124% and 5.890% of the total effect. No causal effect was found for periodontitis. These findings highlight a causal role of dental caries in lung cancer risk and support integrating dental care and pulmonary function monitoring into cancer prevention strategies.",Wenran Zhang; Huihuan Luo; Linda Wei; Ping Nie; Yiqun Wu; Dedong Yu,,2025-07-24T10:46:43Z,http://arxiv.org/abs/2507.18287v1
2507.20406v1,A Topology-Based Machine Learning Model Decisively Outperforms Flux   Balance Analysis in Predicting Metabolic Gene Essentiality,"Background: The rational identification of essential genes is a cornerstone of drug discovery, yet standard computational methods like Flux Balance Analysis (FBA) often struggle to produce accurate predictions in complex, redundant metabolic networks. Hypothesis: We hypothesized that the topological structure of a metabolic network contains a more robust predictive signal for essentiality than functional simulations alone. Methodology: To test this hypothesis, we developed a machine learning pipeline by first constructing a reaction-reaction graph from the e_coli_core metabolic model. Graph-theoretic features, including betweenness centrality and PageRank, were engineered to describe the topological role of each gene. A RandomForestClassifier was trained on these features, and its performance was rigorously benchmarked against a standard FBA single-gene deletion analysis using a curated ground-truth dataset. Results: Our machine learning model achieved a solid predictive performance with an F1-Score of 0.400 (Precision: 0.412, Recall: 0.389). In profound contrast, the standard FBA baseline method failed to correctly identify any of the known essential genes, resulting in an F1-Score of 0.000. Conclusion: This work demonstrates that a ""structure-first"" machine learning approach is a significantly superior strategy for predicting gene essentiality compared to traditional FBA on the E. coli core network. By learning the topological signatures of critical network roles, our model successfully overcomes the known limitations of simulation-based methods in handling biological redundancy. While the performance of topology-only models is expected to face challenges on more complex genome-scale networks, this validated framework represents a significant step forward and highlights the primacy of network architecture in determining biological function.",Justin Boone,,2025-07-27T20:24:11Z,http://arxiv.org/abs/2507.20406v1
2507.20426v1,ResCap-DBP: A Lightweight Residual-Capsule Network for Accurate   DNA-Binding Protein Prediction Using Global ProteinBERT Embeddings,"DNA-binding proteins (DBPs) are integral to gene regulation and cellular processes, making their accurate identification essential for understanding biological functions and disease mechanisms. Experimental methods for DBP identification are time-consuming and costly, driving the need for efficient computational prediction techniques. In this study, we propose a novel deep learning framework, ResCap-DBP, that combines a residual learning-based encoder with a one-dimensional Capsule Network (1D-CapsNet) to predict DBPs directly from raw protein sequences. Our architecture incorporates dilated convolutions within residual blocks to mitigate vanishing gradient issues and extract rich sequence features, while capsule layers with dynamic routing capture hierarchical and spatial relationships within the learned feature space. We conducted comprehensive ablation studies comparing global and local embeddings from ProteinBERT and conventional one-hot encoding. Results show that ProteinBERT embeddings substantially outperform other representations on large datasets. Although one-hot encoding showed marginal advantages on smaller datasets, such as PDB186, it struggled to scale effectively. Extensive evaluations on four pairs of publicly available benchmark datasets demonstrate that our model consistently outperforms current state-of-the-art methods. It achieved AUC scores of 98.0% and 89.5% on PDB14189andPDB1075, respectively. On independent test sets PDB2272 and PDB186, the model attained top AUCs of 83.2% and 83.3%, while maintaining competitive performance on larger datasets such as PDB20000. Notably, the model maintains a well balanced sensitivity and specificity across datasets. These results demonstrate the efficacy and generalizability of integrating global protein representations with advanced deep learning architectures for reliable and scalable DBP prediction in diverse genomic contexts.",Samiul Based Shuvo; Tasnia Binte Mamun; U Rajendra Acharya,,2025-07-27T21:54:32Z,http://arxiv.org/abs/2507.20426v1
2507.20598v1,"Nullstrap-DE: A General Framework for Calibrating FDR and Preserving   Power in DE Methods, with Applications to DESeq2 and edgeR","Differential expression (DE) analysis is a key task in RNA-seq studies, aiming to identify genes with expression differences across conditions. A central challenge is balancing false discovery rate (FDR) control with statistical power. Parametric methods such as DESeq2 and edgeR achieve high power by modeling gene-level counts using negative binomial distributions and applying empirical Bayes shrinkage. However, these methods may suffer from FDR inflation when model assumptions are mildly violated, especially in large-sample settings. In contrast, non-parametric tests like Wilcoxon offer more robust FDR control but often lack power and do not support covariate adjustment. We propose Nullstrap-DE, a general add-on framework that combines the strengths of both approaches. Designed to augment tools like DESeq2 and edgeR, Nullstrap-DE calibrates FDR while preserving power, without modifying the original method's implementation. It generates synthetic null data from a model fitted under the gene-specific null (no DE), applies the same test statistic to both observed and synthetic data, and derives a threshold that satisfies the target FDR level. We show theoretically that Nullstrap-DE asymptotically controls FDR while maintaining power consistency. Simulations confirm that it achieves reliable FDR control and high power across diverse settings, where DESeq2, edgeR, or Wilcoxon often show inflated FDR or low power. Applications to real datasets show that Nullstrap-DE enhances statistical rigor and identifies biologically meaningful genes.",Chenxin Jiang; Changhu Wang; Jingyi Jessica Li,,2025-07-28T08:09:11Z,http://arxiv.org/abs/2507.20598v1
2507.22170v1,Stacked SVD or SVD stacked? A Random Matrix Theory perspective on data   integration,"Modern data analysis increasingly requires identifying shared latent structure across multiple high-dimensional datasets. A commonly used model assumes that the data matrices are noisy observations of low-rank matrices with a shared singular subspace. In this case, two primary methods have emerged for estimating this shared structure, which vary in how they integrate information across datasets. The first approach, termed Stack-SVD, concatenates all the datasets, and then performs a singular value decomposition (SVD). The second approach, termed SVD-Stack, first performs an SVD separately for each dataset, then aggregates the top singular vectors across these datasets, and finally computes a consensus amongst them. While these methods are widely used, they have not been rigorously studied in the proportional asymptotic regime, which is of great practical relevance in today's world of increasing data size and dimensionality. This lack of theoretical understanding has led to uncertainty about which method to choose and limited the ability to fully exploit their potential. To address these challenges, we derive exact expressions for the asymptotic performance and phase transitions of these two methods and develop optimal weighting schemes to further improve both methods. Our analysis reveals that while neither method uniformly dominates the other in the unweighted case, optimally weighted Stack-SVD dominates optimally weighted SVD-Stack. We extend our analysis to accommodate multiple shared components, and provide practical algorithms for estimating optimal weights from data, offering theoretical guidance for method selection in practical data integration problems. Extensive numerical simulations and semi-synthetic experiments on genomic data corroborate our theoretical findings.",Tavor Z. Baharav; Phillip B. Nicol; Rafael A. Irizarry; Rong Ma,,2025-07-29T19:03:01Z,http://arxiv.org/abs/2507.22170v1
2508.04056v1,SCOUT: An in-vivo Methane Sensing System for Real-time Monitoring of   Enteric Emissions in Cattle with ex-vivo Validation,"Accurate measurement of enteric methane emissions remains a critical bottleneck for advancing livestock sustainability through genetic selection and precision management. Existing ambient sampling approaches suffer from low data retention rates, environmental interference, and limited temporal resolution. We developed SCOUT (Smart Cannula-mounted Optical Unit for Trace-methane), the first robust in-vivo sensing system enabling continuous, high-resolution monitoring of ruminal methane concentrations through an innovative closed-loop gas recirculation design. We conducted comprehensive validation with two cannulated Simmental heifers under contrasting dietary treatments, with cross-platform comparison against established ambient sniffer systems. SCOUT achieved exceptional performance with 82% data retention compared to 17% for conventional sniffer systems, while capturing methane concentrations 100-1000x higher than ambient approaches. Cross-platform validation demonstrated strong scale-dependent correlations, with optimal correlation strength (r = -0.564 $\pm$ 0.007) at biologically relevant 40-minute windows and 100% statistical significance. High-frequency monitoring revealed novel behavior-emission coupling, including rapid concentration changes (14.5 $\pm$ 11.3k ppm) triggered by postural transitions within 15 minutes, insights previously inaccessible through existing technologies. The SCOUT system represents a transformative advancement, enabling accurate, continuous emission phenotyping essential for genomic selection programs and sustainable precision livestock management. This validation framework establishes new benchmarks for agricultural sensor performance while generating unprecedented biological insights into ruminal methane dynamics, contributing essential tools for sustainable livestock production in climate-conscious agricultural systems.",Yuelin Deng; Hinayah Rojas de Oliveira; Richard M. Voyles; Upinder Kaur,,2025-08-06T03:31:37Z,http://arxiv.org/abs/2508.04056v1
2508.04743v1,Alz-QNet: A Quantum Regression Network for Studying Alzheimer's Gene   Interactions,"Understanding the molecular-level mechanisms underpinning Alzheimer's disease (AD) by studying crucial genes associated with the disease remains a challenge. Alzheimer's, being a multifactorial disease, requires understanding the gene-gene interactions underlying it for theranostics and progress. In this article, a novel attempt has been made using a quantum regression to decode how some crucial genes in the AD Amyloid Beta Precursor Protein ($APP$), Sterol regulatory element binding transcription factor 14 ($FGF14$), Yin Yang 1 ($YY1$), and Phospholipase D Family Member 3 ($PLD3$) etc. become influenced by other prominent switching genes during disease progression, which may help in gene expression-based therapy for AD. Our proposed Quantum Regression Network (Alz-QNet) introduces a pioneering approach with insights from the state-of-the-art Quantum Gene Regulatory Networks (QGRN) to unravel the gene interactions involved in AD pathology, particularly within the Entorhinal Cortex (EC), where early pathological changes occur. Using the proposed Alz-QNet framework, we explore the interactions between key genes ($APP$, $FGF14$, $YY1$, $EGR1$, $GAS7$, $AKT3$, $SREBF2$, and $PLD3$) within the CE microenvironment of AD patients, studying genetic samples from the database $GSE138852$, all of which are believed to play a crucial role in the progression of AD. Our investigation uncovers intricate gene-gene interactions, shedding light on the potential regulatory mechanisms that underlie the pathogenesis of AD, which help us to find potential gene inhibitors or regulators for theranostics.",Debanjan Konar; Neerav Sreekumar; Richard Jiang; Vaneet Aggarwal,,2025-08-06T04:31:49Z,http://arxiv.org/abs/2508.04743v1
2508.04747v1,GRIT: Graph-Regularized Logit Refinement for Zero-shot Cell Type   Annotation,"Cell type annotation is a fundamental step in the analysis of single-cell RNA sequencing (scRNA-seq) data. In practice, human experts often rely on the structure revealed by principal component analysis (PCA) followed by $k$-nearest neighbor ($k$-NN) graph construction to guide annotation. While effective, this process is labor-intensive and does not scale to large datasets. Recent advances in CLIP-style models offer a promising path toward automating cell type annotation. By aligning scRNA-seq profiles with natural language descriptions, models like LangCell enable zero-shot annotation. While LangCell demonstrates decent zero-shot performance, its predictions remain suboptimal, particularly in achieving consistent accuracy across all cell types. In this paper, we propose to refine the zero-shot logits produced by LangCell through a graph-regularized optimization framework. By enforcing local consistency over the task-specific PCA-based k-NN graph, our method combines the scalability of the pre-trained models with the structural robustness relied upon in expert annotation. We evaluate our approach on 14 annotated human scRNA-seq datasets from 4 distinct studies, spanning 11 organs and over 200,000 single cells. Our method consistently improves zero-shot annotation accuracy, achieving accuracy gains of up to 10%. Further analysis showcase the mechanism by which GRIT effectively propagates correct signals through the graph, pulling back mislabeled cells toward more accurate predictions. The method is training-free, model-agnostic, and serves as a simple yet effective plug-in for enhancing automated cell type annotation in practice.",Tianxiang Hu; Chenyi Zhou; Jiaxiang Liu; Jiongxin Wang; Ruizhe Chen; Haoxiang Xia; Gaoang Wang; Jian Wu; Zuozhu Liu,,2025-08-06T07:09:46Z,http://arxiv.org/abs/2508.04747v1
2508.04943v1,TRKT: Weakly Supervised Dynamic Scene Graph Generation with   Temporal-enhanced Relation-aware Knowledge Transferring,"Dynamic Scene Graph Generation (DSGG) aims to create a scene graph for each video frame by detecting objects and predicting their relationships. Weakly Supervised DSGG (WS-DSGG) reduces annotation workload by using an unlocalized scene graph from a single frame per video for training. Existing WS-DSGG methods depend on an off-the-shelf external object detector to generate pseudo labels for subsequent DSGG training. However, detectors trained on static, object-centric images struggle in dynamic, relation-aware scenarios required for DSGG, leading to inaccurate localization and low-confidence proposals. To address the challenges posed by external object detectors in WS-DSGG, we propose a Temporal-enhanced Relation-aware Knowledge Transferring (TRKT) method, which leverages knowledge to enhance detection in relation-aware dynamic scenarios. TRKT is built on two key components:(1)Relation-aware knowledge mining: we first employ object and relation class decoders that generate category-specific attention maps to highlight both object regions and interactive areas. Then we propose an Inter-frame Attention Augmentation strategy that exploits optical flow for neighboring frames to enhance the attention maps, making them motion-aware and robust to motion blur. This step yields relation- and motion-aware knowledge mining for WS-DSGG. (2) we introduce a Dual-stream Fusion Module that integrates category-specific attention maps into external detections to refine object localization and boost confidence scores for object proposals. Extensive experiments demonstrate that TRKT achieves state-of-the-art performance on Action Genome dataset. Our code is avaliable at https://github.com/XZPKU/TRKT.git.",Zhu Xu; Ting Lei; Zhimin Li; Guan Wang; Qingchao Chen; Yuxin Peng; Yang liu,,2025-08-07T00:17:45Z,http://arxiv.org/abs/2508.04943v1
2508.05692v1,SiCmiR Atlas: Single-Cell miRNA Landscapes Reveals Hub-miRNA and Network   Signatures in Human Cancers,"microRNA are pivotal post-transcriptional regulators whose single-cell behavior has remained largely inaccessible owing to technical barriers in single-cell small-RNA profiling. We present SiCmiR, a two-layer neural network that predicts miRNA expression profile from only 977 LINCS L1000 landmark genes reducing sensitivity to dropout of single-cell RNA-seq data. Proof-of-concept analyses illustrate how SiCmiR can uncover candidate hub-miRNAs in bulk-seq cell lines and hepatocellular carcinoma, scRNA-seq pancreatic ductal carcinoma and ACTH-secreting pituitary adenoma and extracellular-vesicle-mediated crosstalk in glioblastoma. Trained on 6462 TCGA paired miRNA-mRNA samples, SiCmiR attains state-of-the-art accuracy on held-out cancers and generalizes to unseen cancer types, drug perturbations and scRNA-seq. We next constructed SiCmiR-Atlas, containing 632 public datasets, 9.36 million cells, 726 cell types, which is the first dedicated database of single-cell mature miRNA expression--providing interactive visualization, biomarker identification and cell-type-resolved miRNA-target networks. SiCmiR transforms bulk-derived statistical power into a single-cell view of miRNA biology and provides a community resource SiCmiR Atlas for biomarker discovery. SiCmiR Atlas is avilable at https://awi.cuhk.edu.cn/~SiCmiR/.",Xiao-Xuan Cai; Jing-Shan Liao; Jia-Jun Ma; Yu-Xuan Pang; Yi-Gang Chen; Yang-Chi-Dung Lin; Yi-Dan Chen; Xin Cao; Yi-Cheng Zhang; Tao-Sheng Xu; Tzong-Yi Lee; Hsi-Yuan Huang; Hsien-Da Huang,,2025-08-06T12:23:08Z,http://arxiv.org/abs/2508.05692v1
2508.05800v1,Progress and new challenges in image-based profiling,"For over two decades, image-based profiling has revolutionized cellular phenotype analysis. Image-based profiling processes rich, high-throughput, microscopy data into unbiased measurements that reveal phenotypic patterns powerful for drug discovery, functional genomics, and cell state classification. Here, we review the evolving computational landscape of image-based profiling, detailing current procedures, discussing limitations, and highlighting future development directions. Deep learning has fundamentally reshaped image-based profiling, improving feature extraction, scalability, and multimodal data integration. Methodological advancements such as single-cell analysis and batch effect correction, drawing inspiration from single-cell transcriptomics, have enhanced analytical precision. The growth of open-source software ecosystems and the development of community-driven standards have further democratized access to image-based profiling, fostering reproducibility and collaboration across research groups. Despite these advancements, the field still faces significant challenges requiring innovative solutions. By focusing on the technical evolution of image-based profiling rather than the wide-ranging biological applications, our aim with this review is to provide researchers with a roadmap for navigating the progress and new challenges in this rapidly advancing domain.",Erik Serrano; John Peters; Jesko Wagner; Rebecca E. Graham; Zhenghao Chen; Brian Feng; Gisele Miranda; Alexandr A. Kalinin; Loan Vulliard; Jenna Tomkinson; Cameron Mattson; Michael J. Lippincott; Ziqi Kang; Divya Sitani; Dave Bunten; Srijit Seal; Neil O. Carragher; Anne E. Carpenter; Shantanu Singh; Paula A. Marin Zapata; Juan C. Caicedo; Gregory P. Way,,2025-08-07T19:15:37Z,http://arxiv.org/abs/2508.05800v1
2508.06641v1,Fractal Language Modelling by Universal Sequence Maps (USM),"Motivation: With the advent of Language Models using Transformers, popularized by ChatGPT, there is a renewed interest in exploring encoding procedures that numerically represent symbolic sequences at multiple scales and embedding dimensions. The challenge that encoding addresses is the need for mechanisms that uniquely retain contextual information about the succession of individual symbols, which can then be modeled by nonlinear formulations such as neural networks.   Context: Universal Sequence Maps(USM) are iterated functions that bijectively encode symbolic sequences onto embedded numerical spaces. USM is composed of two Chaos Game Representations (CGR), iterated forwardly and backwardly, that can be projected into the frequency domain (FCGR). The corresponding USM coordinates can be used to compute a Chebyshev distance metric as well as k-mer frequencies, without having to recompute the embedded numeric coordinates, and, paradoxically, allowing for non-integers values of k.   Results: This report advances the bijective fractal encoding by Universal Sequence Maps (USM) by resolving seeding biases affecting the iterated process. The resolution had two results, the first expected, the second an intriguing outcome: 1) full reconciliation of numeric positioning with sequence identity; and 2) uncovering the nature of USM as an efficient numeric process converging towards a steady state sequence embedding solution. We illustrate these results for genomic sequences because of the convenience of a planar representation defined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless, the application to alphabet of arbitrary cardinality was found to be straightforward.",Jonas S Almeida; Daniel E Russ; Susana Vinga; Ines Duarte; Lee Mason; Praphulla Bhawsar; Aaron Ge; Arlindo Oliveira; Jeya Balaji Balasubramanian,,2025-08-08T18:41:13Z,http://arxiv.org/abs/2508.06641v1
2508.07043v1,K-Dense Analyst: Towards Fully Automated Scientific Analysis,"The complexity of modern bioinformatics analysis has created a critical gap between data generation and developing scientific insights. While large language models (LLMs) have shown promise in scientific reasoning, they remain fundamentally limited when dealing with real-world analytical workflows that demand iterative computation, tool integration and rigorous validation. We introduce K-Dense Analyst, a hierarchical multi-agent system that achieves autonomous bioinformatics analysis through a dual-loop architecture. K-Dense Analyst, part of the broader K-Dense platform, couples planning with validated execution using specialized agents to decompose complex objectives into executable, verifiable tasks within secure computational environments. On BixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense Analyst achieves 29.2% accuracy, surpassing the best-performing language model (GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what is widely considered the most powerful LLM available. Remarkably, K-Dense Analyst achieves this performance using Gemini 2.5 Pro, which attains only 18.3% accuracy when used directly, demonstrating that our architectural innovations unlock capabilities far beyond the underlying model's baseline performance. Our insights demonstrate that autonomous scientific reasoning requires more than enhanced language models, it demands purpose-built systems that can bridge the gap between high-level scientific objectives and low-level computational execution. These results represent a significant advance toward fully autonomous computational biologists capable of accelerating discovery across the life sciences.",Orion Li; Vinayak Agarwal; Summer Zhou; Ashwin Gopinath; Timothy Kassis,,2025-08-09T16:59:55Z,http://arxiv.org/abs/2508.07043v1
2508.07345v1,ProteoKnight: Convolution-based phage virion protein classification and   uncertainty analysis,"\textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is essential for genomic studies due to their crucial role as structural elements in bacteriophages. Computational tools, particularly machine learning, have emerged for annotating phage protein sequences from high-throughput sequencing. However, effective annotation requires specialized sequence encodings. Our paper introduces ProteoKnight, a new image-based encoding method that addresses spatial constraints in existing techniques, yielding competitive performance in PVP classification using pre-trained convolutional neural networks. Additionally, our study evaluates prediction uncertainty in binary PVP classification through Monte Carlo Dropout (MCD). \textbf{Methods:} ProteoKnight adapts the classical DNA-Walk algorithm for protein sequences, incorporating pixel colors and adjusting walk distances to capture intricate protein features. Encoded sequences were classified using multiple pre-trained CNNs. Variance and entropy measures assessed prediction uncertainty across proteins of various classes and lengths. \textbf{Results:} Our experiments achieved 90.8% accuracy in binary classification, comparable to state-of-the-art methods. Multi-class classification accuracy remains suboptimal. Our uncertainty analysis unveils variability in prediction confidence influenced by protein class and sequence length. \textbf{Conclusions:} Our study surpasses frequency chaos game representation (FCGR) by introducing novel image encoding that mitigates spatial information loss limitations. Our classification technique yields accurate and robust PVP predictions while identifying low-confidence predictions.",Samiha Afaf Neha; Abir Ahammed Bhuiyan; Md. Ishrak Khan,,2025-08-10T13:45:08Z,http://arxiv.org/abs/2508.07345v1
2508.08331v2,miRKatAI: An Integrated Database and Multi-agent AI system for microRNA   Research,"MicroRNAs (miRs) are robust regulators of gene expression, implicated in most biological processes. microRNAs predominantly downregulate the expression of genes post-transcriptionally and each miR is predicted to target several hundred genes. The accurate identification and annotation of miR-mRNA target interactions is central to understanding miRs function and their therapeutic potential. However, computational target prediction is challenging due to imperfect complementarity of miRs with their targets and the growing volume and heterogeneity of experimental data present challenges in accessing, integrating, and analysing miR-target interaction information across biological contexts. This creates a need for integrated resources and intelligent query tools.   We present the miRKat Suite, comprising miRKatDB, a comprehensive, curated database of predicted and validated miR-target interactions and associated annotations, and miRKatAI, a multi-agent system powered by large language models (LLMs) and LangGraph. miRKatDB integrates data from multiple publicly available sources, providing a comprehensive foundation for miR studies, including miR target genes and changes in levels of tissue expression previously reported. miRKatAI offers a natural language interface for complex querying of miRKatDB, facilitates grounded information retrieval from established sources in the field, and supports basic data visualisation. The miRKat Suite aims to accelerate miR research by streamlining data access, enhancing exploratory analysis, and supporting hypothesis generation.",Karen Guerrero-Vazquez; Jacopo Umberto Verga; Pilib O Broin; Eva Szegezdi; Katarzyna Goljanek-Whysall,,2025-08-10T11:24:40Z,http://arxiv.org/abs/2508.08331v2
2508.08567v1,Achievable Rates of Nanopore-based DNA Storage,"This paper studies achievable rates of nanopore-based DNA storage when nanopore signals are decoded using a tractable channel model that does not rely on a basecalling algorithm. Specifically, the noisy nanopore channel (NNC) with the Scrappie pore model generates average output levels via i.i.d. geometric sample duplications corrupted by i.i.d. Gaussian noise (NNC-Scrappie). Simplified message passing algorithms are derived for efficient soft decoding of nanopore signals using NNC-Scrappie. Previously, evaluation of this channel model was limited by the lack of DNA storage datasets with nanopore signals included. This is solved by deriving an achievable rate based on the dynamic time-warping (DTW) algorithm that can be applied to genomic sequencing datasets subject to constraints that make the resulting rate applicable to DNA storage. Using a publicly-available dataset from Oxford Nanopore Technologies (ONT), it is demonstrated that coding over multiple DNA strands of $100$ bases in length and decoding with the NNC-Scrappie decoder can achieve rates of at least $0.64-1.18$ bits per base, depending on the channel quality of the nanopore that is chosen in the sequencing device per channel-use, and $0.96$ bits per base on average assuming uniformly chosen nanopores. These rates are pessimistic since they only apply to single reads and do not include calibration of the pore model to specific nanopores.",Brendon McBain; Emanuele Viterbo,,2025-08-12T02:11:52Z,http://arxiv.org/abs/2508.08567v1
2508.13201v1,Benchmarking LLM-based Agents for Single-cell Omics Analysis,"The surge in multimodal single-cell omics data exposes limitations in traditional, manually defined analysis workflows. AI agents offer a paradigm shift, enabling adaptive planning, executable code generation, traceable decisions, and real-time knowledge fusion. However, the lack of a comprehensive benchmark critically hinders progress. We introduce a novel benchmarking evaluation system to rigorously assess agent capabilities in single-cell omics analysis. This system comprises: a unified platform compatible with diverse agent frameworks and LLMs; multidimensional metrics assessing cognitive program synthesis, collaboration, execution efficiency, bioinformatics knowledge integration, and task completion quality; and 50 diverse real-world single-cell omics analysis tasks spanning multi-omics, species, and sequencing technologies. Our evaluation reveals that Grok-3-beta achieves state-of-the-art performance among tested agent frameworks. Multi-agent frameworks significantly enhance collaboration and execution efficiency over single-agent approaches through specialized role division. Attribution analyses of agent capabilities identify that high-quality code generation is crucial for task success, and self-reflection has the most significant overall impact, followed by retrieval-augmented generation (RAG) and planning. This work highlights persistent challenges in code generation, long-context handling, and context-aware knowledge retrieval, providing a critical empirical foundation and best practices for developing robust AI agents in computational biology.",Yang Liu; Lu Zhou; Ruikun He; Rongbo Shen; Yixue Li,,2025-08-16T04:26:18Z,http://arxiv.org/abs/2508.13201v1
2508.14232v1,Extending a Phylogeny-based Method for Detecting Signatures of   Multi-level Selection for Applications in Artificial Life,"Multilevel selection occurs when short-term individual-level reproductive interests conflict with longer-term group-level fitness effects. Detecting and quantifying this phenomenon is key to understanding evolution of traits ranging from multicellularity to pathogen virulence. Multilevel selection is particularly important in artificial life research due to its connection to major evolutionary transitions, a hallmark of open-ended evolution. Bonetti Franceschi & Volz (2024) proposed to detect multilevel selection dynamics by screening for mutations that appear more often in a population than expected by chance (due to individual-level fitness benefits) but are ultimately associated with negative longer-term fitness outcomes (i.e., smaller, shorter-lived descendant clades). Here, we use agent-based modeling with known ground truth to assess the efficacy of this approach. To test these methods under challenging conditions broadly comparable to the original dataset explored by Bonetti Franceschi & Volz (2024), we use an epidemiological framework to model multilevel selection in trade-offs between within-host growth rate and between-host transmissibility. To achieve success on our in silico data, we develop an alternate normalization procedure for identifying clade-level fitness effects. We find the method to be sensitive in detecting genome sites under multilevel selection with 30% effect sizes on fitness, but do not see sensitivity to smaller 10% mutation effect sizes. To test the robustness of this methodology, we conduct additional experiments incorporating extrinsic, time-varying environmental changes and adaptive turnover in population compositions, and find that screen performance remains generally consistent with baseline conditions. This work represents a promising step towards rigorous generalizable quantification of multilevel selection effects.",Matthew Andres Moreno; Sanaz Hasanzadeh Fard; Luis Zaman; Emily Dolson,,2025-08-19T19:42:38Z,http://arxiv.org/abs/2508.14232v1
2508.14568v1,Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single   Bootstrap per Cell,"This paper presents a novel approach to calculating the Levenshtein (edit) distance within the framework of Fully Homomorphic Encryption (FHE), specifically targeting third-generation schemes like TFHE. Edit distance computations are essential in applications across finance and genomics, such as DNA sequence alignment. We introduce an optimised algorithm that significantly reduces the cost of edit distance calculations called Leuvenshtein. This algorithm specifically reduces the number of programmable bootstraps (PBS) needed per cell of the calculation, lowering it from approximately 94 operations -- required by the conventional Wagner-Fisher algorithm -- to just 1. Additionally, we propose an efficient method for performing equality checks on characters, reducing ASCII character comparisons to only 2 PBS operations. Finally, we explore the potential for further performance improvements by utilising preprocessing when one of the input strings is unencrypted. Our Leuvenshtein achieves up to $278\times$ faster performance compared to the best available TFHE implementation and up to $39\times$ faster than an optimised implementation of the Wagner-Fisher algorithm. Moreover, when offline preprocessing is possible due to the presence of one unencrypted input on the server side, an additional $3\times$ speedup can be achieved.",Wouter Legiest; Jan-Pieter D'Anvers; Bojan Spasic; Nam-Luc Tran; Ingrid Verbauwhede,,2025-08-20T09:40:06Z,http://arxiv.org/abs/2508.14568v1
2508.14997v1,Yeast growth is controlled by the proportional scaling of mRNA and   ribosome concentrations,"Despite growth being fundamental to all aspects of cell biology, we do not yet know its organizing principles in eukaryotic cells. Classic models derived from the bacteria E. coli posit that protein-synthesis rates are set by mass-action collisions between charged tRNAs produced by metabolic enzymes and mRNA-bound ribosomes. These models show that faster growth is achieved by simultaneously raising both ribosome content and peptide elongation speed. Here, we test if these models are valid for eukaryotes by combining single-molecule tracking, spike-in RNA sequencing, and proteomics in 15 carbon- and nitrogen-limited conditions using the budding yeast S. cerevisiae. Ribosome concentration increases linearly with growth rate, as in bacteria, but the peptide elongation speed remains constant (~9 amino acids/s) and charged tRNAs are not limiting. Total mRNA concentration rises in direct proportion to ribosomes, driven by enhanced RNA polymerase II occupancy of the genome. We show that a simple kinetic model of mRNA-ribosome binding predicts both the fraction of active ribosomes, the growth rate, and responses to transcriptional perturbations. Yeast accelerate growth by coordinately and proportionally co-up-regulating total mRNA and ribosome concentrations, not by speeding elongation. Taken together, our work establishes a new framework for eukaryotic growth control and resource allocation.",Xin Gao; Michael Lanz; Rosslyn Grosely; Jonas Cremer; Joseph Puglisi; Jan M. Skotheim,,2025-08-20T18:33:45Z,http://arxiv.org/abs/2508.14997v1
2508.15074v1,A Scalable Trie Building Algorithm for High-Throughput Phyloanalysis of   Wafer-Scale Digital Evolution Experiments,"Agent-based simulation platforms play a key role in enabling fast-to-run evolution experiments that can be precisely controlled and observed in detail. Availability of high-resolution snapshots of lineage ancestries from digital experiments, in particular, is key to investigations of evolvability and open-ended evolution, as well as in providing a validation testbed for bioinformatics method development. Ongoing advances in AI/ML hardware accelerator devices, such as the 850,000-processor Cerebras Wafer-Scale Engine (WSE), are poised to broaden the scope of evolutionary questions that can be investigated in silico. However, constraints in memory capacity and locality characteristic of these systems introduce difficulties in exhaustively tracking phylogenies at runtime. To overcome these challenges, recent work on hereditary stratigraphy algorithms has developed space-efficient genetic markers to facilitate fully decentralized estimation of relatedness among digital organisms. However, in existing work, compute time to reconstruct phylogenies from these genetic markers has proven a limiting factor in achieving large-scale phyloanalyses. Here, we detail an improved trie-building algorithm designed to produce reconstructions equivalent to existing approaches. For modestly-sized 10,000-tip trees, the proposed approach achieves a 300-fold speedup versus existing state-of-the-art. Finally, using 1 billion genome datasets drawn from WSE simulations encompassing 954 trillion replication events, we report a pair of large-scale phylogeny reconstruction trials, achieving end-to-end reconstruction times of 2.6 and 2.9 hours. In substantially improving reconstruction scaling and throughput, presented work establishes a key foundation to enable powerful high-throughput phyloanalysis techniques in large-scale digital evolution experiments.",Vivaan Singhvi; Joey Wagner; Emily Dolson; Luis Zaman; Matthew Andres Moreno,,2025-08-20T21:18:51Z,http://arxiv.org/abs/2508.15074v1
2508.17083v1,Learning ON Large Datasets Using Bit-String Trees,"This thesis develops computational methods in similarity-preserving hashing, classification, and cancer genomics. Standard space partitioning-based hashing relies on Binary Search Trees (BSTs), but their exponential growth and sparsity hinder efficiency. To overcome this, we introduce Compressed BST of Inverted hash tables (ComBI), which enables fast approximate nearest-neighbor search with reduced memory. On datasets of up to one billion samples, ComBI achieves 0.90 precision with 4X-296X speed-ups over Multi-Index Hashing, and also outperforms Cellfishing.jl on single-cell RNA-seq searches with 2X-13X gains. Building on hashing structures, we propose Guided Random Forest (GRAF), a tree-based ensemble classifier that integrates global and local partitioning, bridging decision trees and boosting while reducing generalization error. Across 115 datasets, GRAF delivers competitive or superior accuracy, and its unsupervised variant (uGRAF) supports guided hashing and importance sampling. We show that GRAF and ComBI can be used to estimate per-sample classifiability, which enables scalable prediction of cancer patient survival. To address challenges in interpreting mutations, we introduce Continuous Representation of Codon Switches (CRCS), a deep learning framework that embeds genetic changes into numerical vectors. CRCS allows identification of somatic mutations without matched normals, discovery of driver genes, and scoring of tumor mutations, with survival prediction validated in bladder, liver, and brain cancers. Together, these methods provide efficient, scalable, and interpretable tools for large-scale data analysis and biomedical applications.",Prashant Gupta,,2025-08-23T16:49:42Z,http://arxiv.org/abs/2508.17083v1
2508.18304v1,scI2CL: Effectively Integrating Single-cell Multi-omics by Intra- and   Inter-omics Contrastive Learning,"Single-cell multi-omics data contain huge information of cellular states, and analyzing these data can reveal valuable insights into cellular heterogeneity, diseases, and biological processes. However, as cell differentiation \& development is a continuous and dynamic process, it remains challenging to computationally model and infer cell interaction patterns based on single-cell multi-omics data. This paper presents scI2CL, a new single-cell multi-omics fusion framework based on intra- and inter-omics contrastive learning, to learn comprehensive and discriminative cellular representations from complementary multi-omics data for various downstream tasks. Extensive experiments of four downstream tasks validate the effectiveness of scI2CL and its superiority over existing peers. Concretely, in cell clustering, scI2CL surpasses eight state-of-the-art methods on four widely-used real-world datasets. In cell subtyping, scI2CL effectively distinguishes three latent monocyte cell subpopulations, which are not discovered by existing methods. Simultaneously, scI2CL is the only method that correctly constructs the cell developmental trajectory from hematopoietic stem and progenitor cells to Memory B cells. In addition, scI2CL resolves the misclassification of cell types between two subpopulations of CD4+ T cells, while existing methods fail to precisely distinguish the mixed cells. In summary, scI2CL can accurately characterize cross-omics relationships among cells, thus effectively fuses multi-omics data and learns discriminative cellular representations to support various downstream analysis tasks.",Wuchao Liu; Han Peng; Wengen Li; Yichao Zhang; Jihong Guan; Shuigeng Zhou,,2025-08-23T01:42:28Z,http://arxiv.org/abs/2508.18304v1
2508.21144v1,DNA Dynamics in Dual Nanopore Tug-of-War,"Solid state nanopores have emerged as powerful tools for single-molecule sensing, yet the rapid uncontrolled translocation of the molecule through the pore remains a key limitation. We have previously demonstrated that an active dual-nanopore system, consisting of two closely spaced pores operated via feedback controlled biasing, shows promise in achieving controlled, slowed-down translocation. Translocation control is achieved via capturing the DNA in a special tug-of-war configuration, whereby opposing electrophoretic forces at each pore are applied to a DNA molecule co-captured at the two pores. Here, we systematically explore translocation physics during DNA tug-of-war focusing on genomically relevant longer dsDNA using a T$_4$-DNA model (166\,kbp). We find that longer molecules can be trapped in tug-of-war states with an asymmetric partitioning of contour between the pores. Secondly, we explore the physics of DNA disengagement from a tug-of-war configuration, focusing on the dynamics of DNA free-end escape, in particular how the free-end velocity depends on pore voltage, DNA size and the presence of additional DNA strands between the pores (i.e. arising in the presence of folded translocation). These findings validate theoretical predictions derived from a first passage model and provide new insight into the physical mechanisms governing molecule disengagement in tug-of-war.",Zezhou Liu; Wangwei Dong; Thomas St-Denis; Matheus Azevedo Silva Pessôa; Sajad Shiekh; Preethi Ravikumar; Walter Reisner,,2025-08-28T18:23:48Z,http://arxiv.org/abs/2508.21144v1
2509.00222v1,Strategies to search for two-dimensional materials with long spin qubit   coherence time,"Two-dimensional (2D) materials that can host qubits with long spin coherence time (T2) have the distinct advantage of integrating easily with existing microelectronic and photonic platforms, making them attractive for designing novel quantum devices with enhanced performance. However, the relative lack of 2D materials as spin qubit hosts, as well as appropriate substrates that can help maintain long T2, necessitates a strategy to search for candidates with robust spin coherence. Here, we develop a high-throughput computational workflow to predict the nuclear spin bath-driven qubit decoherence and T2 in 2D materials and heterostructures. We initially screen 1173 2D materials and find 190 monolayers with T2 > 1 ms, higher than that of naturally-abundant diamond. We then construct 1554 lattice-commensurate heterostructures between high-T2 2D materials and select 3D substrates, and we find that T2 is generally lower in a heterostructure than in the bare 2D host material; however, low-noise substrates (such as CeO2 and CaO) can help maintain high T2. To further accelerate the material screening effort, we derive analytical models that enable rapid predictions of T2 for 2D materials and heterotructures. The models offer a simple, yet quantitative, way to determine the relative contributions to decoherence from the nuclear spin baths of the 2D host and substrate in a heterostructural system. By developing a high-throughput workflow and analytical models, we expand the genome of 2D materials and their spin coherence times for the development of spin qubit platforms.",Michael Y. Toriyama; Jiawei Zhan; Shun Kanai; Giulia Galli,,2025-08-29T20:12:14Z,http://arxiv.org/abs/2509.00222v1
2509.01086v1,Almost Tight Approximation Hardness and Online Algorithms for Resource   Scheduling,"We study the precedence-constrained resource scheduling problem [SICOMP'75]. There are $n$ jobs where each job takes a certain time to finish and has a resource requirement throughout the execution time. There are precedence among the jobs. The problem asks that given a resource budget, schedule the jobs obeying the precedence constraints to minimize makespan (maximum completion time of a job) such that at any point in time, the total resource being used by all the jobs is at most the given resource budget. In the offline setting, an important open question is whether a polynomial-time $O(1)$-factor approximation algorithm can be found. We prove almost tight hardness of approximation: For some constant $\alpha > 0$, there is no $o((\log t_{\max})^{\alpha})$-factor ( or $o( ( \log n )^\alpha )$-factor ) approximation algorithm with $n$ jobs of maximum job length $t_{\max}$, unless P = NP ( or NP $\subset$ DTIME$(O( 2^{\text{polylog}(n)}))$ ).   We further show a connection between this scheduling problem and a seemingly unrelated problem called the shortest common super-sequence (SCS) problem, which has wide application in Biology and Genomics. We prove that an $o(\log t_{\max})$-factor approximation of the scheduling problem would imply the existence of an $o(|\Sigma|)$-approximation algorithm for SCS with alphabet $\Sigma$.   We then consider the online setting. We present $\Omega(\log n)$ and $\Omega(\log t_{\max})$ lower bounds of the competitive ratio of any randomized online algorithm. Moreover, we present a matching $O(\min\{\log n, \log t_{\max}\})$-competitive deterministic online algorithm.",Rathish Das; Hao Sun,,2025-09-01T03:18:20Z,http://arxiv.org/abs/2509.01086v1
2509.02639v1,Enhanced Single-Cell RNA-seq Embedding through Gene Expression and   Data-Driven Gene-Gene Interaction Integration,"Single-cell RNA sequencing (scRNA-seq) provides unprecedented insights into cellular heterogeneity, enabling detailed analysis of complex biological systems at single-cell resolution. However, the high dimensionality and technical noise inherent in scRNA-seq data pose significant analytical challenges. While current embedding methods focus primarily on gene expression levels, they often overlook crucial gene-gene interactions that govern cellular identity and function. To address this limitation, we present a novel embedding approach that integrates both gene expression profiles and data-driven gene-gene interactions. Our method first constructs a Cell-Leaf Graph (CLG) using random forest models to capture regulatory relationships between genes, while simultaneously building a K-Nearest Neighbor Graph (KNNG) to represent expression similarities between cells. These graphs are then combined into an Enriched Cell-Leaf Graph (ECLG), which serves as input for a graph neural network to compute cell embeddings. By incorporating both expression levels and gene-gene interactions, our approach provides a more comprehensive representation of cellular states. Extensive evaluation across multiple datasets demonstrates that our method enhances the detection of rare cell populations and improves downstream analyses such as visualization, clustering, and trajectory inference. This integrated approach represents a significant advance in single-cell data analysis, offering a more complete framework for understanding cellular diversity and dynamics.",Hojjat Torabi Goudarzi; Maziyar Baran Pouyan,,2025-09-01T21:19:27Z,http://arxiv.org/abs/2509.02639v1
2509.05661v1,OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph   Anticipation,"A scene graph is a structured represention of objects and their relationships in a scene. Scene Graph Anticipation (SGA) involves predicting future scene graphs from video clips, enabling applications as intelligent surveillance and human-machine collaboration. Existing SGA approaches primarily leverage visual cues, often struggling to integrate valuable commonsense knowledge, thereby limiting long-term prediction robustness. To explicitly leverage such commonsense knowledge, we propose a new approach to better understand the objects, concepts, and relationships in a scene graph. Our approach decouples the SGA task in two steps: first a scene graph capturing model is used to convert a video clip into a sequence of scene graphs, then a pure text-based model is used to predict scene graphs in future frames. Our focus in this work is on the second step, and we call it Linguistic Scene Graph Anticipation (LSGA) and believes it should have independent interest beyond the use in SGA discussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method (OOTSM) where an Large Language Model (LLM) first forecasts object appearances and disappearances before generating detailed human-object relations. We conduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we evaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o, GPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome annotations. For SGA, we combine our OOTSM with STTran++ from, and our experiments demonstrate effective state-of-the-art performance: short-term mean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves dramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.",Xiaomeng Zhu; Changwei Wang; Haozhe Wang; Xinyu Liu; Fangzhen Lin,,2025-09-06T09:35:15Z,http://arxiv.org/abs/2509.05661v1
2501.00767v1,"Infectious diseases, imposing density-dependent mortality on MHC/HLA   variation, can account for balancing selection and MHC/HLA polymorphism","The human MHC transplantation loci (HLA-A, -B, -C, -DPB1, -DQB1, -DRB1) are the most polymorphic in the human genome. It is generally accepted this polymorphism reflects a role in presenting pathogen-derived peptide to the adaptive immune system. Proposed mechanisms for the polymorphism such as negative frequency-dependent selection (NFDS) and heterozygote advantage (HA) focus on HLA alleles, not haplotypes. Here, we propose a model for the polymorphism in which infectious diseases impose independent density-dependent regulation on HLA haplotypes. More specifically, a complex pathogen environment drives extensive host polymorphism through a guild of HLA haplotypes that are specialised and show incomplete peptide recognition. Separation of haplotype guilds is maintained by limiting similarity. The outcome is a wide and stable range of haplotype densities at steady-state in which effective Fisher fitnesses are zero. Densities, and therefore frequencies, emerge theoretically as alternative measures of fitness. A catalogue of ranked frequencies is therefore one of ranked fitnesses. The model is supported by data from a range of sources including a Caucasian HLA dataset compiled by the US National Marrow Donor Program (NMDP). These provide evidence of positive selection on the top 350-2000 5-locus HLA haplotypes taken from an overall NMDP sample set of 10E5. High-fitness haplotypes drive the selection of 137 high-frequency alleles spread across the 5 HLA loci under consideration. These alleles demonstrate positive epistasis and pleiotropy in the formation of haplotypes. Allelic pleiotropy creates a network of highly inter-related HLA haplotypes that account for 97% of the census sample. We suggest this network has properties of a quasi-species and is itself under selection. We also suggest this is the origin of balancing selection in the HLA system.",D. P. L. Green,,2025-01-01T08:06:55Z,http://arxiv.org/abs/2501.00767v1
2501.06548v1,Unsupervised detection and fitness estimation of emerging SARS-CoV-2   variants. Application to wastewater samples (ANRS0160),"Repeated waves of emerging variants during the SARS-CoV-2 pandemics have highlighted the urge of collecting longitudinal genomic data and developing statistical methods based on time series analyses for detecting new threatening lineages and estimating their fitness early in time. Most models study the evolution of the prevalence of particular lineages over time and require a prior classification of sequences into lineages. Such process is prone to induce delays and bias. More recently, few authors studied the evolution of the prevalence of mutations over time with alternative clustering approaches, avoiding specific lineage classification. Most of the aforementioned methods are however either non parametric or unsuited to pooled data characterizing, for instance, wastewater samples. In this context, we propose an alternative unsupervised method for clustering mutations according to their frequency trajectory over time and estimating group fitness from time series of pooled mutation prevalence data. Our model is a mixture of observed count data and latent group assignment and we use the expectation-maximization algorithm for model selection and parameter estimation. The application of our method to time series of SARS-CoV-2 sequencing data collected from wastewater treatment plants in France from October 2020 to April 2021 shows its ability to agnostically group mutations according to their probability of belonging to B.1.160, Alpha, Beta, B.1.177 variants with selection coefficient estimates per group in coherence with the viral dynamics in France reported by Nextstrain. Moreover, our method detected the Alpha variant as threatening as early as supervised methods (which track specific mutations over time) with the noticeable difference that, since unsupervised, it does not require any prior information on the set of mutations.",Alexandra Lefebvre; Vincent Maréchal; Arnaud Gloaguen; Obépine Consortium; Amaury Lambert; Yvon Maday,,2025-01-11T14:02:25Z,http://arxiv.org/abs/2501.06548v1
2501.11760v3,Hypergraph Representations of scRNA-seq Data for Improved Clustering   with Random Walks,"Analysis of single-cell RNA sequencing data is often conducted through network projections such as coexpression networks, primarily due to the abundant availability of network analysis tools for downstream tasks. However, this approach has several limitations: loss of higher-order information, inefficient data representation caused by converting a sparse dataset to a fully connected network, and overestimation of coexpression due to zero-inflation. To address these limitations, we propose conceptualizing scRNA-seq expression data as hypergraphs, which are generalized graphs in which the hyperedges can connect more than two vertices. In the context of scRNA-seq data, the hypergraph nodes represent cells and the edges represent genes. Each hyperedge connects all cells where its corresponding gene is actively expressed and records the expression of the gene across different cells. This hypergraph conceptualization enables us to explore multi-way relationships beyond the pairwise interactions in coexpression networks without loss of information. We propose two novel clustering methods: (1) the Dual-Importance Preference Hypergraph Walk (DIPHW) and (2) the Coexpression and Memory-Integrated Dual-Importance Preference Hypergraph Walk (CoMem-DIPHW). They outperform established methods on both simulated and real scRNA-seq datasets. The improvement brought by our proposed methods is especially significant when data modularity is weak. Furthermore, CoMem-DIPHW incorporates the gene coexpression network, cell coexpression network, and the cell-gene expression hypergraph from the single-cell abundance counts data altogether for embedding computation. This approach accounts for both the local level information from single-cell level gene expression and the global level information from the pairwise similarity in the two coexpression networks.",Wan He; Daniel I. Bolnick; Samuel V. Scarpino; Tina Eliassi-Rad,,2025-01-20T21:39:10Z,http://arxiv.org/abs/2501.11760v3
2501.14248v1,Normalization and selecting non-differentially expressed genes improve   machine learning modelling of cross-platform transcriptomic data,"Normalization is a critical step in quantitative analyses of biological processes. Recent works show that cross-platform integration and normalization enable machine learning (ML) training on RNA microarray and RNA-seq data, but no independent datasets were used in their studies. Therefore, it is unclear how to improve ML modelling performance on independent RNA array and RNA-seq based datasets. Inspired by the house-keeping genes that are commonly used in experimental biology, this study tests the hypothesis that non-differentially expressed genes (NDEG) may improve normalization of transcriptomic data and subsequently cross-platform modelling performance of ML models. Microarray and RNA-seq datasets of the TCGA breast cancer were used as independent training and test datasets, respectively, to classify the molecular subtypes of breast cancer. NDEG (p>0.85) and differentially expressed genes (DEG, p<0.05) were selected based on the p values of ANOVA analysis and used for subsequent data normalization and classification, respectively. Models trained based on data from one platform were used for testing on the other platform. Our data show that NDEG and DEG gene selection could effectively improve the model classification performance. Normalization methods based on parametric statistical analysis were inferior to those based on nonparametric statistics. In this study, the LOG_QN and LOG_QNZ normalization methods combined with the neural network classification model seem to achieve better performance. Therefore, NDEG-based normalization appears useful for cross-platform testing on completely independent datasets. However, more studies are required to examine whether NDEG-based normalization can improve ML classification performance in other datasets and other omic data types.",Fei Deng; Catherine H Feng; Nan Gao; Lanjing Zhang,"Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ; Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ; Department of Biological Sciences, School of Arts & Sciences, Rutgers University, Newark, NJ; Department of Chemical Biology, Ernest Mario School of Pharmacy, Rutgers University, Piscataway, NJ",2025-01-24T05:23:51Z,http://arxiv.org/abs/2501.14248v1
2502.10807v2,HybriDNA: A Hybrid Transformer-Mamba2 Long-Range DNA Language Model,"Advances in natural language processing and large language models have sparked growing interest in modeling DNA, often referred to as the ""language of life"". However, DNA modeling poses unique challenges. First, it requires the ability to process ultra-long DNA sequences while preserving single-nucleotide resolution, as individual nucleotides play a critical role in DNA function. Second, success in this domain requires excelling at both generative and understanding tasks: generative tasks hold potential for therapeutic and industrial applications, while understanding tasks provide crucial insights into biological mechanisms and diseases. To address these challenges, we propose HybriDNA, a decoder-only DNA language model that incorporates a hybrid Transformer-Mamba2 architecture, seamlessly integrating the strengths of attention mechanisms with selective state-space models. This hybrid design enables HybriDNA to efficiently process DNA sequences up to 131kb in length with single-nucleotide resolution. HybriDNA achieves state-of-the-art performance across 33 DNA understanding datasets curated from the BEND, GUE, and LRB benchmarks, and demonstrates exceptional capability in generating synthetic cis-regulatory elements (CREs) with desired properties. Furthermore, we show that HybriDNA adheres to expected scaling laws, with performance improving consistently as the model scales from 300M to 3B and 7B parameters. These findings underscore HybriDNA's versatility and its potential to advance DNA research and applications, paving the way for innovations in understanding and engineering the ""language of life"".",Mingqian Ma; Guoqing Liu; Chuan Cao; Pan Deng; Tri Dao; Albert Gu; Peiran Jin; Zhao Yang; Yingce Xia; Renqian Luo; Pipi Hu; Zun Wang; Yuan-Jyue Chen; Haiguang Liu; Tao Qin,,2025-02-15T14:23:43Z,http://arxiv.org/abs/2502.10807v2
2502.15370v1,Weakly Supervised Video Scene Graph Generation via Natural Language   Supervision,"Existing Video Scene Graph Generation (VidSGG) studies are trained in a fully supervised manner, which requires all frames in a video to be annotated, thereby incurring high annotation cost compared to Image Scene Graph Generation (ImgSGG). Although the annotation cost of VidSGG can be alleviated by adopting a weakly supervised approach commonly used for ImgSGG (WS-ImgSGG) that uses image captions, there are two key reasons that hinder such a naive adoption: 1) Temporality within video captions, i.e., unlike image captions, video captions include temporal markers (e.g., before, while, then, after) that indicate time related details, and 2) Variability in action duration, i.e., unlike human actions in image captions, human actions in video captions unfold over varying duration. To address these issues, we propose a Natural Language-based Video Scene Graph Generation (NL-VSGG) framework that only utilizes the readily available video captions for training a VidSGG model. NL-VSGG consists of two key modules: Temporality-aware Caption Segmentation (TCS) module and Action Duration Variability-aware caption-frame alignment (ADV) module. Specifically, TCS segments the video captions into multiple sentences in a temporal order based on a Large Language Model (LLM), and ADV aligns each segmented sentence with appropriate frames considering the variability in action duration. Our approach leads to a significant enhancement in performance compared to simply applying the WS-ImgSGG pipeline to VidSGG on the Action Genome dataset. As a further benefit of utilizing the video captions as weak supervision, we show that the VidSGG model trained by NL-VSGG is able to predict a broader range of action classes that are not included in the training data, which makes our framework practical in reality.",Kibum Kim; Kanghoon Yoon; Yeonjun In; Jaehyeong Jeon; Jinyoung Moon; Donghyun Kim; Chanyoung Park,,2025-02-21T10:42:04Z,http://arxiv.org/abs/2502.15370v1
2503.09793v1,Integrated Experiment and Simulation Co-Design: A Key Infrastructure for   Predictive Mesoscale Materials Modeling,"The design of structural & functional materials for specialized applications is being fueled by rapid advancements in materials synthesis, characterization, manufacturing, with sophisticated computational materials modeling frameworks that span a wide spectrum of length & time scales in the mesoscale between atomistic & continuum approaches. This is leading towards a systems-based design methodology that will replace traditional empirical approaches, embracing the principles of the Materials Genome Initiative. However, several gaps remain in this framework as it relates to advanced structural materials:(1) limited availability & access to high-fidelity experimental & computational datasets, (2) lack of co-design of experiments & simulation aimed at computational model validation,(3) lack of on-demand access to verified and validated codes for simulation and for experimental analyses, & (4) limited opportunities for workforce training and educational outreach. These shortcomings stifle major innovations in structural materials design. This paper describes plans for a community-driven research initiative that addresses current gaps based on best-practice recommendations of leaders in mesoscale modeling, experimentation & cyberinfrastructure obtained at an NSF-sponsored workshop dedicated to this topic. The proposal is to create a hub for Mesoscale Experimentation and Simulation co-Operation (hMESO)-that will (I) provide curation and sharing of models, data, & codes, (II) foster co-design of experiments for model validation with systematic uncertainty quantification, & (III) provide a platform for education & workforce development. It will engage experimental & computational experts in mesoscale mechanics and plasticity, along with mathematicians and computer scientists with expertise in algorithms, data science, machine learning, & large-scale cyberinfrastructure initiatives.",Shailendra P. Joshi; Ashley Bucsek; Darren C. Pagan; Samantha Daly; Suraj Ravindran; Jaime Marian; Miguel A. Bessa; Surya R. Kalidindi; Nikhil C. Admal; Celia Reina; Somnath Ghosh; Jorge Vinals; Ellad B. Tadmor,,2025-03-12T19:55:34Z,http://arxiv.org/abs/2503.09793v1
2503.16351v1,Lyra: An Efficient and Expressive Subquadratic Architecture for Modeling   Biological Sequences,"Deep learning architectures such as convolutional neural networks and Transformers have revolutionized biological sequence modeling, with recent advances driven by scaling up foundation and task-specific models. The computational resources and large datasets required, however, limit their applicability in biological contexts. We introduce Lyra, a subquadratic architecture for sequence modeling, grounded in the biological framework of epistasis for understanding sequence-to-function relationships. Mathematically, we demonstrate that state space models efficiently capture global epistatic interactions and combine them with projected gated convolutions for modeling local relationships. We demonstrate that Lyra is performant across over 100 wide-ranging biological tasks, achieving state-of-the-art (SOTA) performance in many key areas, including protein fitness landscape prediction, biophysical property prediction (e.g. disordered protein region functions) peptide engineering applications (e.g. antibody binding, cell-penetrating peptide prediction), RNA structure analysis, RNA function prediction, and CRISPR guide design. It achieves this with orders-of-magnitude improvements in inference speed and reduction in parameters (up to 120,000-fold in our tests) compared to recent biology foundation models. Using Lyra, we were able to train and run every task in this study on two or fewer GPUs in under two hours, democratizing access to biological sequence modeling at SOTA performance, with potential applications to many fields.",Krithik Ramesh; Sameed M. Siddiqui; Albert Gu; Michael D. Mitzenmacher; Pardis C. Sabeti,"Broad Institute of MIT and Harvard; Broad Institute of MIT and Harvard; Machine Learning Department, Carnegie Mellon University; Broad Institute of MIT and Harvard; Broad Institute of MIT and Harvard",2025-03-20T17:09:18Z,http://arxiv.org/abs/2503.16351v1
2504.19034v3,"On learning functions over biological sequence space: relating Gaussian   process priors, regularization, and gauge fixing","Mappings from biological sequences (DNA, RNA, protein) to quantitative measures of sequence functionality play an important role in contemporary biology. We are interested in the related tasks of (i) inferring predictive sequence-to-function maps and (ii) decomposing sequence-function maps to elucidate the contributions of individual subsequences. Because each sequence-function map can be written as a weighted sum over subsequences in multiple ways, meaningfully interpreting these weights requires ``gauge-fixing,'' i.e., defining a unique representation for each map. Recent work has established that most existing gauge-fixed representations arise as the unique solutions to $L_2$-regularized regression in an overparameterized ``weight space'' where the choice of regularizer defines the gauge. Here, we establish the relationship between regularized regression in overparameterized weight space and Gaussian process approaches that operate in ``function space,'' i.e.~the space of all real-valued functions on a finite set of sequences. We disentangle how weight space regularizers both impose an implicit prior on the learned function and restrict the optimal weights to a particular gauge. We show how to construct regularizers that correspond to arbitrary explicit Gaussian process priors combined with a wide variety of gauges and characterize the implicit function space priors associated with the most common weight space regularizers. Finally, we derive the posterior distribution of a broad class of sequence-to-function statistics, including gauge-fixed weights and multiple systems for expressing higher-order epistatic coefficients. We show that such distributions can be efficiently computed for product-kernel priors using a kernel trick.",Samantha Petti; Carlos Martí-Gómez; Justin B. Kinney; Juannan Zhou; David M. McCandlish,,2025-04-26T22:00:42Z,http://arxiv.org/abs/2504.19034v3
2504.20710v1,SBMLtoOdin and Menelmacar: Interactive visualisation of systems biology   models for expert and non-expert audiences,"Motivation: Computational models in biology can increase our understanding of biological systems, be used to answer research questions, and make predictions. Accessibility and reusability of computational models is limited and often restricted to experts in programming and mathematics. This is due to the need to implement entire models and solvers from the mathematical notation models are normally presented as. Implementation: Here, we present SBMLtoOdin, an R package that translates differential equation models in SBML format from the BioModels database into executable R code using the R package odin, allowing researchers to easily reuse models. We also present Menelmacar, a a web-based application that provides interactive visualisations of these models by solving their differential equations in the browser. This platform allows non-experts to simulate and investigate models using an easy-to-use web interface. Availability: SBMLtoOdin is published under open source Apache 2.0 licence at https://github.com/bacpop/SBMLtoOdin and can be installed as an R package. The code for the Menelmacar website is published under MIT License at https://github.com/bacpop/odinviewer, and the website can be found at https://biomodels.bacpop.org/.",Leonie J. Lorenz; Antoine Andréoletti; Tung V. N. Nguyen; Henning Hermjakob; Richard G. FitzJohn; Rahuman S. Malik Sheriff; John A. Lees,"European Bioinformatics Institute, European Molecular Biology Laboratory, Wellcome Genome Campus, United Kingdom; European Bioinformatics Institute, European Molecular Biology Laboratory, Wellcome Genome Campus, United Kingdom; European Bioinformatics Institute, European Molecular Biology Laboratory, Wellcome Genome Campus, United Kingdom; European Bioinformatics Institute, European Molecular Biology Laboratory, Wellcome Genome Campus, United Kingdom; , MRC Centre for Global Infectious Disease Analysis, and the Abdul Latif Jameel Institute for Disease and Emergency Analytics; European Bioinformatics Institute, European Molecular Biology Laboratory, Wellcome Genome Campus, United Kingdom; European Bioinformatics Institute, European Molecular Biology Laboratory, Wellcome Genome Campus, United Kingdom",2025-04-28T14:11:47Z,http://arxiv.org/abs/2504.20710v1
2505.00458v2,Memory-Centric Computing: Solving Computing's Memory Problem,"Computing has a huge memory problem. The memory system, consisting of multiple technologies at different levels, is responsible for most of the energy consumption, performance bottlenecks, robustness problems, monetary cost, and hardware real estate of a modern computing system. All this becomes worse as modern and emerging applications become more data-intensive (as we readily witness in e.g., machine learning, genome analysis, graph processing, and data analytics), making the memory system an even larger bottleneck. In this paper, we discuss two major challenges that greatly affect computing system performance and efficiency: 1) memory technology & capacity scaling (at the lower device and circuit levels) and 2) system and application performance & energy scaling (at the higher levels of the computing stack). We demonstrate that both types of scaling have become extremely difficult, wasteful, and costly due to the dominant processor-centric design & execution paradigm of computers, which treats memory as a dumb and inactive component that cannot perform any computation. We show that moving to a memory-centric design & execution paradigm can solve the major challenges, while enabling multiple other potential benefits. In particular, we demonstrate that: 1) memory technology scaling problems (e.g., RowHammer, RowPress, Variable Read Disturbance, data retention, and other issues awaiting to be discovered) can be much more easily and efficiently handled by enabling memory to autonomously manage itself; 2) system and application performance & energy efficiency can, at the same time, be improved by orders of magnitude by enabling computation capability in memory chips and structures (i.e., processing in memory). We discuss adoption challenges against enabling memory-centric computing, and describe how we can get there step-by-step via an evolutionary path.",Onur Mutlu; Ataberk Olgun; Ismail Emir Yuksel,,2025-05-01T11:16:09Z,http://arxiv.org/abs/2505.00458v2
2505.11041v1,In silico tool for identification of colorectal cancer from cell-free   DNA biomarkers,"Colorectal cancer remains a major global health concern, with early detection being pivotal for improving patient outcomes. In this study, we leveraged high throughput methylation profiling of cellfree DNA to identify and validate diagnostic biomarkers for CRC. The GSE124600 study data were downloaded from the Gene Expression Omnibus, as the discovery cohort, comprising 142 CRC and 132 normal cfDNA methylation profiles obtained via MCTA seq. After preprocessing and filtering, 97,863 CpG sites were retained for further analysis. Differential methylation analysis using statistical tests identified 30,791 CpG sites as significantly altered in CRC samples, where p is less than 0.05. Univariate scoring enabled the selection of top ranking features, which were further refined using multiple feature selection algorithms, including Recursive Feature Elimination, Sequential Feature Selection, and SVC L1. Various machine learning models such as Logistic Regression, Support Vector Machines, Random Forest, and Multi layer Perceptron were trained and tested using independent validation datasets. The best performance was achieved with an MLP model trained on 25 features selected by RFE, reaching an AUROC of 0.89 and MCC of 0.78 on validation data. Additionally, a deep learning based convolutional neural network achieved an AUROC of 0.78. Functional annotation of the most predictive CpG sites identified several genes involved in key cellular processes, some of which were validated for differential expression in CRC using the GEPIA2 platform. Our study highlights the potential of cfDNA methylation markers combined with ML and DL models for noninvasive and accurate CRC detection, paving the way for clinically relevant diagnostic tools.",Kartavya Mathur; Shipra Jain; Nisha Bajiya; Nishant Kumar; Gajendra P. S. Raghava,"Department of Computational Biology, Indraprastha Institute of Information Technology, Delhi; Department of Computational Biology, Indraprastha Institute of Information Technology, Delhi; Department of Computational Biology, Indraprastha Institute of Information Technology, Delhi; Department of Computational Biology, Indraprastha Institute of Information Technology, Delhi; Department of Computational Biology, Indraprastha Institute of Information Technology, Delhi",2025-05-16T09:35:11Z,http://arxiv.org/abs/2505.11041v1
2505.14725v1,HR-VILAGE-3K3M: A Human Respiratory Viral Immunization Longitudinal Gene   Expression Dataset for Systems Immunity,"Respiratory viral infections pose a global health burden, yet the cellular immune responses driving protection or pathology remain unclear. Natural infection cohorts often lack pre-exposure baseline data and structured temporal sampling. In contrast, inoculation and vaccination trials generate insightful longitudinal transcriptomic data. However, the scattering of these datasets across platforms, along with inconsistent metadata and preprocessing procedure, hinders AI-driven discovery. To address these challenges, we developed the Human Respiratory Viral Immunization LongitudinAl Gene Expression (HR-VILAGE-3K3M) repository: an AI-ready, rigorously curated dataset that integrates 14,136 RNA-seq profiles from 3,178 subjects across 66 studies encompassing over 2.56 million cells. Spanning vaccination, inoculation, and mixed exposures, the dataset includes microarray, bulk RNA-seq, and single-cell RNA-seq from whole blood, PBMCs, and nasal swabs, sourced from GEO, ImmPort, and ArrayExpress. We harmonized subject-level metadata, standardized outcome measures, applied unified preprocessing pipelines with rigorous quality control, and aligned all data to official gene symbols. To demonstrate the utility of HR-VILAGE-3K3M, we performed predictive modeling of vaccine responders and evaluated batch-effect correction methods. Beyond these initial demonstrations, it supports diverse systems immunology applications and benchmarking of feature selection and transfer learning algorithms. Its scale and heterogeneity also make it ideal for pretraining foundation models of the human immune response and for advancing multimodal learning frameworks. As the largest longitudinal transcriptomic resource for human respiratory viral immunization, it provides an accessible platform for reproducible AI-driven research, accelerating systems immunology and vaccine development against emerging viral threats.",Xuejun Sun; Yiran Song; Xiaochen Zhou; Ruilie Cai; Yu Zhang; Xinyi Li; Rui Peng; Jialiu Xie; Yuanyuan Yan; Muyao Tang; Prem Lakshmanane; Baiming Zou; James S. Hagood; Raymond J. Pickles; Didong Li; Fei Zou; Xiaojing Zheng,,2025-05-19T19:37:49Z,http://arxiv.org/abs/2505.14725v1
2506.01714v2,The optimization of crop response to climatic stress through modulation   of plant stress response mechanisms. Opportunities for biostimulants and   plant hormones to meet climate challenges,"Climate change is a major threat to crop potential and is characterized by both long-term shifts in temperature and precipitation patterns as well as increased occurrence of extreme weather events, these extreme weather events are the most immediate and intractable threat to agriculture. Crop resilience in the face of stress depends upon the speed and effectiveness with which plants and cropping systems sense and respond to that stress. A variety of agronomic practices including breeding, exogenous inputs (nutrients, water, biostimulants and others) and shifts in cultivation practice have been used to influence plant stress response to achieve the goal of increased plant and cropping system resilience. Traditional breeding is a powerful tool that has resulted in stable and long-term cultivar improvements but is often too slow and complex to meet the diverse, complex and unpredictable challenges of climate induced stresses. Increased inputs (water, nutrients, pesticides etc.) and management strategies (cropping system choice, soil management etc.) can alleviate stress but are often constrained by cost and availability of inputs. Exogenous biostimulants, microbials and plant hormones have shown great promise as mechanisms to optimize natural plant resilience resulting in immediate but non-permanent improvements in plant responses to climate induced stresses. The failure to modernize regulatory frameworks for the use of biostimulants in agriculture will constrain the development of safe effective tools and deprive growers of means to respond to the vagaries of climate change. Here we discuss the scientific rationale for eliminating the regulatory barriers that constrain the potential for biostimulants or products that modulate plant regulatory networks to address climate change challenges and propose a framework for enabling legislation to strengthen cropping system resilience.",Jing Li; Giulia Forghieri; Danny Geelen; Patrick du Jardin; Patrick H. Brown,,2025-06-02T14:22:14Z,http://arxiv.org/abs/2506.01714v2
2506.10916v1,Semi-Automated Quality Assurance in Digital Pathology: Tile   Classification Approach,"Quality assurance is a critical but underexplored area in digital pathology, where even minor artifacts can have significant effects. Artifacts have been shown to negatively impact the performance of AI diagnostic models. In current practice, trained staff manually review digitized images prior to release of these slides to pathologists which are then used to render a diagnosis. Conventional image processing approaches, provide a foundation for detecting artifacts on digital pathology slides. However, current tools do not leverage deep learning, which has the potential to improve detection accuracy and scalability. Despite these advancements, methods for quality assurance in digital pathology remain limited, presenting a gap for innovation.   We propose an AI algorithm designed to screen digital pathology slides by analyzing tiles and categorizing them into one of 10 predefined artifact types or as background. This algorithm identifies and localizes artifacts, creating a map that highlights regions of interest. By directing human operators to specific tiles affected by artifacts, the algorithm minimizes the time and effort required to manually review entire slides for quality issues.   From internal archives and The Cancer Genome Atlas, 133 whole slide images were selected and 10 artifacts were annotated using an internally developed software ZAPP (Mayo Clinic, Jacksonville, FL). Ablation study of multiple models at different tile sizes and magnification was performed. InceptionResNet was selected. Single artifact models were trained and tested, followed by a limited multiple instance model with artifacts that performed well together (chatter, fold, and pen). From the results of this study we suggest a hybrid design for artifact screening composed of both single artifact binary models as well as multiple instance models to optimize detection of each artifact.",Meredith VandeHaar; M. Clinch; I. Yilmaz; M. A. Rahman; Y. Xiao; F. Dogany; H. M. Alazab; A. Nassar; Z. Akkus; B. Dangott,,2025-06-12T17:30:34Z,http://arxiv.org/abs/2506.10916v1
2506.14861v1,BMFM-RNA: An Open Framework for Building and Evaluating Transcriptomic   Foundation Models,"Transcriptomic foundation models (TFMs) have recently emerged as powerful tools for analyzing gene expression in cells and tissues, supporting key tasks such as cell-type annotation, batch correction, and perturbation prediction. However, the diversity of model implementations and training strategies across recent TFMs, though promising, makes it challenging to isolate the contribution of individual design choices or evaluate their potential synergies. This hinders the field's ability to converge on best practices and limits the reproducibility of insights across studies. We present BMFM-RNA, an open-source, modular software package that unifies diverse TFM pretraining and fine-tuning objectives within a single framework. Leveraging this capability, we introduce a novel training objective, whole cell expression decoder (WCED), which captures global expression patterns using an autoencoder-like CLS bottleneck representation. In this paper, we describe the framework, supported input representations, and training objectives. We evaluated four model checkpoints pretrained on CELLxGENE using combinations of masked language modeling (MLM), WCED and multitask learning. Using the benchmarking capabilities of BMFM-RNA, we show that WCED-based models achieve performance that matches or exceeds state-of-the-art approaches like scGPT across more than a dozen datasets in both zero-shot and fine-tuning tasks. BMFM-RNA, available as part of the biomed-multi-omics project ( https://github.com/BiomedSciAI/biomed-multi-omic ), offers a reproducible foundation for systematic benchmarking and community-driven exploration of optimal TFM training strategies, enabling the development of more effective tools to leverage the latest advances in AI for understanding cell biology.",Bharath Dandala; Michael M. Danziger; Ella Barkan; Tanwi Biswas; Viatcheslav Gurev; Jianying Hu; Matthew Madgwick; Akira Koseki; Tal Kozlovski; Michal Rosen-Zvi; Yishai Shimoni; Ching-Huei Tsou,,2025-06-17T15:40:08Z,http://arxiv.org/abs/2506.14861v1
2506.15728v1,Smartphone-integrated RPA-CRISPR-Cas12a Detection System with   Microneedle Sampling for Point-of-Care Diagnosis of Potato Late Blight in   Early Stage,"Potato late blight, caused by the oomycete pathogen Phytophthora infestans, is one of the most devastating diseases affecting potato crops in the history. Although conventional detection methods of plant diseases such as PCR and LAMP are highly sensitive and specific, they rely on bulky and expensive laboratory equipment and involve complex operations, making them impracticable for point-of care diagnosis in the field. Here in this study, we report a portable RPA-CRISPR based diagnosis system for plant disease, integrating smartphone for acquisition and analysis of fluorescent images. A polyvinyl alcohol (PVA) microneedle patch was employed for sample extraction on the plant leaves within one minute, the DNA extraction efficiency achieved 56 ug/mg, which is approximately 3 times to the traditional CTAB methods (18 ug/mg). The system of RPA-CRISPR-Cas12a isothermal assay was established to specifically target P. infestans with no cross-reactivity observed against closely-related species (P. sojae, P. capsici). The system demonstrated a detection limit of 2 pg/uL for P. infestans genomic DNA, offering sensitivity comparable to that of benchtop laboratory equipment. The system demonstrates the early-stage diagnosis capability by achieving a approximately 80% and 100% detection rate on the third and fourth day post-inoculation respectively, before visible symptoms observed on the leaves. The smartphone-based ""sample-to-result"" system decouples the limitations of traditional methods that rely heavily on specialized equipment, offering a promising way for early-stage plant disease detection and control in the field.",Jiangnan Zhao; Hanbo Xu; Cifu Xu; Wenlong Yin; Laixin Luo; Gang Liu; Yan Wang,"Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, PR China; Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, PR China; Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, PR China; Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, PR China; Department of Plant Pathology, China Agricultural University, Beijing Key Laboratory of Seed Disease Testing and Control, Beijing, PR China; Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, PR China; Key Laboratory of Smart Agriculture Systems, Ministry of Education, China Agricultural University, Beijing, PR China",2025-06-10T13:43:36Z,http://arxiv.org/abs/2506.15728v1
2507.09028v1,From Classical Machine Learning to Emerging Foundation Models: Review on   Multimodal Data Integration for Cancer Research,"Cancer research is increasingly driven by the integration of diverse data modalities, spanning from genomics and proteomics to imaging and clinical factors. However, extracting actionable insights from these vast and heterogeneous datasets remains a key challenge. The rise of foundation models (FMs) -- large deep-learning models pretrained on extensive amounts of data serving as a backbone for a wide range of downstream tasks -- offers new avenues for discovering biomarkers, improving diagnosis, and personalizing treatment. This paper presents a comprehensive review of widely adopted integration strategies of multimodal data to assist advance the computational approaches for data-driven discoveries in oncology. We examine emerging trends in machine learning (ML) and deep learning (DL), including methodological frameworks, validation protocols, and open-source resources targeting cancer subtype classification, biomarker discovery, treatment guidance, and outcome prediction. This study also comprehensively covers the shift from traditional ML to FMs for multimodal integration. We present a holistic view of recent FMs advancements and challenges faced during the integration of multi-omics with advanced imaging data. We identify the state-of-the-art FMs, publicly available multi-modal repositories, and advanced tools and methods for data integration. We argue that current state-of-the-art integrative methods provide the essential groundwork for developing the next generation of large-scale, pre-trained models poised to further revolutionize oncology. To the best of our knowledge, this is the first review to systematically map the transition from conventional ML to advanced FM for multimodal data integration in oncology, while also framing these developments as foundational for the forthcoming era of large-scale AI models in cancer research.",Amgad Muneer; Muhammad Waqas; Maliazurina B Saad; Eman Showkatian; Rukhmini Bandyopadhyay; Hui Xu; Wentao Li; Joe Y Chang; Zhongxing Liao; Cara Haymaker; Luisa Solis Soto; Carol C Wu; Natalie I Vokes; Xiuning Le; Lauren A Byers; Don L Gibbons; John V Heymach; Jianjun Zhang; Jia Wu,,2025-07-11T21:23:21Z,http://arxiv.org/abs/2507.09028v1
2507.10502v2,Benchmarking and Evaluation of AI Models in Biology: Outcomes and   Recommendations from the CZI Virtual Cells Workshop,"Artificial intelligence holds immense promise for transforming biology, yet a lack of standardized, cross domain, benchmarks undermines our ability to build robust, trustworthy models. Here, we present insights from a recent workshop that convened machine learning and computational biology experts across imaging, transcriptomics, proteomics, and genomics to tackle this gap. We identify major technical and systemic bottlenecks such as data heterogeneity and noise, reproducibility challenges, biases, and the fragmented ecosystem of publicly available resources and propose a set of recommendations for building benchmarking frameworks that can efficiently compare ML models of biological systems across tasks and data modalities. By promoting high quality data curation, standardized tooling, comprehensive evaluation metrics, and open, collaborative platforms, we aim to accelerate the development of robust benchmarks for AI driven Virtual Cells. These benchmarks are crucial for ensuring rigor, reproducibility, and biological relevance, and will ultimately advance the field toward integrated models that drive new discoveries, therapeutic insights, and a deeper understanding of cellular systems.",Elizabeth Fahsbender; Alma Andersson; Jeremy Ash; Polina Binder; Daniel Burkhardt; Benjamin Chang; Georg K. Gerber; Anthony Gitter; Patrick Godau; Ankit Gupta; Genevieve Haliburton; Siyu He; Trey Ideker; Ivana Jelic; Aly Khan; Yang-Joon Kim; Aditi Krishnapriyan; Jon M. Laurent; Tianyu Liu; Emma Lundberg; Shalin B. Mehta; Rob Moccia; Angela Oliveira Pisco; Katherine S. Pollard; Suresh Ramani; Julio Saez-Rodriguez; Yasin Senbabaoglu; Elana Simon; Srinivasan Sivanandan; Gustavo Stolovitzky; Marc Valer; Bo Wang; Xikun Zhang; James Zou; Katrina Kalantar,,2025-07-14T17:25:28Z,http://arxiv.org/abs/2507.10502v2
2508.01490v2,A Large-Scale Benchmark of Cross-Modal Learning for Histology and Gene   Expression in Spatial Transcriptomics,"Spatial transcriptomics enables simultaneous measurement of gene expression and tissue morphology, offering unprecedented insights into cellular organization and disease mechanisms. However, the field lacks comprehensive benchmarks for evaluating multimodal learning methods that leverage both histology images and gene expression data. Here, we present HESCAPE, a large-scale benchmark for cross-modal contrastive pretraining in spatial transcriptomics, built on a curated pan-organ dataset spanning 6 different gene panels and 54 donors. We systematically evaluated state-of-the-art image and gene expression encoders across multiple pretraining strategies and assessed their effectiveness on two downstream tasks: gene mutation classification and gene expression prediction. Our benchmark demonstrates that gene expression encoders are the primary determinant of strong representational alignment, and that gene models pretrained on spatial transcriptomics data outperform both those trained without spatial data and simple baseline approaches. However, downstream task evaluation reveals a striking contradiction: while contrastive pretraining consistently improves gene mutation classification performance, it degrades direct gene expression prediction compared to baseline encoders trained without cross-modal objectives. We identify batch effects as a key factor that interferes with effective cross-modal alignment. Our findings highlight the critical need for batch-robust multimodal learning approaches in spatial transcriptomics. To accelerate progress in this direction, we release HESCAPE, providing standardized datasets, evaluation protocols, and benchmarking tools for the community",Rushin H. Gindra; Giovanni Palla; Mathias Nguyen; Sophia J. Wagner; Manuel Tran; Fabian J Theis; Dieter Saur; Lorin Crawford; Tingying Peng,,2025-08-02T21:11:36Z,http://arxiv.org/abs/2508.01490v2
2508.11190v1,Quantum-Boosted High-Fidelity Deep Learning,"A fundamental limitation of probabilistic deep learning is its predominant reliance on Gaussian priors. This simplistic assumption prevents models from accurately capturing the complex, non-Gaussian landscapes of natural data, particularly in demanding domains like complex biological data, severely hindering the fidelity of the model for scientific discovery. The physically-grounded Boltzmann distribution offers a more expressive alternative, but it is computationally intractable on classical computers. To date, quantum approaches have been hampered by the insufficient qubit scale and operational stability required for the iterative demands of deep learning. Here, we bridge this gap by introducing the Quantum Boltzmann Machine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable hybrid quantum-classical architecture. Our framework leverages a quantum processor for efficient sampling from the Boltzmann distribution, enabling its use as a powerful prior within a deep generative model. Applied to million-scale single-cell datasets from multiple sources, the QBM-VAE generates a latent space that better preserves complex biological structures, consistently outperforming conventional Gaussian-based deep learning models like VAE and SCVI in essential tasks such as omics data integration, cell-type classification, and trajectory inference. It also provides a typical example of introducing a physics priori into deep learning to drive the model to acquire scientific discovery capabilities that breaks through data limitations. This work provides the demonstration of a practical quantum advantage in deep learning on a large-scale scientific problem and offers a transferable blueprint for developing hybrid quantum AI models.",Feng-ao Wang; Shaobo Chen; Yao Xuan; Junwei Liu; Qi Gao; Hongdong Zhu; Junjie Hou; Lixin Yuan; Jinyu Cheng; Chenxin Yi; Hai Wei; Yin Ma; Tao Xu; Kai Wen; Yixue Li,,2025-08-15T03:51:20Z,http://arxiv.org/abs/2508.11190v1
2505.06395v1,Contributions of the Petabyte Scale Sequence Search Codeathon toward   efforts to scale sequence-based searches on SRA,"The volume of biological data being generated by the scientific community is growing exponentially, reflecting technological advances and research activities. The National Institutes of Health's (NIH) Sequence Read Archive (SRA), which is maintained by the National Center for Biotechnology Information (NCBI) at the National Library of Medicine (NLM), is a rapidly growing public database that researchers use to drive scientific discovery across all domains of life. This increase in available data has great promise for pushing scientific discovery but also introduces new challenges that scientific communities need to address. As genomic datasets have grown in scale and diversity, a parade of new methods and associated software have been developed to address the challenges posed by this growth. These methodological advances are vital for maximally leveraging the power of next-generation sequencing (NGS) technologies. With the goal of laying a foundation for evaluation of methods for petabyte-scale sequence search, the Department of Energy (DOE) Office of Biological and Environmental Research (BER), the NIH Office of Data Science Strategy (ODSS), and NCBI held a virtual codeathon 'Petabyte Scale Sequence Search: Metagenomics Benchmarking Codeathon' on September 27 - Oct 1 2021, to evaluate emerging solutions in petabyte scale sequence search. The codeathon attracted experts from national laboratories, research institutions, and universities across the world to (a) develop benchmarking approaches to address challenges in conducting large-scale analyses of metagenomic data (which comprises approximately 20% of SRA), (b) identify potential applications that benefit from SRA-wide searches and the tools required to execute the search, and (c) produce community resources i.e. a public facing repository with information to rebuild and reproduce the problems addressed by each team challenge.",Priyanka Ghosh; Kjiersten Fagnan; Ryan Connor; Ravinder Pannu; Travis J. Wheeler; Mihai Pop; C. Titus Brown; Tessa Pierce-Ward; Rob Patro; Jacquelyn S. Michaelis; Thomas L. Madden; Christiam Camacho; Olaitan I. Awe; Arianna I. Krinos; René KM Xavier; Rodrigo Ortega Polo; Jack W. Roddy; Adelaide Rhodes; Alexander Sweeten; Adrian Viehweger; Bariş Ekim; Harihara Subrahmaniam Muralidharan; Amatur Rahman; Vinícius W. Salazar; Andrew Tritt; Thomas Colligan; Katrina Kalantar; Genevieve R. Krause; Taylor Reiter; George Lesica; Artem Babaian; Victor Lin; Sergey Madaminov; Vadim Zalunin; David M. Kristensen; Alexa Salsbury; Daniel P. Rice; J. Rodney Brister,,2025-05-09T19:39:55Z,http://arxiv.org/abs/2505.06395v1
2508.12780v1,"Revisiting the systematics of Brevipalpus flat mites (Tenuipalpidae):   phylogeny, species groups and cryptic diversity","Phytophagous mites in the genus Brevipalpus (Tenuipalpidae) can be major pests, causing direct damage to their host plants or vectoring viruses. However, a phylogeny-based classification to understand their evolution and predict their bioecological aspects is lacking. Accurate species identification is crucial for studying pathosystem interactions, implementing quarantine measures, and developing control strategies. This study revisited the classification of Brevipalpus based on phylogenetic relationships, identifying cryptic species and determining genetic distance boundaries. A multi-tool exploration of DNA datasets, including mitochondrial (two COI regions) and nuclear (ITS2 and D1-D3) data combined with a detailed morphological study using light and scanning microscopy, was performed. Specimens were collected from 20 host plant families from South America, Europe, and the Middle East. Species were discriminated using three different approaches, namely, Automatic Barcode Gap Discovery (ABGD), Assemble Species by Automatic Partition (ASAP), and a local regression to establish consistent genetic distance thresholds. Results indicate that the current species-group classification only partially matches that based on genetic lineages. Some species currently classified as belonging to the phoenicis and cuneatus species groups were found to be polyphyletic and related to species placed in other species groups. A rearrangement of the species groups to align with the observed genetic lineages is proposed, and phylogenetically informative morphological traits are defined. Cryptic diversity in certain taxa was consistently confirmed by various methods. Nuclear and mitochondrial markers were essential for identifying candidate lineages when morphology alone was insufficient. Nuclear markers detected host-associated lineages within certain species. Inconsistencies between markers and methods suggest the presence of ongoing speciation processes, and hypotheses about speciation events were explored. Intra- and interspecific genetic distances and threshold values were determined for the four studied genomic fragments and can be used for routine taxonomic identification and uncovering cryptic lineages. Further research should combine genetic data, carefully selected markers, and thorough morphological analyses before proposing new species.",Renata Santos de Mendonça; Francisco Ferragut; Isis Carolina S. de Oliveira; Aline Daniele Tassi; Felipe Fileni; Ronald Ochoa; Denise Navia,"PNPD/CAPES, UnB, CENARGEN; UPV, IAM; UnB, CENARGEN; ESALQ, UF; UMR CBGP; UMR CBGP; UMR CBGP",2025-08-18T09:55:49Z,http://arxiv.org/abs/2508.12780v1
2502.06320v1,KMT2B-related disorders: expansion of the phenotypic spectrum and   long-term efficacy of deep brain stimulation,"Heterozygous mutations in KMT2B are associated with an early-onset, progressive, and often complex dystonia (DYT28). Key characteristics of typical disease include focal motor features at disease presentation, evolving through a caudocranial pattern into generalized dystonia, with prominent oromandibular, laryngeal, and cervical involvement. Although KMT2B-related disease is emerging as one of the most common causes of early-onset genetic dystonia, much remains to be understood about the full spectrum of the disease. We describe a cohort of 53 patients with KMT2B mutations, with detailed delineation of their clinical phenotype and molecular genetic features. We report new disease presentations, including atypical patterns of dystonia evolution and a subgroup of patients with a non-dystonic neurodevelopmental phenotype. In addition to the previously reported systemic features, our study has identified co-morbidities, including the risk of status dystonicus, intrauterine growth retardation, and endocrinopathies. Analysis of this study cohort (n = 53) in tandem with published cases (n = 80) revealed that patients with chromosomal deletions and protein-truncating variants had a significantly higher burden of systemic disease (with earlier onset of dystonia) than those with missense variants. Eighteen individuals had detailed longitudinal data available after insertion of deep brain stimulation for medically refractory dystonia. Median age at deep brain stimulation was 11.5 years (range: 4.5 to 37.0 years). Follow-up after deep brain stimulation ranged from 0.25 to 22 years. Significant improvement of motor function and disability (as assessed by the Burke-Fahn-Marsden Dystonia Rating Scales, BFMDRS-M and BFMDRS-D) was evident at 6 months, 1 year, and last follow-up (motor, P = 0.001, P = 0.004, and P = 0.012; disability, P = 0.009, P = 0.002, and P = 0.012).",L Cif; D Demailly; JP Lin; KE Barwick; M Sa; L Abela; S Malhotra; WK Chong; D Steel; A Sanchis-Juan; A Ngoh; N Trump; E Meyer; X Vasques; J Rankin; MW Allain; CD Applegate; S Attaripour Isfahani; J Baleine; B Balint; JA Bassetti; EL Baple; KP Bhatia; C Blanchet; L Burglen; G Cambonie; EC Seng; SC Bastaraud; F Cyprien; C Coubes; V d'Hardemare; Deciphering Developmental Disorders Study; A Doja; N Dorison; D Doummar; ME Dy-Hollins; E Farrelly; DR Fitzpatrick; C Fearon; EL Fieg; BL Fogel; EB Forman; RG Fox; Genomics England Research Consortium; WA Gahl; S Galosi; V Gonzalez; TD Graves; A Gregory; M Hallett; H Hasegawa; SJ Hayflick; A Hamosh; M Hully; S Jansen; SY Jeong; JB Krier; S Krystal; KR Kumar; C Laurencin; H Lee; G Lesca; LL François; T Lynch; N Mahant; JA Martinez-Agosto; C Milesi; KA Mills; M Mondain; H Morales-Briceno; NIHR BioResource; JR Ostergaard; S Pal; JC Pallais; F Pavillard; PF Perrigault; AK Petersen; G Polo; G Poulen; T Rinne; T Roujeau; C Rogers; A Roubertie; M Sahagian; E Schaefer; L Selim; R Selway; N Sharma; R Signer; AG Soldatos; DA Stevenson; F Stewart; M Tchan; Undiagnosed Diseases Network; IC Verma; BBA de Vries; JL Wilson; DA Wong; R Zaitoun; D Zhen; A Znaczko; RC Dale; CM de Gusmão; J Friedman; VSC Fung; MD King; SS Mohammad; L Rohena; JL Waugh; C Toro; FL Raymond; M Topf; P Coubes; KM Gorman; MA Kurian,,2025-02-10T10:15:54Z,http://arxiv.org/abs/2502.06320v1
2503.02112v2,Building Machine Learning Challenges for Anomaly Detection in Science,"Scientific discoveries are often made by finding a pattern or object that was not predicted by the known rules of science. Oftentimes, these anomalous events or objects that do not conform to the norms are an indication that the rules of science governing the data are incomplete, and something new needs to be present to explain these unexpected outliers. The challenge of finding anomalies can be confounding since it requires codifying a complete knowledge of the known scientific behaviors and then projecting these known behaviors on the data to look for deviations. When utilizing machine learning, this presents a particular challenge since we require that the model not only understands scientific data perfectly but also recognizes when the data is inconsistent and out of the scope of its trained behavior. In this paper, we present three datasets aimed at developing machine learning-based anomaly detection for disparate scientific domains covering astrophysics, genomics, and polar science. We present the different datasets along with a scheme to make machine learning challenges around the three datasets findable, accessible, interoperable, and reusable (FAIR). Furthermore, we present an approach that generalizes to future machine learning challenges, enabling the possibility of large, more compute-intensive challenges that can ultimately lead to scientific discovery.",Elizabeth G. Campolongo; Yuan-Tang Chou; Ekaterina Govorkova; Wahid Bhimji; Wei-Lun Chao; Chris Harris; Shih-Chieh Hsu; Hilmar Lapp; Mark S. Neubauer; Josephine Namayanja; Aneesh Subramanian; Philip Harris; Advaith Anand; David E. Carlyn; Subhankar Ghosh; Christopher Lawrence; Eric Moreno; Ryan Raikman; Jiaman Wu; Ziheng Zhang; Bayu Adhi; Mohammad Ahmadi Gharehtoragh; Saúl Alonso Monsalve; Marta Babicz; Furqan Baig; Namrata Banerji; William Bardon; Tyler Barna; Tanya Berger-Wolf; Adji Bousso Dieng; Micah Brachman; Quentin Buat; David C. Y. Hui; Phuong Cao; Franco Cerino; Yi-Chun Chang; Shivaji Chaulagain; An-Kai Chen; Deming Chen; Eric Chen; Chia-Jui Chou; Zih-Chen Ciou; Miles Cochran-Branson; Artur Cordeiro Oudot Choi; Michael Coughlin; Matteo Cremonesi; Maria Dadarlat; Peter Darch; Malina Desai; Daniel Diaz; Steven Dillmann; Javier Duarte; Isla Duporge; Urbas Ekka; Saba Entezari Heravi; Hao Fang; Rian Flynn; Geoffrey Fox; Emily Freed; Hang Gao; Jing Gao; Julia Gonski; Matthew Graham; Abolfazl Hashemi; Scott Hauck; James Hazelden; Joshua Henry Peterson; Duc Hoang; Wei Hu; Mirco Huennefeld; David Hyde; Vandana Janeja; Nattapon Jaroenchai; Haoyi Jia; Yunfan Kang; Maksim Kholiavchenko; Elham E. Khoda; Sangin Kim; Aditya Kumar; Bo-Cheng Lai; Trung Le; Chi-Wei Lee; JangHyeon Lee; Shaocheng Lee; Suzan van der Lee; Charles Lewis; Haitong Li; Haoyang Li; Henry Liao; Mia Liu; Xiaolin Liu; Xiulong Liu; Vladimir Loncar; Fangzheng Lyu; Ilya Makarov; Abhishikth Mallampalli Chen-Yu Mao; Alexander Michels; Alexander Migala; Farouk Mokhtar; Mathieu Morlighem; Min Namgung; Andrzej Novak; Andrew Novick; Amy Orsborn; Anand Padmanabhan; Jia-Cheng Pan; Sneh Pandya; Zhiyuan Pei; Ana Peixoto; George Percivall; Alex Po Leung; Sanjay Purushotham; Zhiqiang Que; Melissa Quinnan; Arghya Ranjan; Dylan Rankin; Christina Reissel; Benedikt Riedel; Dan Rubenstein; Argyro Sasli; Eli Shlizerman; Arushi Singh; Kim Singh; Eric R. Sokol; Arturo Sorensen; Yu Su; Mitra Taheri; Vaibhav Thakkar; Ann Mariam Thomas; Eric Toberer; Chenghan Tsai; Rebecca Vandewalle; Arjun Verma; Ricco C. Venterea; He Wang; Jianwu Wang; Sam Wang; Shaowen Wang; Gordon Watts; Jason Weitz; Andrew Wildridge; Rebecca Williams; Scott Wolf; Yue Xu; Jianqi Yan; Jai Yu; Yulei Zhang; Haoran Zhao; Ying Zhao; Yibo Zhong,,2025-03-03T22:54:07Z,http://arxiv.org/abs/2503.02112v2
2505.16619v1,"Open and Sustainable AI: challenges, opportunities and the road ahead in   the life sciences","Artificial intelligence (AI) has recently seen transformative breakthroughs in the life sciences, expanding possibilities for researchers to interpret biological information at an unprecedented capacity, with novel applications and advances being made almost daily. In order to maximise return on the growing investments in AI-based life science research and accelerate this progress, it has become urgent to address the exacerbation of long-standing research challenges arising from the rapid adoption of AI methods. We review the increased erosion of trust in AI research outputs, driven by the issues of poor reusability and reproducibility, and highlight their consequent impact on environmental sustainability. Furthermore, we discuss the fragmented components of the AI ecosystem and lack of guiding pathways to best support Open and Sustainable AI (OSAI) model development. In response, this perspective introduces a practical set of OSAI recommendations directly mapped to over 300 components of the AI ecosystem. Our work connects researchers with relevant AI resources, facilitating the implementation of sustainable, reusable and transparent AI. Built upon life science community consensus and aligned to existing efforts, the outputs of this perspective are designed to aid the future development of policy and structured pathways for guiding AI implementation.",Gavin Farrell; Eleni Adamidi; Rafael Andrade Buono; Mihail Anton; Omar Abdelghani Attafi; Salvador Capella Gutierrez; Emidio Capriotti; Leyla Jael Castro; Davide Cirillo; Lisa Crossman; Christophe Dessimoz; Alexandros Dimopoulos; Raul Fernandez-Diaz; Styliani-Christina Fragkouli; Carole Goble; Wei Gu; John M. Hancock; Alireza Khanteymoori; Tom Lenaerts; Fabio G. Liberante; Peter Maccallum; Alexander Miguel Monzon; Magnus Palmblad; Lucy Poveda; Ovidiu Radulescu; Denis C. Shields; Shoaib Sufi; Thanasis Vergoulis; Fotis Psomopoulos; Silvio C. E. Tosatto,"Department of Biomedical Sciences, University of Padova, Padova, Italy; Athena Research and Innovation Center, Marousi, Greece; VIB.AI Center for AI and Computational Biology, Ghent, Belgium; ELIXIR Europe Hub, EMBL-EBI, Hinxton, United Kingdom; Department of Biomedical Sciences, University of Padova, Padova, Italy; Barcelona Supercomputing Center; Department of Pharmacy and Biotechnology, University of Bologna, Bologna, Italy and Computational Genomics Platform, IRCCS University Hospital of Bologna, Bologna, Italy; ZB MED Information Centre for Life Sciences, Cologne, Germany; Barcelona Supercomputing Center; SequenceAnalysis.co.uk, United Kingdom and University of East Anglia, Norwich, United Kingdom; Department of Computational Biology, University of Lausanne, Lausanne, Switzerland and Swiss Institute of Bioinformatics, Lausanne, Switzerland; Institute for Fundamental Biomedical Science, Biomedical Sciences Research Center Alexander Fleming, Vari, Greece and Department of Informatics & Telematics, School of Digital Technology, Harokopio University, Athens, Greece; School of Medicine, University College Dublin, Dublin, Ireland and Conway Institute of Biomolecular and Biomedical Research, University College Dublin, Dublin, Ireland and IBM Research Dublin, Dublin, Ireland; Institute of Applied Biosciences, Centre for Research and Technology Hellas, Thessaloniki, Greece and Department of Biology, National & Kapodistrian University of Athens, Athens, Greece; Department of Computer Science, University of Manchester, Manchester, United Kingdom; Luxembourg National Data Service, Esch-sur-Alzette, Luxembourg; Institute of Biochemistry and Molecular Genetics, Faculty of Medicine, University of Ljubljana, Ljubljana, Slovenia; Department of Psychology, University of Freiburg, Freiburg, Germany; Machine Learning Group, Universite Libre de Bruxelles, Brussels, Belgium and Artificial Intelligence Lab, Vrije Universiteit Brussel, Brussels, Belgium and Interuniversity Institute of Bioinformatics in Brussels, ULB-VUB, Brussels, Belgium and FARI, AI for the common good institute, ULB-VUB, Brussels, Belgium and Center for Human-Compatible AI, UC Berkeley, Berkeley, CA, USA; ELIXIR Europe Hub, EMBL-EBI, Hinxton, United Kingdom; ELIXIR Europe Hub, EMBL-EBI, Hinxton, United Kingdom; Department of Biomedical Sciences, University of Padova, Padova, Italy; Leiden University Medical Center, Leiden, Netherlands; Swiss Institute of Bioinformatics, Lausanne, Switzerland; LPHI, University of Montpellier, CNRS, INSERM, Montpellier, France; School of Medicine, University College Dublin, Dublin, Ireland and Conway Institute of Biomolecular and Biomedical Research, University College Dublin, Dublin, Ireland; Department of Computer Science, University of Manchester, Manchester, United Kingdom; Athena Research and Innovation Center, Marousi, Greece; Institute of Applied Biosciences, Centre for Research and Technology Hellas, Thessaloniki, Greece; Department of Biomedical Sciences, University of Padova, Padova, Italy and Institute of Biomembranes, Bioenergetics and Molecular Biotechnologies, National Research Council",2025-05-22T12:52:34Z,http://arxiv.org/abs/2505.16619v1
